{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#research-computing-documentation","title":"Research Computing Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Research Computing provides services and support for computational research at MCW. Our primary focus is High Performance Computing (HPC) and research data storage. We also work with MCW investigators to facilitate adoption and use of these advanced computational resources.</p>"},{"location":"#services","title":"Services","text":""},{"location":"#hpc-cluster","title":"HPC Cluster","text":"<p>The HPC cluster is the institution's primary computational resource, and has been available to MCW researchers since March 2021. The cluster contains a variety of compute node architectures, including large memory and GPU, all connected by a 100 Gbps network. The cluster also includes a NVMe scratch storage filesystem. Please see the Quick Start guide for more detail.</p>"},{"location":"#reshpc","title":"ResHPC","text":"<p>Restricted HPC (ResHPC) is a secure way to access and utilize the HPC cluster. It is specifically designed for restricted datasets that have a defined Data Use Agreement (DUA). The ResHPC service is built on the existing HPC cluster, but incorporates a separate, secure login method, and project specific accounts and directories. With ResHPC, you can work with familiar tools while also satisfying complex data provider security requirements. Please see the ResHPC overview to get started.</p>"},{"location":"#data-storage","title":"Data Storage","text":"<p>In addition to the cluster's scratch storage, RCC also provides general purpose research storage with a replicated filesystem. This persistent storage is mounted on the cluster via NFS, or provided directly to users via NFS and SMB. Please see the Storage Overview for more detail.</p>"},{"location":"#software","title":"Software","text":"<p>Cluster software installation and tuning services are available to all users. We will help install most supported software packages on the clusters, and when able, will advise on best use for our clusters. This might include advice on data staging, parallelization, memory utilization, etc. Please note that team members are not domain knowledge experts in the sciences, and will not advise you on the accuracy or applicability of any software package for your research. You can request software installation today.</p>"},{"location":"#consulting","title":"Consulting","text":"<p>We are glad to help with your research data and computing needs. Consulting topics might include how best to use the cluster for a workflow, software install, research data management, help with grants, security, etc. Project time, software development, paid support, and other dedicated resources are not available.</p>"},{"location":"#support-and-training","title":"Support and Training","text":""},{"location":"#email","title":"Email","text":"<p>Contact Research Computing support at help-rcc@mcw.edu.</p>"},{"location":"#virtual-office-hours","title":"Virtual Office Hours","text":"<p>Virtual office hours are available via Zoom. Please email help-rcc@mcw.edu to schedule an appointment.</p>"},{"location":"#workshops","title":"Workshops","text":"<p>RCC is planning workshops on a variety of topics. These are meant to include multiple levels of expertise and cover such topics as HPC, scripting, containers, etc. If you have a suggestion for a new workshop, please contact help-rcc@mcw.edu.</p> Title Level Slides HPC Cluster Onboarding Introductory Download"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>Please include the following acknowledgement in any publication resulting from work on Research Computing resources:</p> <p>Acknowledgement</p> <p>This research was completed in part with computational resources and  technical support provided by the Research Computing Center at the  Medical College of Wisconsin.</p> <p>Check out the list of publications to see how Research Computing is enabling science at MCW.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faq/#access-login","title":"Access &amp; Login","text":"How do I get an account? <p>Please go to the Account Request page and click \"Request an Account\". Fill in the form and we will process your request. We will inform you once your account has been created or if we need further information.</p> How do I login? <p>The method you use to login depends on your computer and use case. We suggest you start with the quickstart guide.</p> Why can't I login? <ul> <li> <p>You might not have an account.</p> </li> <li> <p>You aren't using your MCW NetID and password to login. Remember that if you are a student, your NetID might be different to your email login.</p> </li> <li> <p>You followed a guide and are using the word ''NetID'' as your username.</p> </li> </ul> Why don't I have access to my PI's <code>/group</code> directory? <p>You might not be correctly affiliated with your PI. Contact help-rcc@mcw.edu to correct this issue.</p> How do I get access to a collaborator's files? <p>Ask the collaborator PI to contact help-rcc@mcw.edu to request the access.</p> How do I add a student to my Research Group Storage security group? <ul> <li> <p>PIs should contact help-rcc@mcw.edu with the username of your new student.</p> </li> <li> <p>Non-PIs should contact help-rcc@mcw.edu with your username, the PI's username, and the username of the new student.</p> </li> </ul> How do I reset my password? <p>RCC uses the same credentials as MCW's other services. If you need to reset your password, use the Self Service Password Reset.</p>"},{"location":"faq/#job-management","title":"Job Management","text":"How do I submit a job to the HPC cluster? <p>You can submit a job to the HPC cluster with the <code>sbatch</code> command. For more information about how to write and submit jobs to the cluster, see the cluster jobs guide.</p> <pre><code>$ sbatch hpc-run.slurm\nSubmitted batch job 6782\n</code></pre> How do I run an interactive job on the cluster? <p>You can start an interactive job on the HPC cluster using the SLURM <code>srun</code> command. For more information about interactive jobs, see the cluster jobs guide.</p> <pre><code>$ srun --ntasks=1 --mem-per-cpu=4GB --time=01:00:00 --job-name=interactive --pty bash\n</code></pre> How do I check the status of my job? <p>You can find the current status of your job with the <code>squeue</code> command.</p> <pre><code>$ squeue -j 6696\n            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                1234    normal Testing user  R      37:09      1 cn59\n</code></pre> How can I see the status of the HPC cluster? <p>You can find the current status of the HPC cluster resources with the <code>slurminfo</code> command.</p> <pre><code>$ slurminfo\n        QUEUE   FREE  TOTAL   FREE  TOTAL   RESORC    OTHER  MAXJOBTIME    CORES    NODE  GPU\n    PARTITION  CORES  CORES  NODES  NODES  PENDING  PENDING   DAY-HR:MN  PERNODE  MEM-GB (COUNT)\n        normal   1715   2880     34     60        0        0     7-00:00       48     360 -\n        bigmem      0     96      0      2        0        0     7-00:00       48    1500 -\n        gpu       279    288      3      6        0        0     7-00:00       48     360 gpu:v100(4)\n</code></pre> Why is my job not running? <p>There are several reasons that your job might not be running.</p> <ul> <li> <p>First, the cluster could be busy and your job might be waiting for resources to become available. The cluster is a shared resource and some wait time, while often short or non-existent, can be expected.</p> </li> <li> <p>Your job might be requesting resources that are not currently available. Check the output of <code>squeue -j JobID</code>, where JobID is your SLURM job number. In the output, check the final column '''Nodelist (Reason)'''. You might see '''PD''' followed by a reason why the job is not running. This could indicate that your job is temporarily waiting for resources (see above) or is blocked. The more resources that you request, the higher chance that your job might sit in queue waiting for those resources to become available. Try to limit the amount of resources you request to only the ones you really need.</p> </li> <li> <p>Your job might be blocked by a maintenance window. For details see Job Scheduling and Maintenance.</p> </li> </ul> Can I run a task/script on a login node? <p>Yes, but you should make sure you know exactly how many cores and memory the task will use. Additionally, you should make sure this task will not run for more than 30 min. Examples of allowed tasks might be a pre- or post- processing task for your input/output files. This might also include compiling or installing software. In general, please try to run all computationally intensive tasks on on the cluster compute nodes. For details see the User Etiquette Guide.</p> Can I have root or sudo access on a compute node? <p>No. RCC does not allow root and/or sudo access on any system.</p> What if my job requires more than 7 days? Can I extend my job time? <p>Yes. After you start the job, email the job number and time extension request to help-rcc@mcw.edu.</p>"},{"location":"faq/#software","title":"Software","text":"What software is available on the HPC cluster? <p>You can list all available software on the cluster with the <code>module avail</code> command. See the cluster software guide for details.</p> Can I install my own software? <p>Yes, you may install your own software. The process will vary depending on the type of software. Please contact help-rcc@mcw.edu if you need assistance.</p> Will RCC install my software for me? <p>You may request to have RCC install your software on the cluster. RCC will make a best effort but cannot guarantee that your software will run on the cluster. Please send software installation requests to help-rcc@mcw.edu.</p> How do I install an R package? <p>You can install your own R packages. See the R guide for details. Please also that RCC has centrally installed many packages that are either commonly used, or difficult to install. We suggest to check first if your package is already installed. Please contact help-rcc@mcw.edu for assistance or to have RCC install the package for you.</p> How do I install a Python package? <p>You can install your own Python packages using the system Python, a local Python, a virtual environment, or a conda environment. See the Python guide for details. Please also that RCC has centrally installed many packages that are either commonly used, or difficult to install. We suggest to check first if your package is already installed. Please contact help-rcc@mcw.edu for assistance or to have RCC install the package for you.</p> Can I use Docker on the cluster? <p>No. Docker is not installed or allowed on RCC systems due to security. However, RCC does provide a container solution called Singularity, which can import Docker containers to run on the cluster.</p>"},{"location":"faq/#data-transfer-storage","title":"Data Transfer &amp; Storage","text":"How do I check my available storage directories and utilization? <p>You can easily find your available storage directories and current utilization with the <code>mydisks</code> command.</p> <pre><code>$ mydisks\n======My Lab======\nSize  Used Avail Use% File\n47G   29G   19G  61% /home/user\n932G  158G  774G  17% /group/pi\n4.6T     0  4.6T   0% /scratch/g/pi\n</code></pre> Why does my quota show as less space? <p>Your computer reports storage utilization in base-2 math and the storage system quotas use base-10 math. So, if your quota is 1TB, the <code>mydisks</code> command will show 932GB. Please note that this is a difference in mathematical representation of the same value. While the <code>mydisks</code> command shows less space, you are still able to use your full quota.</p> What types of storage are available? <p>RCC offers multiple types of storage to all users. Each storage type has a unique path on the cluster, and a unique use-case. Some storage is meant only for use with the cluster, while other storage like Research Group Storage can be used as generalized Windows storage. Please see the Storage Overview for details.</p> Can I increase my storage limits? <p>Your home directory limit cannot be increased. You scratch directory limit may be increase upon request but this is subject to availability. Your <code>/group</code> Research Group Storage directory may be increased by purchasing additional space.</p> Why is <code>/group</code> not available on HPC cluster compute nodes? <p>The file system that contains <code>/group</code> is not designed for performance computing. In order to preserve every user's experience with <code>/group</code>, it is not available on compute nodes. You should follow the scratch directory procedures to make sure your data is available to your cluster job.</p> Can I mount my own storage to the cluster? <p>No. RCC provides storage that is mounted to the cluster. This storage is specifically designed to work within the cluster network and meets performance requirements.</p> Where can I store reference data? <p>Reference data will be stored at <code>/hpc/refdata</code> upon request. Please see Reference Data for more information.</p> How do I use my group storage for collaboration outside my lab? <p>Your lab group storage can be used to collaborate with non-lab members. This is done with by-request project directories. For example, if lab <code>pi</code> would like to collaborate with some data for a project called <code>zephyr</code>, then the lab PI would contact RCC to create a project directory.</p> <pre><code>$ ls -l /group/pi\ntotal 8\ndrwxrws---. 3 root sg-pi-zephyr 512 Mar 26 13:52 zephyr\ndrwxrws---. 4 root sg-pi       1024 May 19 14:04 work\n</code></pre> <p>In addition to the usual <code>work</code> directory, there is now a <code>zephyr</code> project directory, which is controlled by the <code>sg-pi-zephyr</code> security group. Project data would go in that directory, and any number of collaborators (MCW researchers) can be added to the security group.</p> Why did my Windows SMB storage share stopped working? <p>Stale connections for Windows shares are not uncommon. If your storage share stopped working, chances are this is a stale connection. This is especially true if the storage was working one day, and you cannot connect the next day. To fix this issue, you'll need to remount the storage with the following steps.</p> <ul> <li> <p>Open Windows File Explorer and right-click on the drive, then select Disconnect.</p> </li> <li> <p>Remount the storage using this guide. In step #4, make sure to select <code>Connect using different credentials</code> during that process. If you are on a non-MCW managed computer, please enter your MCW username with the \"MCWCORP\\\" domain prefix (example MCWCORP\\jsmith).</p> </li> </ul>"},{"location":"faq/#open-ondemand","title":"Open OnDemand","text":"Why does the Open OnDemand web page not load? <p>Open OnDemand supports most modern browsers. However, there is no IE 11 support. To have the best experience using Open OnDemand, use the latest versions of Google Chrome, Mozilla Firefox or Microsoft Edge.</p> How do I fix Open OnDemand if it stops working and shows a <code>Proxy Error</code> web page? <p>The proxy error web page is a common issue that occurs when Open OnDemand is used for a long time without logging out. To diagnose the issue, try an incognito browser session. If that fixes your issue temporarily, proceed to clear your browser cache as a permanent fix.</p> How do I use RStudio on the cluster? <p>Open OnDemand has a built-in RStudio app. You can use this to get a RStudio session on a compute node that you can access via your web browser.</p> How do I get a Jupyter Notebook on the cluster? <p>Open OnDemand has a built-in Jupyter Notebook app. You can use this to get a Jupyter Notebook on a compute node that you can access via your web browser.</p> Can I get a virtual desktop on the cluster? <p>Yes, Open OnDemand does have a virtual desktop app built-in. However, since the cluster compute nodes are not designed or built for desktop use, functionality may be limited.</p>"},{"location":"faq/#about-rcc","title":"About RCC","text":"What is RCC? <p>The Research Computing Center (RCC) provides infrastructure and campus-wide access to resources required for computationally intensive biomedical research. This includes shared hardware and research-specific software which is supported by MCW and research grants.</p> How do I acknowledge Research Computing resources in my publication? <p>Please include the following acknowledgement in any publication resulting from work on Research Computing resources:</p> <p>Acknowledgement</p> <p>This research was completed in part with computational resources and technical support provided by the Research Computing Center at the Medical College of Wisconsin.</p> Where can I find other publications from RCC users? <p>Check out the list of publications to see how Research Computing is enabling science at MCW. The list is update periodically. Please contact help-rcc@mcw.edu to add your publication.</p> Where can I find RCC information for my grant submission? <p>RCC provides boilerplate language for all current systems including cluster, storage, network, staffing, etc. See Grant Assistance for details. Please note that RCC will also provide a letter of support upon request.</p>"},{"location":"grants/","title":"Grant Boilerplate","text":""},{"location":"grants/#grant-assistance","title":"Grant Assistance","text":"<p>Contact RCC</p> <p>Please contact help-rcc@mcw.edu to determine if you have the computing resources necessary for your proposal.</p> <p>Below are descriptions of RCC services that can be used when preparing the resources section of grant proposals. Please feel free to copy relevant portions of this text.</p>"},{"location":"grants/#overview","title":"Overview","text":"<p>The MCW Research Computing Center (RCC) is a division within MCW Information Services. RCC provides campus-wide access to high performance computing (HPC) resources required for computationally-intensive biomedical research. RCC is institutionally supported and available to all MCW students, staff, and faculty. RCC services and operations are governed by representatives of the MCW Faculty in partnership with RCC leadership.</p>"},{"location":"grants/#high-performance-computing","title":"High Performance Computing","text":"<p>The High Performance Computing (HPC) environment includes 79 computational nodes, 4,200 processor cores, and 96 graphical processing units (GPUs). The nodes are interconnected by 100 Gb/s Ethernet, allowing efficient parallel computing for both CPU and GPU intensive workloads. All nodes run the Rocky Linux 8 operating system. Job submission and scheduling is controlled by the Simple Linux Utility for Resource Management (SLURM). SLURM is an open-source HPC scheduling system that automates job submission, controls resource access, and maintains fair use of all systems. Each compute node includes a standardized operating system image, set of compilers, math libraries, and system software. RCC also supports a variety of open-source software and containerized workloads are supported.</p>"},{"location":"grants/#restricted-hpc","title":"Restricted HPC","text":"<p>Restricted HPC (ResHPC) is a secure way to access and utilize the HPC cluster. It is specifically designed with security enhancements for restricted datasets that have a defined Data Use Agreement (DUA). The ResHPC service is built on the existing HPC cluster, but incorporates a separate, secure login method, and project specific accounts and directories.</p>"},{"location":"grants/#data-storage","title":"Data Storage","text":"<p>The system includes a 467 TB NVMe scratch file system enabling high-performance file I/O during job processing. A second 2.6 PB file system provides replicated, persistent storage for active projects. A 100 Gbps network provides fast data transfer between file systems.</p> <p>RCC provides 1 TB of persistent storage to any MCW faculty member for free. Additional persistent storage is available for a fee.</p>"},{"location":"grants/#data-center-and-network-facilities","title":"Data center and Network Facilities","text":"<p>All RCC hardware is housed in a managed data center. This facility includes redundant HVAC cooling systems, redundant power distribution, UPS battery backup, and diesel-powered backup generators. All equipment is on dedicated subnets providing high-capacity redundant (1 Gbps) networking. The data center also has Internet 2 access through WiscNet.</p>"},{"location":"grants/#staff-and-support","title":"Staff and Support","text":"<p>RCC is supported by a team of research computing specialists with additional support and assistance from several central IT teams. RCC provides training, software installation and setup, end-user support, and troubleshooting. This includes consultation with full-time RCC staff members who have extensive system administration and research computing experience.</p>"},{"location":"maintenance/","title":"Maintenance","text":""},{"location":"maintenance/#maintenance","title":"Maintenance","text":"<p>Research Computing uses scheduled maintenance windows to perform upgrades and general upkeep of all systems.</p> <p>Maintenance Schedule Change</p> <p>The maintenance schedule is changing from 3-hour monthly to 9-hour quarterly. The first quarterly maintenance window is September 16, 2025 8AM-5PM. The full 2026 maintenance schedule will be announced later this year. Please send any questions/concerns to help-rcc@mcw.edu.</p>"},{"location":"maintenance/#2025-schedule","title":"2025 Schedule","text":"<ul> <li>September 16, 2025 8AM-5PM</li> </ul>"},{"location":"maintenance/#2026-schedule","title":"2026 Schedule","text":"<ul> <li>TBD</li> </ul>"},{"location":"maintenance/#job-scheduling-around-maintenance","title":"Job scheduling around maintenance","text":"<p>If you submit a job before a maintenance window and the job is sitting in the queue, first check to see why with <code>squeue</code>. In the output, you'll see the last column <code>NODELIST(REASON)</code>. If your job is held for maintenance, you'll see <code>(ReqNodeNotAvail, Reserved for maintenance)</code>.</p> <p>Each job has a walltime request. The walltime request sets the amount of time that your job will be allowed to run on the cluster. On the HPC cluster, the maximum allowed walltime is 7 days.</p> <p>If you submit your job with X hours walltime request, and the maintenance window starts in less than X hours, the job will be held until after the maintenance window closes. The reason is that the scheduler cannot guarantee your job will finish before maintenance begins.</p> <p>To make the job run, resubmit your job with a walltime request that ensures the job will finish before the maintenance window. We can use a simple formula to find that walltime.</p> <p><code>walltime = ( $maint_start_time - $current_time )</code></p> <p>To simplify the process, use the <code>maxwalltime</code> command, which provides the maximum walltime request that will also allow your job to run before maintenance.</p>"},{"location":"pubs/","title":"Publications","text":""},{"location":"pubs/#publications-supported-by-research-computing","title":"Publications supported by Research Computing","text":"<p>Acknowledging Research Computing is easy to do and helps people understand how important RCC resources are to you and MCW.</p> <p>All you have to do is include the following statement in your publication:</p> <p>Acknowledgement</p> <p>This research was completed in part with computational resources and technical support provided by the Research Computing Center at the Medical College of Wisconsin.</p>"},{"location":"pubs/#2025","title":"2025","text":"<p>Target-Based Design of Praziquantel Analogs at Cestode TRPM<sub>PZQ</sub> Sprague DJ, Park S, ... Marchant JS, et al.</p> <p>Oscillating gradient spin echo diffusion time effects implicate variations in neurite beading for the heterogeneous reduced diffusion in human acute ischemic stroke lesions Zhou M, Budde M, ... Beaulieu C, et al.</p> <p>Impact of breathing technique and particle size on laryngopharyngeal dose delivered by dry powder inhalers Dey S, Jadcherla A, Johnston N, Bock JM, Garcia GJM</p> <p>Validation and application of a finite element model simulating failure thresholds of skin during blunt puncture with varying impactor geometries LeSueur J, Hampton CE, Pintar FA</p> <p>Molecular basis of pyruvate transport and inhibition of the human mitochondrial pyruvate carrier Sichrovsky M, Leone V, ... Kunji ERS, et al.</p> <p>Investigating the Association of Concussion and Contact Sport Exposure History With Brain Microstructure Using Quantitative Susceptibility Mapping Espa\u00f1a LY, Brett BL, ... Meier TB, et al.</p> <p>The Prevalence of Undiagnosed Concussions and Their Associations With Current Symptom Reporting in Collegiate-Aged Athletes Bartels HM. Van Bortel KM, Mayer AR, Brett BL, Meier TB</p> <p>Reutericyclin, a specialized metabolite of Limosilactobacillus reuteri, mitigates risperidone-induced weight gain in mice Aboulalazm FA, Kazen AB, ... Kirby JR, et al.</p> <p>A realistic FastQ-based framework FastQDesign for ScRNA-seq study design issues Wang Y, Chen Y, Ahn KW, Lin C</p> <p>Deep learning-based quick MLC sequencing for MRI-guided online adaptive radiotherapy: a feasibility study for pancreatic cancer patients Ahunbay A, Paulson E, Ahunbay E, Zhang Y</p> <p>Discovery of a MET-driven monogenic cause of steatotic liver disease Pinto e Vairo F, Zimmermann MT, ... Lazaridis KN, et al.</p> <p>An Analytical Approach that Combines Knowledge from Germline and Somatic Mutations Enhances Tumor Genomic Reanalyses in Precision Oncology DeVoe E, Reddi HV, ... Zimmermann MT, et al</p> <p>Dry powder inhaler deposition in the larynx and the risk of steroid inhaler laryngitis: A computational fluid dynamics study Dey S, Bock JM, Garcia GJM</p>"},{"location":"pubs/#2024","title":"2024","text":"<p>Systematic analysis of the relationship between fold-dependent flexibility and artificial intelligence protein structure prediction Haque N, Wagenknecht JB, Ratnasinghe BD, Zimmermann MT</p> <p>Expanded Functionality and Portability for the Colvars Library Fiorina G, Marinelli F, ... H\u00e9nin J, et al.</p> <p>Deep learning-based automatic contour quality assurance for auto-segmented abdominal MR-Linac contours Zarenia M, Zhang Y, Sarosiek C, Conlin R, Amjad A, Paulson ES</p> <p>Structural basis for selectivity and antagonism in extracellular GPCR-nanobodies Schlimgen RR, Peterson FC, Heukers R, Smit MJ, McCorvy JD, Volkman BF</p> <p>Bhlhe40 Promotes CD4+ T Helper 1 Cell and Suppresses T Follicular Helper Cell Differentiation during Viral Infection Nguyen C, Kudek M, ... Cui W, et al.</p> <p>Single Cell RNAseq analysis of cytokine-treated human islets: Association of cellular stress with impaired cytokine responsiveness Stancill JS, Kasmani MY, Cui W, Corbett JA</p> <p>Structural and Dynamic Analyses of Pathogenic Variants in PIK3R1 Reveal a Shared Mechanism Associated among Cancer, Undergrowth, and Overgrowth Syndromes Dsouza NR, Cottrell CE, ... Zimmermann MT, et al.</p>"},{"location":"pubs/#2023","title":"2023","text":"<p>Bi-allelic variants in HMGCR cause an autosomal-recessive progressive limb-girdle muscular dystrophy Morales-Rosado JA, Schwab TL, ... Klee EW, et al.</p> <p>Tensor-valued diffusion MRI of human acute stroke Zhou M, Stobbe R, ... Beaulieu C, et al.</p> <p>Associations of prior concussion severity with brain microstructure using mean apparent propagator magnetic resonance imaging Goeckner BD, Brett BL, ... Meier TB, et al.</p> <p>Molecular and cellular basis of praziquantel action in the cardiovascular system Yahya NA, Lanham JK, Sprague DJ, Palygin O, McCorvy JD, Marchant JS</p> <p>Robustness of single-cell RNA-seq for identifying differentially expressed genes Liu Y, Huang J, ... Liang M, et al.</p> <p>Effect of muscle activation scheme in human head-neck model on estimating cervical spine ligament strain from military volunteer frontal impact data Gerringer JW, Somasundaram K, Pintar FA</p> <p>Conformational changes in the activation loop of a bacterial PASTA kinase Bluma MS, Schultz KM, Kristich CJ, Klug CS</p> <p>Loss of PBAF promotes expansion and effector differentiation of CD8+ T cells during chronic viral infection and cancer Kharel A, Shen J, ... Cui W, et al.</p> <p>The effects of cytomegalovirus on brain structure following sport-related concussion Savitz J, Goeckner BD, ... Meier TB, et al.</p> <p>Lymphocytic Choriomeningitis Virus Clone 13 Infection Results in CD8 T Cell\u2013Mediated Host Mortality in Diacylglycerol Kinase \u03b1\u2013Deficient Mice Kudek MR, Xin G, ... Cui W, et al.</p> <p>Nonparametric failure time: Time-to-event machine learning with heteroskedastic bayesian additive regression trees and low information omnibus dirichlet process mixtures Sparapani RA, Logan BR, Maiers MJ, Laud PW, McCulloch RE</p> <p>Fragment-based drug discovery of small molecule ligands for the human chemokine CCL28 Zhou AL, Jensen DR, ... Volkman BF, et al.</p> <p>Comparison of Various Metrics of Repetitive Head Impact Exposure And Their Associations With Neurocognition in Collegiate-Aged Athletes Amadon GK, Goeckner BD, Brett BL, Meier TB</p>"},{"location":"pubs/#2022","title":"2022","text":"<p>Natural variation in the binding pocket of a parasitic flatworm TRPM channel resolves the basis for praziquantel sensitivity Rohr CM, Sprague DJ, Park S, Marchant JS</p> <p>Structural studies of human fission protein FIS1 reveal a dynamic region important for GTPase DRP1 recruitment and mitochondrial fission Egner JM, Nolden KA, ... Hill RB, et al.</p> <p>Spatial transcriptomics demonstrates the role of CD4 T cells in effector CD8 T cell differentiation during chronic viral infection Topchyan P, Zander R, ... Cui W, et al.</p> <p>Clonal lineage tracing reveals mechanisms skewing CD8+ T cell fate decisions in chronic infection Kasmani MY,Zander R, ... Cui W, et al.</p> <p>Tension-bending risk curves for the ATD lower lumbar spine subjected to oblique impact under FAA emergency landing conditions Somasundaram K, Humm JR, Yoganandan N, Moorcroft DM, Pintar FA</p> <p>Occupant Injury and Response on Oblique-Facing Aircraft Seats: A Computational Study Somasundaram K, Humm JR, Khandelwal P, Umale S, Moorcroft DM, Pintar FA</p> <p>Multi-Omic Integration by Machine Learning (MIMaL) Dickinson Q, Aufschnaiter A, Ott M, Meyer JG</p> <p>Group 3 innate lymphoid cells require BATF to regulate gut homeostasis in mice Wu X, Khatun A, ... Cui W, et al.</p> <p>A single, peri-operative antibiotic can persistently alter the post-operative gut microbiome after Roux-en-Y gastric bypass Fernando DG, Saravia FL, Atkinson SN, Barron M, Kirby JR, Kindel TL</p> <p>Bias or biology? Importance of model interpretation in machine learning studies from electronic health records Momenzadeh A, Shamsa A, Meyer JG</p> <p>Conformational selection guides b-arrestin recruitment at a biased G protein\u2013coupled receptor Kleist AB, Jenjak S, ... Volkman BF, et al.</p> <p>Autoreactive CD8 T cells in NOD mice exhibit phenotypic heterogeneity but restricted TCR gene usage Kasmani MY, Ciecko AE, ... Cui W, et al.</p> <p>Single-cell immune profiling reveals a developmentally distinct CD4+ GM-CSF+ T-cell lineage that induces GI tract GVHD Piper C, Hainstock E, ... Drobyski WR, et al.</p> <p>Tfh-cell-derived interleukin 21 sustains effector CD8+ T cell responses during chronic viral infection Zander R, Kasmani MY, ... Cui W, et al.</p> <p>Cytokine and Nitric Oxide-Dependent Gene Regulation in Islet Endocrine and Nonendocrine Cells Stancill JS, Kasmani MY, Khatun A, Cui W, Corbett JA</p> <p>The non-ELR CXC chemokine encoded by human cytomegalovirus UL146 genotype 5 contains a C-terminal \u03b2-hairpin and induces neutrophil migration as a selective CXCR2 agonist Berg C, Wedemeyer MJ, ... L\u00fcttichau HR, et al.</p> <p>Investigating the overlapping associations of prior concussion, default mode connectivity, and executive function-based symptoms Brett BL, Bryant AM, Espa\u00f1a LY, Mayer AR, Meier TB</p> <p>BATF promotes group 2 innate lymphoid cell\u2013mediated lung tissue protection during acute respiratory virus infection Wu X, Kasmani MY, ... Cui W, et al.</p> <p>Enhanced interpretation of 935 hotspot and non-hotspot RAS variants using evidence-based structural bioinformatics Tripathi S, Dsouza NR, Mathison AJ, Leverence E, Urrutia R, Zimmermann MT</p> <p>Improved prediction of older adult discharge after trauma using a novel machine learning paradigm Morris RS, Tignanelli CJ, deRoon-Cassini T, Laud P, Sparapani R</p> <p>The association between concussion history and increased symptom severity reporting is independent of common medical comorbidities, personality factors, and sleep quality in collegiate athletes Brett BL, Nelson LD, Meier TB</p> <p>Positional SHAP (PoSHAP) for Interpretation of machine learning models trained from biological sequences Dickinson Q, Meyer JG</p>"},{"location":"pubs/#2021","title":"2021","text":"<p>Interpreting Sequence Variation in PDAC-Predisposing Genes Using a Multi-Tier Annotation Approach Performed at the Gene, Patient, and Cohort Level Zimmermann MT, Mathison AJ, ... Urrutia RA, et al.</p> <p>Structural bioinformatics enhances mechanistic interpretation of genomic variation, demonstrated through the analyses of 935 distinct RAS family mutations Tripathi S, Dsouza NR, Urrutia R, Zimmermann MT</p> <p>Association of Previous Concussion with Hippocampal Volume and Symptoms in Collegiate-Aged Athletes Meier TB, Espa\u00f1a LY, ... Brett BL, et al.</p> <p>Association of Head Impact Exposure with White Matter Macrostructure and Microstructure Metrics Brett BL, Koch KM, Muftuler LT, Budde M, McCrea MA, Meier TB</p> <p>Acute post-concussive assessments of brain tissue magnetism using magnetic resonance imaging Koch KM, Nencka AS, Swearingen B, Bauer A, Meier TB, McCrea MA</p> <p>P2T2: Protein Panoramic annoTation Tool for the interpretation of protein coding genetic variants DeVoe E, Oliver GR, ... Zimmermann MT, et al.</p> <p>Influence of cervical spine sagittal alignment on range of motion after corpectomy: a finite element study John JD, Kumar GS, Yoganandan N, Rajshekhar V</p> <p>BATF regulates progenitor to cytolytic effector CD8+ T cell transition during chronic viral infection Chen Y, Zander RA, ... Cui W, et al.</p> <p>E2A-regulated epigenetic landscape promotes memory CD8 T cell differentiation Schauder DM, Shen J, ... Cui W, et al.</p> <p>Single-cell RNA sequencing of mouse islets exposed to proinflammatory cytokines Stancill JS, Kasmani MY, Khatun A, Cui W, Corbett JA</p> <p>Suppressive neutrophils require PIM1 for metabolic fitness and survival during chronic viral infection Volberding PJ, Xin G, ... Cui W, et al.</p>"},{"location":"pubs/#2020","title":"2020","text":"<p>Conditional Deletion of PGC-1\u03b1 Results in Energetic and Functional Defects in NK Cells Gerbec ZJ, Hashemi E, ... Malarkannan S, et al.</p> <p>The chemokine X-factor: Structure-function analysis of the CXC motif at CXCR4 and ACKR3 Wedemeyer MJ, Mahn SA, ... Volkman BF, et al.</p> <p>Single-cell lineage mapping of a diverse virus-specific naive CD4 T cell repertoire Khatun A, Kasmani MY, ... Williams MA, et al.</p> <p>Comparative modeling and docking of chemokine-receptor interactions with Rosetta Wedemeyer MJ, Mueller BK, Bender BJ, Meiler J, Volkman BF</p> <p>Amygdala response to emotional faces in adolescents with persistent post-concussion symptoms Bohorquez-Montoya L, Espa\u00f1a LY, Nader AM, Furger RE, Mayer AR, Meier TB</p> <p>Accurate segmentation of prostate cancer histomorphometric features using a weakly supervised convolutional neural network Bukowy JD, Foss H, ... LaViolette PS, et al.</p> <p>Systemic inflammation moderates the association of prior concussion with hippocampal volume and episodic memory in high school and collegiate athletes Brett BL, Savitz J, ... Meier TB, et al.</p> <p>Genetic variants in DGAT1 cause diverse clinical presentations of malnutrition through a specific molecular mechanism Gupta A, Dsouza NR, ... Zimmermann MT, et al.</p> <p>Cumulative effects of prior concussion and primary sport participation on brain morphometry in collegiate athletes: a study from the NCAA\u2013DoD CARE consortium Brett BL, Bobholz SA, ... Meier TB, et al.</p> <p>Resting-state fMRI metrics in acute sport-related concussion and their association with clinical recovery: a study from the NCAA-DOD CARE Consortium Meier TB, Giraldo-Chica M, ... McCrea MA, et al.</p> <p>Novel destabilizing Dynactin variant (DCTN1 p. Tyr78His) in patient with Perry syndrome \u010cierny M, Zimmermann MT ... Brennan R, et al.</p> <p>Optimization of hyperparameters for SMS reconstruction Muftuler LT, Arpinar VE, ... Nencka AS, et al.</p> <p>Amygdala functional connectivity features in grief: a pilot longitudinal study Chen G, Ward BD, Claesges SA, Li SJ, Goveas JS</p> <p>Serial diffusion kurtosis magnetic resonance imaging study during acute, subacute, and recovery periods after sport-related concussion Muftuler LT, Meier TB, Keith M, Budde MD, Huber DL, McCrea MA</p> <p>Covalent-fragment screening of BRD4 identifies a ligandable site orthogonal to the acetyl-lysine binding sites Olp MD, Sprague DJ, ... Smith BC, et al.</p>"},{"location":"pubs/#2019","title":"2019","text":"<p>Integration of Multi-level Molecular Scoring for the Interpretation of RAS-Family Genetic Variation Tripathi S, Dsouza NR, Urrutia RA, Zimmermann MT</p> <p>Modeling the complete chemokine\u2013receptor interaction Wedemeyer MJ, Mueller BK, Bender BJ, Meiler J, Volkman BF</p> <p>Prevalence of potentially clinically significant MRI findings in athletes with and without sport-related concussion Klein AP, Tetzlaff JE, ... Meier TB, et al.</p> <p>Identification and verification of ubiquitin-activated bacterial phospholipases Tessmer MH, Anderson DM, Pickrum AM, Riegert MO, Frank DW</p> <p>CD4+ T cell help is required for the formation of a cytolytic CD8+ T cell subset that protects against chronic infection and cancer Zander R, Schauder D, ... Cui W, et al.</p>"},{"location":"pubs/#2018","title":"2018","text":"<p>The solution structure of CCL28 reveals structural lability that does not constrain antifungal activity Thomas MA, He J, Peterson FC, Huppler AR, Volkman BF</p> <p>mTORC1 and mTORC2 differentially promote natural killer cell development Yang C, Tsaih SW, Lemke A, Flister MJ, Thakar MS, Malarkannan S</p> <p>The importance of biologic knowledge and gene expression context for genomic data interpretation Zimmermann MT</p> <p>Region-based convolutional neural nets for localization of glomeruli in trichrome-stained whole kidney sections Bukowy JD, Dayton A, ... Cowley AW, et al.</p> <p>Resting-state functional connectivity after concussion is associated with clinical recovery Kaushal M, Espana LY, ... Meier TB, et al.</p> <p>Transient receptor potential vanilloid 4 (TRPV4) activation by arachidonic acid requires protein kinase A-mediated phosphorylation Cao S, Anishkin A, ... Zhang DX, et al.</p> <p>Simulating Airway Collapse in Obstructive Sleep Apnea using Fluid-Structure Interaction Methodologies Le TB, Garcia GJM</p>"},{"location":"pubs/#2017","title":"2017","text":"<p>Development and Validation of 2D Difference Intensity Analysis for Chemical Library Screening by Protein-Detected NMR Spectroscopy Egner JM, Jensen DR, ... Hill RB, et al.</p> <p>Metabolically Derived Lysine Acylations and Neighboring Modifications Tune the Binding of the BET Bromodomains to Histone H4 Olp MD, Zhu N, Smith BC</p>"},{"location":"pubs/#2016","title":"2016","text":"<p>Mechanism of Sirt1 NAD+-dependent Protein Deacetylase Inhibition by Cysteine S-Nitrosation Kalous KS, Wynia-Smith SL, Olp MD, Smith BC</p>"},{"location":"cluster/accounts/","title":"Getting an Account","text":""},{"location":"cluster/accounts/#getting-an-account","title":"Getting an Account","text":"<p>A user account is required for access to Research Computing resources, including the clusters. To obtain RCC access, you must be a PI or sponsored by a PI, and have an active MCW account. A PI may sponsor students, postdoctoral fellows, staff, or colleagues with whom they are collaborating on research. Please note, a PI sponsor must have an active RCC account.</p> <p>PI eligibility</p> <p>Research Computing follows MCW policy RS.GN.160 when determining PI eligibility.</p> <p>Request an Account</p>"},{"location":"cluster/accounts/#terms-of-use","title":"Terms of Use","text":"<p>Agreement upon sign-up</p> <p>The following terms of use are presented during RCC account sign-up. By submitting an account request or using a RCC resource, you acknowledge and agree to abide by these terms.</p>"},{"location":"cluster/accounts/#terms-for-users","title":"Terms for Users","text":"<ol> <li>I will ensure that the security, access, storage, and use of data on Research Computing systems comply with any agreements or regulations, including but not limited to the MCW code of Conduct, MCW Corporate Policies, or any applicable data-use agreement (i.e. IRB, federal grant regulations, etc.).</li> <li>I will not store data that is subject to HIPAA on Research Computing systems.</li> <li>In the event that Research Computing ceases to provide the HPC environment, research group storage, etc., I will transfer the data for which I am the steward to another resource.</li> </ol>"},{"location":"cluster/accounts/#additional-terms-for-pi","title":"Additional Terms for PI","text":"<ol> <li>I accept responsibility for:<ul> <li>Ensuring that all users in my permissions group(s) comply with the Terms for Users set by Research Computing.</li> <li>All actions taken by users in my permissions group(s) on Research Computing systems.</li> <li>The Security of my data stored on Research Computing systems.</li> </ul> </li> <li>I accept that Research Computing is not:<ul> <li>Responsible for providing backup of data stored on Research Computing systems.</li> <li>Liable for data loss resulting from hardware failure, unforeseen catastrophic events, and/or software failure.</li> </ul> </li> </ol>"},{"location":"cluster/etiquette/","title":"User Etiquette","text":""},{"location":"cluster/etiquette/#user-etiquette","title":"User Etiquette","text":"<p>RCC clusters are shared resources. Please be respectful of your computational neighbors and adhere to the following guidelines.</p>"},{"location":"cluster/etiquette/#guidelines","title":"Guidelines","text":"<ol> <li> <p>All jobs must be run through the queueing system.</p> <p>For the cluster to work properly as a shared resource, all jobs must go through the queueing system.</p> </li> <li> <p>Do not start computationally intensive work on cluster login nodes.</p> <p>Login nodes are designed for user logins, managing jobs, and accessing data storage. All three of those services must work for the cluster to function. If you start intensive computing on a login node, you may cause some or all of those services to fail or the node itself to fail, resulting in lost work for you and others.</p> </li> <li> <p>User login to compute nodes is prohibited.</p> <p>Any processing on a compute node that is done outside the queueing system can cause the node to fail. Simply put, if you\u2019re not supposed to be computing there, you don\u2019t need to be logged in.</p> </li> <li> <p>Be accurate with your resource requests.</p> <p>Do not request more resources than are needed to complete your job. Most jobs are not parallel. Requesting multiple cores when your job cannot use them wastes resources that might be used by other users. This is a very common problem! Do your best and ask for help if you're unsure.</p> </li> </ol>"},{"location":"cluster/etiquette/#enforcement","title":"Enforcement","text":"<p>Each login node has a safety mechanism built-in to prevent a user from crashing the server or other users' processes. This per-user limit is 4 CPU cores and 20 GB memory. When a user is at or above their limit, the system will throttle their processes. Users will receive email messages when violations occur.</p> <p>Intensive computing on a login node is prohibited.</p> <p>If a command requires more than 1 core, then it should be submitted in a job to the scheduler. The 4 core limit is a safety mechanism to prevent accidentally crashing a login node, not an approval to run a small computational workflow.</p>"},{"location":"cluster/hardware/","title":"Hardware Specs","text":""},{"location":"cluster/hardware/#overview","title":"Overview","text":"<p>The HPC environment became available to MCW researchers in March 2021. The cluster consists of 79 compute nodes, 4,200 CPU cores, and 96 GPUs. The cluster is connected by 7 100 Gbps switches running RoCEv2 (ethernet equivalent to Infiniband). Additionally, a 467 TB NVMe provides scratch storage, and a 2.6 PB scale-out NAS provides persistent storage.</p>"},{"location":"cluster/hardware/#cluster","title":"Cluster","text":"<p>Detailed information is available below. Please note, the table is wide and might require side scrolling to view all data.</p> Nodes Type Cores/node Mem/node (Gb) Disk/node (Gb) GPUs/node Sockets/node Cores/socket Threads/core CPU Vendor CPU Model CPU Base Freq (GHz) CPU Turbo Freq (GHz) GPU Vendor GPU Model GPU Mem (Gb) 60 CPU 48 384 440 2 24 1 Intel 6240R 2.4 4 6 GPU 48 384 440 4 2 24 1 Intel 5220R 2.2 4 NVIDIA Tesla V100 32 2 GPU 48 512 440 4 2 24 1 Intel 6336Y 2.4 3.6 NVIDIA Ampere A40 48 1 GPU 40 512 7000 8 2 20 1 Intel E5-2698 v4 2.2 3.6 NVIDIA Tesla V100 SXM2 32 2 GPU 128 750 7000 8 2 64 1 AMD EPYC 9554 3.1 3.75 NVIDIA Ada Lovelace L40S 48 2 Large Mem 48 1536 440 2 24 1 Intel 6240R 2.4 4 <p>Condo hardware</p> <p>Condo nodes are factored into the overall cluster metrics, but specific hardware details for condo systems are not listed in the table.</p>"},{"location":"cluster/quickstart/","title":"Quick Start","text":""},{"location":"cluster/quickstart/#cluster-quick-start","title":"Cluster Quick Start","text":"<p>This guide contains the minimal steps to get started running computational workflows, with links to further reading included. If you're a first time user, please follow the links and review the full documentation.</p> <p>Need help getting started? Send us an email at help-rcc@mcw.edu.</p>"},{"location":"cluster/quickstart/#prerequisites","title":"Prerequisites","text":"<p>To start using the cluster, you will need:</p> <ul> <li>an active MCW NetID</li> <li>a Research Computing account (see Getting an Account)</li> <li>a good understanding of the cluster documentation</li> <li>some familiarity with Linux</li> </ul>"},{"location":"cluster/quickstart/#logging-in","title":"Logging in","text":"<p>Login is available both on and off campus via SSH and Open OnDemand. Please see the Login guide for details.</p> <p>Clusters are shared resources.</p> <p>Please be respectful of all other users. Do not start resource-intensive scripts on a login node. Do not request more resources than your job can use. For more info see User Etiquette.</p>"},{"location":"cluster/quickstart/#cluster-storage","title":"Cluster Storage","text":"<p>Your account has access to a set of storage directories by default. You can easily find your available storage paths and current utilization on the cluster with the <code>mydisks</code> command. Please see the storage guide for details.</p> <pre><code>$ mydisks\n=====My Lab=====\nSize  Used Avail Use% File\n47G   29G   19G  61% /home/user\n932G  158G  774G  17% /group/pi\n4.6T     0  4.6T   0% /scratch/g/pi\n</code></pre>"},{"location":"cluster/quickstart/#transferring-files","title":"Transferring Files","text":"<p>Most users will need to transfer files to cluster storage. Several methods are available depending on your need. These include Open OnDemand, SSH (command-line), and desktop client software. Please see the file transfer guide for details.</p>"},{"location":"cluster/quickstart/#using-software","title":"Using Software","text":"<p>Software is managed by modules using Lmod. The modules contain information about an application's version, executable, libraries, and documentation. Using module commands, users can add, remove, or switch versions of a application. Please see the cluster software guide for more information.</p>"},{"location":"cluster/quickstart/#running-a-slurm-job","title":"Running a SLURM Job","text":"<p>The clusters use SLURM to manage jobs. Most jobs are scheduled using batch job scripts. A batch job script includes a request for cluster resources, and the commands to run in the job. Each batch job is submitted by the user for remote execution on a compute node. The SLURM scheduler decides where the job should run, based on the requested and available resources. Please see the SLURM job guide for details.</p>"},{"location":"cluster/quickstart/#write-a-job-script","title":"Write a job script","text":"<p>The SLURM job script syntax is shown below. Just like previous clusters, every job needs a job script, which tells the scheduler how and when to run your workload. The following is a simple SLURM batch job script example.</p> test-job.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=test-job\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:01:00\n#SBATCH --account=PI_NetID\n#SBATCH --output=%x-%j.out\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=NetID@mcw.edu # NetID is your username\n\necho \"Starting at $(date)\"\necho \"Job name: ${SLURM_JOB_NAME}, Job ID: ${SLURM_JOB_ID}\"\necho \"I have ${SLURM_CPUS_ON_NODE} CPUs on compute node $(hostname -s)\"\n</code></pre>"},{"location":"cluster/quickstart/#submit-the-job","title":"Submit the job","text":"<p>Now I can submit my example job:</p> <pre><code>sbatch test-job.slurm\n</code></pre> <p>Then I can check my job status:</p> <pre><code>squeue\n</code></pre>"},{"location":"cluster/quickstart/#more-info","title":"More info","text":"<p>This guide has links to additional information about each topic. We strongly recommend to review all documentation.</p> <p>Need help getting started? Send us an email at help-rcc@mcw.edu.</p>"},{"location":"cluster/access/login/","title":"Logging in","text":""},{"location":"cluster/access/login/#logging-in","title":"Logging in","text":"<p>Login is available via web browser app and traditional SSH connection. All logins require your MCW username and password. Remote access via these methods is also available with the addition of MCW Citrix or VPN.</p>"},{"location":"cluster/access/login/#web-browser","title":"Web Browser","text":"<p>Open OnDemand is a web browser-based interface to RCC computing resources. You can manage files, submit and monitor jobs, and run pre-configured interactive apps such as Jupyter and RStudio. All of this is possible without logging in via a traditional SSH client.</p> <p>This is the recommended login method for most users. For more info, see Open OnDemand.</p>"},{"location":"cluster/access/login/#ssh-connection","title":"SSH Connection","text":""},{"location":"cluster/access/login/#web-based","title":"Web-based","text":"<p>Open OnDemand includes a command-line terminal app that connects you to the cluster via SSH. This is the simplest method for users connecting from Windows computers. However, this method does not account for X11. If you plan to run windowed applications, please see the X11 guide below.</p>"},{"location":"cluster/access/login/#from-windows","title":"From Windows","text":"<p>Windows users will need a SSH client application installed on the desktop. Research Computing recommends MobaXterm which includes a terminal emulator, file browser, and X11 server. PuTTY is another option and can be made available in Citrix.</p>"},{"location":"cluster/access/login/#from-mac-or-linux","title":"From Mac or Linux","text":"<p>Mac and Linux users also need a SSH client but benefit from built-in options within the operating system. Additional options exist that will include more features. For example, the iTerm2 application for Mac users is an alternative to the built-in Terminal app.</p>"},{"location":"cluster/access/login/#using-ssh-to-login","title":"Using SSH to login","text":"<p>Each SSH client will vary in specific detail but all will ask for a hostname, username, and password.</p>"},{"location":"cluster/access/login/#hostname","title":"Hostname","text":"<p>Each login hostname follows a common naming convention, login-clustername.rcc.mcw.edu.</p> <p>Depending on your permissions, you may login to the following clusters:</p> <ul> <li>login-hpc.rcc.mcw.edu - primary SLURM cluster</li> </ul>"},{"location":"cluster/access/login/#username","title":"Username","text":"<p>When asked for your username, please use your MCW NetID, which is used for most MCW resources including email login.</p>"},{"location":"cluster/access/login/#password","title":"Password","text":"<p>When prompted, enter your MCW password, which is used for most MCW resources including email login.</p> <p>Never share your password</p> <p>MCW policy prohibits sharing of passwords. Research Computing staff never has access to your password, and will never ask you for your password.</p>"},{"location":"cluster/access/login/#example-login","title":"Example login","text":"<p>In a command-line session:</p> <pre><code>ssh netid@login-clustername.rcc.mcw.edu\n</code></pre> <p>Hidden Password</p> <p>Most SSH client applications will hide your password as you type. This is a security feature, not an error.</p>"},{"location":"cluster/access/login/#using-ssh-with-x11","title":"Using SSH with X11","text":"<p>X11 allows you to run a windowed application on a remote server and transmit the graphical view back to your computer. This is helpful if you want to run a graphical application that is installed on the cluster, but do not want to use a compute node or go through Open OnDemand. Mac users will need Xquartz installed. Windows users should use MobaXterm.</p> <p>User experience</p> <p>Running a windowed application via SSH with X11 can be a very slow, and sometimes unusable experience. When possible, avoid using X11 over VPN or WiFi.</p> <p>To login to HPC cluster with X11 enabled:</p> <pre><code>ssh -Y netid@login-hpc.rcc.mcw.edu\n</code></pre> <p>Your SSH login will proceed as normal. To launch a windowed application, load the correct module and start the software. The graphical view of the application will open on your computer.</p>"},{"location":"cluster/access/login/#additional-ssh-options","title":"Additional SSH options","text":"<p>OpenSSH has many configuration options that you can use to customize your login experience. Custom SSH options are added to <code>~/.ssh/config</code> on your local computer.</p>"},{"location":"cluster/access/login/#custom-hostnames","title":"Custom hostnames","text":"<p>You can customize and simplify the login hostnames with a host entry in <code>~/.ssh/config</code>.</p> <pre><code>Host login-hpc\n    HostName login-hpc.rcc.mcw.edu\n    User netid\n</code></pre> <p>This will simplify:</p> <p><code>ssh netid@login-hpc.rcc.mcw.edu</code></p> <p>to:</p> <p><code>ssh login-hpc</code></p>"},{"location":"cluster/access/mobaxterm/","title":"MobaXterm","text":""},{"location":"cluster/access/mobaxterm/#mobaxterm","title":"MobaXterm","text":"<p>MobaXterm is a full-featured login tool for Windows users. It features a pseudo-shell, SSH client, SFTP client, and X11 server. Unlike other programs that handle one or two of these features, MobaXterm bundles all features in an easy-to-use format. Lastly, it doesn't require installation to run.</p>"},{"location":"cluster/access/mobaxterm/#downloading","title":"Downloading","text":"<p>MobaXterm is available at the link below. Select the Installer version and proceed with normal Windows style install. Select the Portable version if you want to run without installing.</p> <p>Download MobaXterm</p> <p>If you downloaded the portable version, make sure to save it in a place that is easy to access and will not be deleted.</p>"},{"location":"cluster/access/mobaxterm/#cluster-login","title":"Cluster Login","text":"<p>To get started, start MobaXterm and select <code>Session &gt; SSH</code>. In the session settings, enter <code>login-hpc.rcc.mcw.edu</code> for the Remote host, and enter your MCW username for the Username. To login, select <code>Ok</code>.</p> <p></p> <p>You will see a prompt for your password. Type in your MCW password and press enter. Please note that the password is hidden or security purposes.</p> <p>Local terminal</p> <p>Alternatively, you may select Start local terminal on the MobaXterm startup screen. This will create a local terminal on your Windows computer, which you could use to login to the cluster via traditional SSH commands.</p>"},{"location":"cluster/access/mobaxterm/#file-transfer","title":"File Transfer","text":"<p>The built-in SFTP client will start and load on the left side of the MobaXterm window when you login to a remote host. This will show you your files on the remote host. In the case of the cluster, it will show your cluster home directory. Additionally, all of the folders that you normally access from the cluster command-line are available.</p> <p>MobaXterm also has a standalone SFTP client application. To start the SFTP client, select <code>Session &gt; SFTP</code> and fill in the Remote host and Username.</p>"},{"location":"cluster/access/mobaxterm/#multi-factor-authentication","title":"Multi-factor Authentication","text":"<p>To transfer data to secure systems such as ResHPC, you will need to enable multi-factor authentication (MFA) for the MobaXterm SFTP client. In non-CLI cases such as MobaXterm SFTP, the dialogue interface for MFA requires additional configuration. To enable MFA for SFTP using MobaXterm, select <code>Session &gt; SFTP &gt; Advanced Sftp settings</code>. Enter the Remote host, Username, and make sure the 2-steps authentication box is checked.</p> <p></p>"},{"location":"cluster/access/mobaxterm/#x11-windowed-apps","title":"X11 Windowed Apps","text":"<p>MobaXterm includes a X11 server that allows you to run windowed applications that are installed on the cluster, but have them appear on your desktop. While this is convenient, it can be very slow for resource intensive applications, especially if you are on VPN. For resource intensive windowed apps you should try the Remote Desktop app in Open OnDemand.</p>"},{"location":"cluster/access/mobaxterm/#example-igv","title":"Example - IGV","text":"<p>To launch IGV, first login to the cluster as described above and load the IGV module.</p> <pre><code>module load igv\n</code></pre> <p>Then, launch IGV.</p> <pre><code>igv\n</code></pre> <p>IGV should open in a new window on your desktop.</p>"},{"location":"cluster/access/mobaxterm/#ssh-keys","title":"SSH Keys","text":"<p>Please see the MobaXterm section in the SSH keys guide.</p>"},{"location":"cluster/access/ondemand/","title":"Open OnDemand","text":""},{"location":"cluster/access/ondemand/#open-ondemand","title":"Open OnDemand","text":"<p>Open OnDemand is a web browser-based interface to RCC computing resources. You can manage files, submit and monitor jobs, and run pre-configured interactive apps such as Jupyter and RStudio. All of this is possible without logging in via a traditional SSH terminal.</p>"},{"location":"cluster/access/ondemand/#documentation","title":"Documentation","text":"<p>This documentation is specific for RCC's Open OnDemand instance. Open OnDemand was created by the Ohio Supercomputer Center, which provides full documentation.</p>"},{"location":"cluster/access/ondemand/#connecting-to-open-ondemand","title":"Connecting to Open OnDemand","text":"<p>Browser Support</p> <p>No IE 11 support. To have the best experience using Open OnDemand, use the latest versions of Google Chrome, Mozilla Firefox or Microsoft Edge.</p>"},{"location":"cluster/access/ondemand/#on-campus","title":"On Campus","text":"<p>To connect to Open OnDemand point your browser to https://ondemand.rcc.mcw.edu. Open OnDemand is configured to use your MCW username and password. Access does require you have an RCC user account.</p> <p>After login, the Dashboard will appear. The menu includes apps to manage files, submit jobs, monitor jobs, open a terminal session, or launch interactive apps. To end your session, click Log Out at the top right.</p>"},{"location":"cluster/access/ondemand/#off-campus","title":"Off Campus","text":"<p>Open OnDemand is available for remote work through Citrix. Please see the remote access guide for details.</p> <p>To access Open OnDemand, point your Citrix web browser to https://ondemand.rcc.mcw.edu.</p>"},{"location":"cluster/access/ondemand/#command-line-terminal","title":"Command-line Terminal","text":"<p>Open OnDemand includes a command-line terminal app that connects you to the HPC cluster login node via SSH. From the Dashboard menu, select Clusters &gt; Cluster Shell Access A new window will open and you will be logged in to a cluster login node. This shell access is the same as if you had used a Terminal app from your desktop (i.e. Putty, SSH Secure Shell, MobaXterm, etc.).</p> <p>The shell app now supports themes!</p> <p>You can customize the them of your OOD shell by selecting a new option from the themes drop-down menu in the upper right-hand corner of the window.</p> <p>Logout!</p> <p>When you are done with your shell, please logout with the <code>exit</code> or <code>quit</code> commands.</p>"},{"location":"cluster/access/ondemand/#file-management","title":"File Management","text":"<p>The Files menu contains links to common storage locations. Clicking one of the file links opens the File Explorer in a new browser tab.</p> Button Function <code>Open in Terminal</code> Open current directory in a terminal window in a new browser tab <code>New File</code> Create a new, empty file <code>New Directory</code> Create a new directory <code>Upload</code> Upload a file from your local machine <code>Download</code> Download selected file to your local machine <code>Copy/Move</code> Copy selected file to clipboard <code>Delete</code> Delete selected file <code>Show Owner/Mode</code> Toggle the display of owner and permission settings <code>Show Dotfiles</code> Toggle the display of dotfiles (files starting by a ., which are usually hidden) <code>Filter</code> Filter files/folders by pattern <p>Each file and folder will have additional options. Select the <code>\u22ee</code> dropdown menu to rename, download, delete, view (file only), or edit (file only) a single item.</p>"},{"location":"cluster/access/ondemand/#upload-a-file","title":"Upload a File","text":"<p>To upload a file, select the Upload button. Then use the file selector to choose a file to upload. Please note that this only works for individual files. If you need to upload a folder, see Upload a Folder.</p>"},{"location":"cluster/access/ondemand/#upload-a-folder","title":"Upload a Folder","text":""},{"location":"cluster/access/ondemand/#on-campus_1","title":"On-Campus","text":"<p>To upload a folder, drag and drop the folder from your local desktop to your OnDemand Files app browser window. As you drag the folder to the window, your cursor should show a copy indicator.</p>"},{"location":"cluster/access/ondemand/#off-campus_1","title":"Off-Campus","text":"<p>If you're off-campus, you'll be accessing OnDemand via Citrix browser. To upload a folder, drag and drop the folder from your Windows File Explorer (Citrix app) to your OnDemand Files app (Citrix browser window). Again, as you drag the folder to the window, your cursor should show a copy indicator.</p>"},{"location":"cluster/access/ondemand/#job-submission","title":"Job Submission","text":"<p>Jobs can be created, submitted, and monitored via the Jobs Menu. This is an alternative to creating and submitting job scripts in the command line.</p> <p>From the Dashboard menu, select Jobs &gt; Job Composer which will open in a new window. The Job Composer app has two tabs: Jobs and Templates The Jobs tab contains a list of all jobs previously submitted through Open OnDemand. The Templates tab allows you to create your own job templates.</p> <p>Jobs can be created from previous jobs or job templates. See OSC's Job Management Guide for more information.</p>"},{"location":"cluster/access/ondemand/#create-a-new-job","title":"Create a New Job","text":"<p>In the Job Composer app, select the Jobs tab. To create a new job script, click the + New Job button and select From Default Template You'll see a new job script entry with status Not Submitted. On the right hand side you'll see the Job Details, including the location and name of the job script.</p>"},{"location":"cluster/access/ondemand/#edit-a-job-script","title":"Edit a Job Script","text":"<p>The default job template creates a new generic job script. You'll need to edit this job script so that it contains the workflow you'll submit to the cluster. To edit a job script, click the Open Editor button at the lower right.</p> <p>This will open a job editor in a new tab. You should write your job script following the normal SLURM job syntax. For information on writing SLURM job scripts, see the Job Script Guide. After editing the job script, save the script and close the editor window.</p>"},{"location":"cluster/access/ondemand/#edit-job-options","title":"Edit job options","text":"<p>Click the blue Job Options button. Here you can change the job name and the cluster where the job will run. Click Save or Back to close the job options window.</p>"},{"location":"cluster/access/ondemand/#submit-a-job","title":"Submit a Job","text":"<p>To submit a job, select a job and click the green Submit button. A message at the top of the window will indicate if the job submission was successful or not. After job submission, the job status will change to Queued or Running When the job completes, the status will show Completed</p>"},{"location":"cluster/access/ondemand/#monitor-jobs","title":"Monitor Jobs","text":"<p>From the Dashboard menu, select Jobs &gt; Active Jobs for a live view of the cluster queue. You can view all jobs, your own jobs, and select by cluster.</p>"},{"location":"cluster/access/ondemand/#interactive-applications","title":"Interactive Applications","text":"<p>Open OnDemand features several interactive applications that run on cluster nodes and are accessed through the web interface. This allows you to run interactive apps directly in your web browser.</p> <p>GUI Apps</p> <p>Some Open OnDemand apps require a Graphical User Interface (GUI). One example is the Remote Desktop app. When starting a GUI app, you can select Compression and Image Quality settings before connecting. Most sessions will run best with minimum compression and maximum image quality. If you have a slow network connection, you can maximize compression and minimize image quality.</p>"},{"location":"cluster/access/ondemand/#jupyter-notebook-example","title":"Jupyter Notebook Example","text":"<p>Here we focus on the Jupyter Notebook app as a popular example, but there are many apps to choose from. Check the full app list for details.</p> <p>To start a Jupyter Notebook on a cluster server:</p> <ol> <li>From the Dashboard menu, select Interactive Apps &gt; Jupyter Notebook. You'll see two categories in the Interactive Apps drop-down menu. Select for the resource you need.</li> <li>The My Interactive Sessions screen will open. Select for the resource parameters that you need. You can also select to be notified by email when your session will start. </li> <li>Click the Launch button to start your Jupyter Notebook session. You may have to wait for cluster resources to become available.</li> <li>Once the session starts, click the Connect to Jupyter button. A Jupyter Notebook will open in a new window. </li> <li>To terminate your Jupyter Notebook session, go back to the My Interactive Sessions page and click the red Delete button.</li> </ol> <p>Please see our full Jupyter guide for additional help.</p>"},{"location":"cluster/access/ondemand/#known-issues","title":"Known Issues","text":""},{"location":"cluster/access/ondemand/#proxy-error","title":"Proxy Error","text":"<p>If you use Open OnDemand for long periods without logging out, you may see a <code>Proxy Error</code> web page. To diagnose the issue, try an incognito browser session. If that fixes your issue temporarily, proceed to clear your browser cache as a permanent fix. To prevent this issue, logout of Open OnDemand at the end of each day when you finish working.</p>"},{"location":"cluster/access/putty/","title":"PuTTY","text":""},{"location":"cluster/access/putty/#putty","title":"PuTTY","text":"<p>PuTTY is a lightweight SSH client for Windows users. It features session and profile management and can be run with or without installation.</p>"},{"location":"cluster/access/putty/#downloading","title":"Downloading","text":"<p>PuTTY is available at the link below. For Windows installation, select the MSI (Windows Installer) 64-bit x86 version and proceed with install. If you prefer not to install, download the putty.exe 64-bit x86 version in the Alternative binary section.</p> <p>Download PuTTY</p> <p>If you downloaded the no install version, make sure to save it in a place that is easy to access and will not be deleted.</p>"},{"location":"cluster/access/putty/#cluster-login","title":"Cluster Login","text":"<p>PuTTY features profile management so that the login node hostname can be reused. To get started, open PuTTY and fill in the Host Name box with login-hpc.rcc.mcw.edu. Make sure the connection type is SSH (default). In the Saved Sessions box give the profile a meaningful name (i.e., \"HPC Cluster\") and select Save.</p> <p>Now you can select Open to initiate an SSH connection to the cluster. The first time you connect, you'll be asked to accept the cluster's security key. Select Yes to accept the key and continue.</p> <p>You will see a terminal prompt <code>login as:</code>. Type in your MCW username and press enter. You will see a prompt for your password. Type in your MCW password and press enter. Please note that the password is hidden as you type for security purposes.</p>"},{"location":"cluster/access/remote-access/","title":"Remote Access","text":""},{"location":"cluster/access/remote-access/#remote-access","title":"Remote Access","text":"<p>Off-campus access to RCC resources is dependent on MCW-IS standard remote access methods. For more information on remote work, see Remote Work Guide. RCC resources are available remotely using Citrix.</p>"},{"location":"cluster/access/remote-access/#citrix","title":"Citrix","text":"<p>Citrix is the recommended method of remote access to RCC resources. Citrix is available off-campus using MFA for login.</p>"},{"location":"cluster/access/remote-access/#putty-in-citrix","title":"PuTTY in Citrix","text":"<p>PuTTY is a well-documented terminal application used for SSH access to Linux systems. Contact the MCW-IS help desk to request PuTTY for your Citrix account.</p> <p>To access PuTTY, first login to Citrix. From the Citrix home page, select Admin Tools, then select PuTTY.</p>"},{"location":"cluster/access/remote-access/#open-ondemand-in-citrix","title":"Open OnDemand in Citrix","text":"<p>Use the web browser in Citrix to access RCC's Open OnDemand service. This is a web-portal for utilizing our HPC resources. This access includes file browsing, SSH access, job management, etc. For more information, please see Open OnDemand.</p> <p>To access Open OnDemand, first login to Citrix. From the Citrix home page, select Browsers, and select your Citrix web browser of choice. Open OnDemand is most stable in Google Chrome.</p> <p>Point your Citrix web browser to https://ondemand.rcc.mcw.edu.</p>"},{"location":"cluster/access/ssh-keys/","title":"SSH Keys","text":""},{"location":"cluster/access/ssh-keys/#ssh-keys","title":"SSH Keys","text":"<p>A key pair may be used in place of a password for SSH or SFTP authentication. This is a convenient and secure way to authenticate.</p>"},{"location":"cluster/access/ssh-keys/#maclinux","title":"Mac/Linux","text":"<p>Use the built-in ssh-keygen tool to generate your key pair.</p> <pre><code>ssh-keygen -t rsa -b 4096\n</code></pre> <p>You will be prompted save the key files. Press Enter to accept the default value.</p> <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/Users/username/.ssh/id_rsa):\n</code></pre> <p>You will be prompted to enter a passphrase to save. Use a strong password with complexity and length.</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre> <p>Add the new private key to your computer's list of keys. Your private key is the file without the .pub extension. You will be prompted for your passphrase.</p> <pre><code>ssh-add id_rsa\n</code></pre> <p>To complete this setup for cluster login, copy your public key to the cluster login node.</p> <pre><code>cat ~/.ssh/id_rsa.pub | ssh NetID@login-hpc.rcc.mcw.edu 'cat &gt;&gt; ~/.ssh/authorized_keys'\n</code></pre> <p>Test your setup by logging in to the cluster. A successful login should not prompt you for a password.</p>"},{"location":"cluster/access/ssh-keys/#windows","title":"Windows","text":""},{"location":"cluster/access/ssh-keys/#mobaxterm-cli","title":"MobaXterm CLI","text":"<p>MobaXterm supports a similar functionality for Windows users. To generate a key pair, open MobaXterm and select <code>Start local terminal</code>.</p> <p>Use the built-in ssh-keygen tool to generate your key pair.</p> <pre><code>ssh-keygen -t rsa -b 4096\n</code></pre> <p>You will be prompted save the key files. Press Enter to accept the default value.</p> <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/Users/username/.ssh/id_rsa):\n</code></pre> <p>You will be prompted to enter a passphrase to save. Use a strong password with complexity and length.</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre> <p>Copy your public key to the cluster login node.</p> <pre><code>cat ~/.ssh/id_rsa.pub | ssh NetID@login-hpc.rcc.mcw.edu 'cat &gt;&gt; .ssh/authorized_keys\n</code></pre> <p>To complete the setup, first logout of all shells and close MobaXterm. Open a new MobaXterm window and select Session. In the Session menu, select SSH. Enter the remote host, login-hpc.rcc.mcw.edu. Select Specify username and enter your MCW username. Select Use private key and enter the private key you generated (should be automatic).</p> <p></p> <p>Select the icon to the right of the username field to open the Password settings window. Finally, select Save SSH keys passphrases as well, and close this window.</p> <p></p> <p>To test your setup, login by selecting Ok. Your first successful login should prompt you for your private key passphrase, but not your password. Subsequent logins should not prompt for either password.</p>"},{"location":"cluster/access/ssh-keys/#mobaxterm-gui","title":"MobaXterm GUI","text":"<p>If you do not want to use command-line, try the <code>MobaKeyGen</code> tool in MobaXterm.</p> <p></p> <p>Adjust the default settings so that key type is RSA, and key length is 4096. Select Generate, and move your mouse over the open space.</p> <p></p> <p>You should see key information. Enter an optional comment and a secure passphrase. Use a strong password with complexity and length. Lastly, save your public and private keys.</p> <p></p>"},{"location":"cluster/access/ssh-keys/#security","title":"Security","text":"<p>Private SSH Key</p> <p>Treat your private key like a password. Do not share it with anyone. We will never ask you for your private key or your password.</p>"},{"location":"cluster/jobs/running-jobs/","title":"Submitting SLURM Jobs","text":""},{"location":"cluster/jobs/running-jobs/#running-jobs","title":"Running Jobs","text":"<p>We use SLURM (Simple Linux Utility for Resource Management) for submitting, scheduling, and monitoring workloads, which we call jobs. Each job consists of resource requests and a set of commands to run. SLURM helps to schedule these jobs to run on the cluster using efficient methods to maximize throughput and minimize waiting. From the moment you submit a job, SLURM will make many decisions about priority of your workload, fair utilization of the cluster, and which resource is best to complete your job.</p> <p>Clusters are shared resources.</p> <p>Please be respectful of all other users. Do not start resource-intensive scripts on a login node. Do not request more resources than your job can use. For more info see User Etiquette.</p> <p>Need help scheduling your job? Send us an email at help-rcc@mcw.edu.</p>"},{"location":"cluster/jobs/running-jobs/#about-slurm","title":"About SLURM","text":"<p>As a cluster workload manager, SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.</p>"},{"location":"cluster/jobs/running-jobs/#common-slurm-commands","title":"Common SLURM Commands","text":"<p>These are the most common SLURM commands.</p> <p>Submit a batch job script:</p> <pre><code>sbatch test-job.slurm\n</code></pre> <p>List queued and running jobs:</p> <pre><code>squeue\n</code></pre> <p>Show information about a running job:</p> <pre><code>squeue -j jobId # jobId is the job number\n</code></pre> <p>Cancel a queued job or stop a running job:</p> <pre><code>scancel jobId # jobId is the job number\n</code></pre> <p>Show history of job:</p> <pre><code>sacct -j jobId # jobId is the job number\n</code></pre> <p>RCC provides an additional command <code>slurminfo</code>.</p> <p>Use this command to see cluster info and a summary of SLURM stats.</p> <p> </p><pre><code>$ slurminfo\n       QUEUE   FREE  TOTAL   FREE  TOTAL   RESORC    OTHER  MAXJOBTIME    CORES    NODE  GPU      \n   PARTITION  CORES  CORES  NODES  NODES  PENDING  PENDING   DAY-HR:MN    /NODE  MEM-GB (COUNT)\n      normal   2066   2880     41     60        0      144     7-00:00       48     360 -         \n      bigmem     48     96      0      2        0        0     7-00:00       48    1500 -         \n         gpu    223    384      1      8        0       12     7-00:00       48 360-480 gpu:v100:4(S:0-1)(6),gpu:a40:4(S:0-3)(2)\n</code></pre> <p></p>"},{"location":"cluster/jobs/running-jobs/#managing-job-inputoutput-files","title":"Managing Job Input/Output Files","text":"<p>The HPC cluster requires a specific job workflow:</p> <ol> <li>Copy job input/supporting files from your RGS directory within <code>/group/PI_NetID/...</code> to your scratch directory <code>/scratch/g/PI_NetID</code></li> <li>Submit a job from your <code>/scratch/...</code> directory that utilizes the staged job input/supporting files. You must make sure the job I/O runs in your scratch directory. The easiest way is to run the sbatch command in your <code>/scratch/...</code> directory.</li> <li>When job finishes, copy results from <code>/scratch/...</code> back to <code>/group/PI_NetID/</code>....</li> <li>If there are further computations, continues utilizing the job input/supporting files</li> <li>When jobs are finished and staged job input/supporting files are no longer needed, delete the job input/supporting files from <code>/scratch/...</code></li> </ol> <p>Warning</p> <p>Your jobs will fail if you do not follow this procedure. Please note that files in <code>/scratch/...</code> that are older than 180 days will be deleted.</p>"},{"location":"cluster/jobs/running-jobs/#slurm-concepts","title":"SLURM Concepts","text":""},{"location":"cluster/jobs/running-jobs/#nodes-cores-and-tasks","title":"Nodes, Cores, and Tasks","text":"<p>Before we discuss writing jobs scripts, or submitting jobs, it is important to understand how resources are requested and allocated through SLURM on HPC. SLURM uses three <code>#SBATCH</code> directives, <code>--ntasks</code>, <code>--nodes</code>, and <code>--cpus-per-task</code>, which can be used to run . SLURM uses the concept of tasks, which are processes that can use one or more CPU cores to run a copy of a program. Below we'll discuss how nodes, cores, and tasks are used by single-thread, multi-thread, multi-process, and MPI applications.</p>"},{"location":"cluster/jobs/running-jobs/#single-thread","title":"Single-thread","text":"<p>Many applications are only able to use one CPU core. For these single-thread programs, one task running on one core will suffice.</p> <pre><code>#SBATCH --ntasks=1\n</code></pre> <p>Most applications do not use more than one CPU core. Single-thread jobs that request multiple cores, waste resources. If your application does not mention multi-thread, multi-core, or MPI, please do not request more than one CPU core. When in doubt, contact RCC for clarification.</p>"},{"location":"cluster/jobs/running-jobs/#multi-thread","title":"Multi-thread","text":"<p>A multi-thread application is able to use multiple cores on a single server by spawning multiple threads from a single process. Each thread uses one CPU core, and all share memory. For a multi-thread application, we can use a single task, and specify multiple CPUs per task.</p> <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n</code></pre> <p>In this case, the job requests one task that can use 4 CPU cores. The user's application should then start one process that runs 4 threads.</p> <p>Since all CPU cores are attached to one process, multi-thread applications cannot use more than one server. Multi-thread jobs that request multiple nodes, waste resources. If your application does not say MPI, please do not request more than one node. If your application does not mention multi-thread, multi-core, or MPI, please do not request more than one CPU core. When in doubt, contact RCC for clarification.</p>"},{"location":"cluster/jobs/running-jobs/#multi-process","title":"Multi-process","text":"<p>A multi-process application, often referred to as \"embarrassingly parallel\", is able to use multiple cores by starting multiple independent processes. Each process uses one CPU core. For a multi-process application, we request multiple tasks.</p> <pre><code>#SBATCH --ntasks=4\n</code></pre> <p>In this case, the job requests 4 tasks, which will use 4 CPU cores. Each task can run one copy of the user's application on one CPU core. If your application does not mention multi-thread, multi-core, or MPI, please do not request more than one CPU core. When in doubt, contact RCC for clarification.</p>"},{"location":"cluster/jobs/running-jobs/#mpi","title":"MPI","text":"<p>A multi-process application that is written to use Message Passing Interface (MPI), is able to use multiple cores across multiple nodes. MPI uses the SLURM task information to distribute the application processing across multiple cores and nodes. Each MPI process is able to communicate with the others using the high-performance, low-latency network. Therefore, an MPI application can scale across cores and nodes with little drop in performance. For a MPI application, we request multiple nodes and multiple tasks.</p> <pre><code>#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=8\n</code></pre> <p>In this case, the job requests 4 nodes, each running 8 MPI processes. The total number of MPI processes is equal to <code>nodes * tasks-per-node</code>. SLURM will run these MPI processes with the fewest nodes possible. In our case above, we request 4 nodes and 8 tasks-per-node, or 32 total MPI processes. If we run this job on HPC, SLURM will use 1 node and 32 CPU cores, since HPC has 48 cores per node. If we had requested a higher number of tasks-per-node, SLURM would allocate multiple nodes. This is all done transparently by SLURM.</p> <p>Please note that there are far fewer MPI applications than single- or multi-thread. If your application does not specifically mention MPI in its documentation, it probably does not support MPI. Again, single- or multi-thread jobs that request multiple nodes, waste resources. If your application does not say MPI, please do not request more than one node. When in doubt, contact RCC for clarification.</p>"},{"location":"cluster/jobs/running-jobs/#partitions","title":"Partitions","text":"<p>SLURM partitions (aka queues) organize nodes in groups by type, and allow for scheduling, priority, fairshare, and more. The HPC cluster uses 3 partitions, normal, bigmem, and gpu. The partition configuration is a work in progress and we adjust resource limits as needed.</p> <p>Default Partition</p> <p>The default normal partition is applied automatically and the <code>--partition</code> flag is not required for jobs.</p>"},{"location":"cluster/jobs/running-jobs/#normal","title":"Normal","text":"<p>The normal partition contains the CPU-only nodes, cn01-cn60. This is the default partition, so any job that does not specify a partition will run in normal. Each node in this partition has 48 cores, 360GB RAM, and a 480GB SSD for local scratch. Most jobs should use this partition, especially if you're using an MPI application. A job that does not parallelize, should use 1 core. A job that does not use MPI, should use 48 cores or less. Jobs using MPI can use multiple nodes.</p>"},{"location":"cluster/jobs/running-jobs/#bigmem","title":"Bigmem","text":"<p>The bigmem partition contains the large memory nodes, hm01-hm02. Each node in this partition has 48 cores, 1.5TB RAM, and a 480GB SSD for local scratch. Use the <code>#SBATCH --partition=bigmem</code> directive to have your job run on a large memory server.</p>"},{"location":"cluster/jobs/running-jobs/#gpu","title":"GPU","text":"<p>The gpu partition contains the gpu nodes. The original nodes gn01-gn06 each have 48 cores, 360GB RAM, 4 V100 GPUs, and a 480GB SSD for local scratch. Additional nodes have been added over time. Please see the hardware guide for details.</p>"},{"location":"cluster/jobs/running-jobs/#qos","title":"QOS","text":"<p>A Quality Of Service (QOS) modifier combines additional job resource limits and settings to existing partitions. The dev QOS is meant to allow interactive, development, and/or debugging jobs to be more responsive on the cluster. This QOS has more restricted limits for these jobs, since dev/debug jobs are not considered production jobs. However, the benefit of dev QOS is the higher priority setting, which in many cases will float your job to the top of the queue, potentially allowing it to run quicker. The dev QOS can be combined with any partition, so that these jobs can be combined with specialized resources such as high memory or GPU.</p>"},{"location":"cluster/jobs/running-jobs/#job-scheduling-policies","title":"Job Scheduling Policies","text":"<p>Job scheduling policies include resource limits on partitions and QOS's, and a fairshare algorithm. The fairshare configuration is currently in development.</p>"},{"location":"cluster/jobs/running-jobs/#resource-limits","title":"Resource Limits","text":"Partition Max Time (default) Max Nodes/User Max Cores/User Max GPUs/User Max Mem/Core (default) Priority normal 7 days (2 hrs) 30 1440 N/A 7.5GB (7.5GB) 10 bigmem 7 days (2 hrs) 1 48 N/A 31GB (31GB) 10 gpu 7 days (2 hrs) 4 192 16 10GB (7.5GB) 10 QOS Max Time Max Nodes/User Max Cores/User Max GPUs/User Max Mem/Core (default) Priority dev 8 hrs 1 8 1 7.5GB (7.5GB) 10000"},{"location":"cluster/jobs/running-jobs/#fairshare-and-priority","title":"Fairshare and Priority","text":"<p>Fairshare is a scheduler algorithm that manages job priority, based on a comparison of all cluster utilization, to promote equal use of the cluster. Fairshare tracks daily cluster usage and incorporates data from the previous 7 days to adjust job priority. The effect of fairshare data is reduced each day using a decay factor. In practice, when a user increases their cluster utilization, their job priority is lowered compared to other users. Fairshare effectively allows infrequent users a fair chance to run their jobs on the cluster among the many jobs of more frequent users.</p> <p>You can print the queued job priority with the <code>sprio</code> command.</p> <pre><code>$ sprio \u2013j 5197367\nJOBID   PARTITION  PRIORITY  SITE  AGE  FAIRSHARE  JOBSIZE  QOS TRES\n5197367 gpu        1372      0     0    8432       225      0   cpu=83,mem132,gres/\n</code></pre> <p>The PRIORITY column contains the value used to prioritize jobs in the queue. It is based on the values in the other columns including AGE, FAIRSHARE, JOBSIZE, and TRES. AGE is based on length of time queued, and helps ensure jobs do not get queued indefinitely. FAIRSHARE is based on previous cluster utilization. JOBSIZE is based on the number of nodes or CPUs a job is allocated and ensures large and small jobs are prioritized. TRES is based on specific resources and represents their rarity or abundance.</p> <p>While most priority factors are fixed, the age factor increases as job queue time increases. Therefore, a job's priority will increase over time. You can see this when we run the <code>sprio</code> command again.</p> <pre><code>$ sprio \u2013j 5197367\nJOBID   PARTITION  PRIORITY  SITE  AGE  FAIRSHARE  JOBSIZE  QOS TRES\n5197367 gpu        1374      0     2    8432       225      0   cpu=83,mem132,gres/\n</code></pre> <p>Notice that both the priority and age have increased.</p> <p>You can use <code>sprio</code> to check how many jobs are ahead of yours in the corresponding partition queue. Replace <code>gpu</code> by your job partition and <code>5197367</code> by your jobId. In this example, there are 6 jobs ahead of <code>5197367</code>.</p> <pre><code>$ sprio \u2013p gpu --sort \u2013y | awk '{print NR-1 $0}' | less +g -p 5197367\n0 JOBID PARTITION PRIORITY SITE AGE FAIRSHARE JOBSIZE  QOS  TRES\n1 5200677 gpu     9835     0    0   7564      1353     0    cpu=118,mem=176,gres\n2 5191312 gpu     2239     0    6   111       1353     0    cpu=142,mem=3,gres/g\n3 5191332 gpu     2239     0    6   111       1353     0    cpu=142,mem=3,gres/g\n4 5168126 gpu     2239     0    6   111       1353     0    cpu=142,mem=3,gres/g\n5 5191352 gpu     2236     0    3   111       1353     0    cpu=142,mem=3,gres/g\n6 5167923 gpu     2236     0    3   111       1353     0    cpu=142,mem=3,gres/g\n7 5197367 gpu     1376     0    27  222       64       0    cpu=142,mem=28,gres/\n8 5198832 gpu     1377     0    27  222       64       0    cpu=142,mem=28,gres/\n9 5198833 gpu     1107     0    27  222       64       0    cpu=142,mem=28,gres/\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#writing-a-job-script","title":"Writing a Job Script","text":"<p>A job script tells the scheduler what resources are required to run a specific set of workload commands. It is a text file using shell script syntax, denoted by the required first line, <code>#!/bin/bash</code>. It can be broken into two sections; resource requests and executable commands. Each section will be explained using the following example SLURM job script that can be used as a starting template for your jobs.</p> test-job.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=test-job\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:01:00\n#SBATCH --account=PI_NetID\n#SBATCH --partition=partition\n#SBATCH --output=%x-%j.out\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=NetID@mcw.edu\n\necho \"Starting at $(date)\"\necho \"Job name: ${SLURM_JOB_NAME}, Job ID: ${SLURM_JOB_ID}\"\necho \"I have ${SLURM_CPUS_ON_NODE} CPUs on compute node $(hostname -s)\"\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#job-requests","title":"Job Requests","text":"<p>This section is comprised of <code>#SBATCH</code> directives that tell the scheduler what resources you're requesting. Some of the directives are required, as noted below. The job request section is always required at the top of a job script, but there is no specific order for the <code>#SBATCH</code> directives. While more than one directive can be combined on a single line, RCC does recommend a separate line for each.</p>"},{"location":"cluster/jobs/running-jobs/#job-name","title":"Job Name","text":"<p>A job name is required and is set with the <code>#SBATCH --job-name=</code> option. Your job will appear in the queue with this name and job output files may be based on it. Letters, digits, underscore, and hyphens are allowed.</p> <pre><code>#SBATCH --job-name=myCoolJobName ### REQUIRED\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#cpu-resources","title":"CPU Resources","text":"<p>A CPU resource request is required and tells the job scheduler how to allocate processes. CPU resources can include <code>--ntasks</code>, <code>--cpus-per-task</code>, <code>--nodes</code>, or <code>--ntasks-per-node</code>.</p> <pre><code>#SBATCH --ntasks=1\n</code></pre> <p>Please see Nodes, Cores, and Tasks for more information.</p>"},{"location":"cluster/jobs/running-jobs/#memory","title":"Memory","text":"<p>A memory request is required and tells the job scheduler how much memory to allocate to processes. SLURM has default- and maximum-memory-per-core settings. If your job is single-thread, please use the <code>--mem</code> flag. If your job is multi-thread or MPI, please use the <code>--mem-per-cpu</code> flag. Please note that SLURM enforces memory per core and total memory. Your job will fail if your application exceeds the memory you requested, or the total available memory.</p> <pre><code>#SBATCH --mem=1gb\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#time","title":"Time","text":"<p>A time request is required and tells SLURM how long your job will run. The <code>--time</code> flag sets the max time, in DD-HH:MM:SS, that your job can run. If your job exceeds that time limit, it will fail. SLURM is also configured with default and max time limits.</p> <pre><code>#SBATCH --time=00:01:00\n</code></pre> <p>Does your job require more than the maximum 7 day time limit?</p> <p>Email the jobid number and time extension request to help-rcc@mcw.edu</p>"},{"location":"cluster/jobs/running-jobs/#account","title":"Account","text":"<p>An account is required for your job to run. The <code>--account</code> option should be set to your PI's NetID, or the NetID of a collaborator PI. If you need to adjust your account membership, please contact RCC.</p> <pre><code>#SBATCH --account=PI_NetID\n</code></pre> <p>You can easily find your accounts with the <code>myaccts</code> command.</p> <p> </p><pre><code>$ myaccts\nAccount        Partition\npi             bigmem,dev,gpu,normal\n</code></pre> <p></p>"},{"location":"cluster/jobs/running-jobs/#partition","title":"Partition","text":"<p>A partition is not required but can be added with <code>#SBATCH --partition=</code>. The partition flag is only needed if requesting complex resources, such as the large memory or GPU nodes.</p> <pre><code>#SBATCH --partition=&lt;partition&gt;\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#job-output","title":"Job Output","text":"<p>A job output flag is not required. By default SLURM will send job output to <code>slurm-%j.out</code> in the working directory, where <code>%j</code> is the SLURM jobid. If the <code>--output</code> flag is used, SLURM will output the application's STDOUT to the given filename. In our example script, SLURM output is sent to <code>%x-%j.out</code>, where <code>%x</code> is the job name.</p> <pre><code>#SBATCH --output=%x-%j.out\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#notifications","title":"Notifications","text":"<p>Notifications are optional and can be sent from the job scheduler to an email address of your choice. Many types are supported, but common options include when a job begins <code>#SBATCH --mail-type=BEGIN</code>, when a job ends <code>#SBATCH --mail-type=END</code>, and when a job fails <code>#SBATCH --mail-type=FAIL</code>. Options may be combined, i.e. <code>#SBATCH --mail-type=BEGIN,END</code>. There is also an option to send all notifications <code>#SBATCH --mail-type=ALL</code> The recipient email is specified with <code>#SBATCH --mail-user=NetID@mcw.edu</code>. RCC recommends using notifications during job debugging. Adding notifications as a default may result in email spam, especially if you're submitting many jobs.</p> <pre><code>#SBATCH --mail-type=ALL ### OPTIONAL\n#SBATCH --mail-user=NetID@mcw.edu ### OPTIONAL\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#job-commands","title":"Job Commands","text":"<p>The job commands section always follows the job request section and begins with the first non #SBATCH line. Executable commands commonly include file I/O, software commands, and directory cleanup.</p> <pre><code>echo \"Starting at $(date)\"\necho \"Job name: ${SLURM_JOB_NAME}, Job ID: ${SLURM_JOB_ID}\"\necho \"I have ${SLURM_CPUS_ON_NODE} CPUs on compute node $(hostname -s)\"\n</code></pre> <p>In the example job script, the job commands include several environment variables that are created by SLURM when your job starts. These can be useful to automate tasks or print helpful information about your job to your output file.</p>"},{"location":"cluster/jobs/running-jobs/#submit-a-batch-job","title":"Submit a Batch Job","text":"<p>Most jobs that run on HPC are batch jobs. A batch job is submitted with the <code>sbatch</code> command and requires a job script. This is the best method for production job as it allows you to submit many jobs and let SLURM do the work. With a batch job, there is no requirement that you sit and watch the command-line. You can submit the job and come back later.</p> <pre><code>sbatch test-job.slurm\n</code></pre> <p>A job script is required. If you haven't already, please review Writing a Job Script.</p>"},{"location":"cluster/jobs/running-jobs/#submit-an-interactive-job","title":"Submit an Interactive Job","text":"<p>Interactive jobs are limited to 8 hours max walltime!</p> <p>Make sure to request 8 hours or less or your job will not run!</p> <p>Users may need an interactive session for debugging or related tasks. An interactive job is submitted with the <code>srun</code> command. All of the SLURM options that are required in job scripts are required with the <code>srun</code> command. In addition, <code>--qos=dev</code> is added automatically.</p> <pre><code>srun --ntasks=1 --mem-per-cpu=4GB --time=01:00:00 --job-name=interactive --account=PI_NetID --pty bash\n</code></pre> <p>To stop your interactive job, use the <code>exit</code> command.</p> <pre><code>exit\n</code></pre> <p>Open OnDemand</p> <p>We encourage all users to make use of Open OnDemand for interactive sessions.</p>"},{"location":"cluster/jobs/running-jobs/#gpu-jobs","title":"GPU Jobs","text":"<p>In SLURM, GPUs are referred to as generic resources (GRES) for scheduling purposes. You can add a GPU to your batch or interactive job submission with the <code>--gres=gpu:N</code> flag, where N is the number of GPUs.</p> <p>In a job script add:</p> <pre><code>#SBATCH --gres=gpu:1\n#SBATCH --partition=gpu\n</code></pre> <p>Interactive job command:</p> <pre><code>srun --ntasks=1 --mem-per-cpu=4GB --gres=gpu:1 --time=01:00:00 --job-name=interactive --partition=gpu --account=PI_NetID --pty bash\n</code></pre> <p>Use the <code>#SBATCH --partition=gpu</code> and <code>#SBATCH --gres=gpu:1</code> directives to have your job run on a gpu node.</p>"},{"location":"cluster/jobs/running-jobs/#gpu-type","title":"GPU Type","text":"<p>To use a specific GPU type, use <code>--gres=gpu:type:1</code>, where type is <code>v100</code>, <code>a40</code>, or <code>l40s</code>. Please note that most jobs will not benefit from specifying a GPU type, and instead it may potentially delay scheduling of your job.</p>"},{"location":"cluster/jobs/running-jobs/#gpu-compute-mode","title":"GPU Compute Mode","text":"<p>Your workload may require a specific GPU compute mode (not common). If this is required, Research Computing staff will provide instructions for that software either in the software pages on this site, or the software's <code>module help</code> output.</p> <p>For reference, you can specify GPU compute mode with <code>--gpu_cmode=mode</code>, where mode is <code>shared</code> or <code>exclusive</code>. Run command <code>srun --help</code> to see additional information.`</p>"},{"location":"cluster/jobs/running-jobs/#debugdev-jobs","title":"Debug/Dev Jobs","text":"<p>The cluster includes a special <code>dev</code> QOS. Any job using this QOS will have a higher priority than non-QOS jobs. The goal is for a interactive/dev/debug job to start running quicker, to speed up the developer's work. While this QOS will attempt to move your job to the top of the queue, it does not guarantee the job will start immediately. This can be combined with any of the other partition and gres requests.</p> <p>In a job script add:</p> <pre><code>#SBATCH --qos=dev\n#SBATCH --partition=&lt;partition&gt;\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#array-jobs","title":"Array Jobs","text":"<p>A job array is used to submit multiple similar jobs. Each element in the array represents an independent subjob.</p> <p>Job arrays are submitted with the SLURM option <code>--array=x-y</code>. Here we specify an array with index 1-5, or 5 subjobs.</p> <pre><code>#SBATCH --array=1-5\n</code></pre> <p>If the SLURM job number is 1000, then the array subjobs will have ids <code>1000_1</code>, <code>1000_2</code>, ..., <code>1000_5</code>. Each subjob will also have an environment variable <code>SLURM_ARRAY_TASK_ID</code> that is set to its array index value.</p> <p>When submitting many jobs at once, the job array can limit the number of subjobs running at a time. Here we specify an array with index 1-50, limited to 5 running at a time.</p> <pre><code>#SBATCH --array=1-50%5\n</code></pre> <p>A job array can be deleted with the <code>scancel</code> command.</p> <pre><code>scancel 1000\n</code></pre> <p>Alternatively, a single subjob can be deleted.</p> <pre><code>scancel 1000_3\n</code></pre> <p>Example: We want to run the same code on 50 different input files. The input files are conveniently named <code>input-1</code>, <code>input-2</code>, etc. The following script will process all 50 input files, 5 at a time.</p> test-array-job.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=array_example\n#SBATCH --nodes=1\n#SBATCH --ntasks=5\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:10:00\n#SBATCH --array=1-50%5\n#SBATCH --output=array_%A-%a.out\n\ncommand input-${SLURM_ARRAY_TASK_ID}\n</code></pre>"},{"location":"cluster/jobs/running-jobs/#job-scheduling-and-maintenance","title":"Job Scheduling and Maintenance","text":"<p>If you submit a job before a maintenance window and the job is sitting in the queue, first check to see why with <code>squeue</code>. In the output, you'll see the last column <code>NODELIST(REASON)</code>. If your job is held for maintenance, you'll see <code>(ReqNodeNotAvail, Reserved for maintenance)</code>.</p> <p>Each job has a walltime request. The walltime request sets the amount of time that your job will be allowed to run on the cluster. On the HPC cluster, the maximum allowed walltime is 7 days.</p> <p>If you submit your job with X hours walltime request, and the maintenance window starts in less than X hours, the job will be held until after the maintenance window closes. The reason is that the scheduler cannot guarantee your job will finish before maintenance begins.</p> <p>To make the job run, resubmit your job with a walltime request that ensures the job will finish before the maintenance window. We can use a simple formula to find that walltime.</p> <p><code>walltime = ( $maint_start_time - $current_time )</code></p> <p>To simplify the process, use the <code>maxwalltime</code> command, which provides the maximum walltime request that will also allow your job to run before maintenance.</p> <p>Info</p> <p>Please note, RCC schedules maintenance windows every first Wednesday of the month from 9PM-12AM.</p>"},{"location":"cluster/jobs/software-job/","title":"Software in a Job","text":""},{"location":"cluster/jobs/software-job/#loading-software-in-a-job","title":"Loading Software in a Job","text":"<p>For most jobs, you'll need to load software prior to running your commands. Module commands are used to add a software environment, including executable commands and environment variables. For convenience, you can use an abbreviation <code>ml</code>, in place of the <code>module</code> command.</p> <pre><code>#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n#SBATCH ...\n\nml reset\nml load R/4.1.1\n\nRscript ...\n</code></pre> <p>Please see the software modules guide for more detail.</p>"},{"location":"cluster/jobs/storage-job/","title":"Using Data in a Job","text":""},{"location":"cluster/jobs/storage-job/#using-data-in-a-job","title":"Using Data in a Job","text":"<p>You will find storage paths that are common on all cluster compute nodes. These include <code>/tmp</code> and <code>/scratch</code>, which can be used for job files. If you would like more detail about Research Computing storage, please see the storage guide.</p>"},{"location":"cluster/jobs/storage-job/#workflow","title":"Workflow","text":"<p>Many jobs have specific needs, but there are some best practices that apply in all cases.</p> <ul> <li>Always run a test job (preferably a small sample) to verify storage requirements</li> <li>Always check your quota to ensure sufficient storage space</li> <li>Always read/write data from fastest storage available</li> <li>Always write high I/O output to <code>/scratch</code></li> </ul> <p>For best performance, job files should be located on <code>/scratch</code>. We suggest the following workflow.</p> <ol> <li>User copies job input/supporting files from <code>/group</code> to <code>/scratch</code></li> <li>User submits job that computes with the staged job input/supporting files</li> <li>Job finishes and user copies results from <code>/scratch</code> to <code>/group</code></li> <li>User cleans up files in <code>/scratch</code></li> </ol>"},{"location":"cluster/jobs/troubleshoot/","title":"Troubleshoot Jobs","text":""},{"location":"cluster/jobs/troubleshoot/#troubleshoot-jobs","title":"Troubleshoot Jobs","text":""},{"location":"cluster/jobs/troubleshoot/#overview","title":"Overview","text":"<p>It is often the case in computational work that your job may not do what you expected, or intended. This can be confusing and is a leading source of questions to Research Computing. This guide will explain more about queueing system outcomes and show you how to proactively diagnose the most common issues.</p>"},{"location":"cluster/jobs/troubleshoot/#job-is-not-running","title":"Job is not running","text":"<p>If your job is queued longer than expected, you can find a reason with the <code>squeue -j jobId</code> command, where <code>jobId</code> is the SLURM job number.</p> <pre><code>$ squeue -j 12345\nJOBID   PARTITION   NAME        USER    ST  TIME    NODELIST(REASON)\n12345   normal      testing     NetID   PD  0:00    (QOSMaxJobsPerUserLimit)\n</code></pre> <p>In the example job 12345 the state (ST) is reported pending (PD). The reason is listed last under the NODELIST(REASON) heading (a compute node list is printed if job is running). The reason listed for the example job is QOSMaxJobsPerUserLimit. This translates to a queue (partition) based resource limit for users. It is important to know that the cluster has limits on what resources any single lab group or user can use. These resource limits, along with other cluster policies, help maintain fair utilization of the cluster.</p> <p>So, the example job is in the queue because of the cluster's resource limits. Several scenarios could result in this example. The most likely explanation is that you are already running jobs on the cluster, and you've hit that particular limit. In that case, the best thing to do is wait, and the job will likely start running when your previous jobs finish. However, there are other cases of resource limits that are less obvious. For example, the cluster does limit the number of interactive jobs you can run.</p> <p>The reason listed for our example job is one of several that SLURM provides. Please see the table below for additional examples.</p> Reason Why What to do <code>QOSMaxJobsPerUserLimit</code> You reached the number of running jobs allowed per user for the corresponding partition. You can wait for previous jobs to finish or cancel running jobs. <code>Resources</code> The job is waiting for resources to become available. In many cases you should wait and the job will run. Additionally, try to make sure that you don't request more resources than what you need. <code>Dependency</code> A job dependency is not yet satisfied. You can learn more about job dependencies in our video for creating an advanced SLURM script. <code>Maintenance</code> Your job is blocked until maintenance is finished. You can wait or cancel and resubmit your job according to the job scheduling and maintenance section of our job submission guide. <code>Priority</code> Your job is waiting for higher priority jobs to finish. Please wait and rest assured your job will run in due time. For more info about job priority, please see the job priority section of our job submission guide. <p>When to contact help</p> <p>It is not possible to provide an exhaustive table of scenarios, reasons, and guidance here. When in doubt, please feel free to contact help-rcc@mcw.edu.</p>"},{"location":"cluster/jobs/troubleshoot/#job-failed-immediately","title":"Job failed immediately","text":"<p>Many issues can cause a job to fail immediately. By immediately, we mean the job starts and finishes without producing any useful output. Often there is an error listed in the job output file. The output file is named according to your job name, or can have a specific syntax based on your <code>#SBATCH --output</code> option. Most often we find the job output file is something like <code>slurm-12345.out</code>, where 12345 is the jobId number.</p> <p>Incorrect file names or paths are the most common source of immediate job failure. You will see a specific error in your output file with the syntax <code>command: /file/path: No such file or directory</code>. This indicates that your job is trying to manipulate a file or directory that does not exist. Most often this is a simple typo error, but could also be caused by trying to use files in <code>/group</code>, which is not available on compute nodes. We suggest to double check the file names and paths, and then resubmit. If the issue persists, contact help-rcc@mcw.edu.</p> <p>Storage limits can also cause this issue. Every user has access to at least three storage paths including <code>/home/netId</code>, <code>/group/pi_netId</code>, and <code>/scratch/g/pi_netId</code>. Each of these spaces has a finite limit according to our storage guide. Your job should be using <code>/scratch</code> for input/output and will fail immediately if your scratch quota is 100% full. You can find your available storage paths and quotas with the <code>mydisks</code> command.</p> <pre><code>$ mydisks\n=====My Lab=====\nSize  Used Avail Use% File\n47G   29G   19G  61% /home/netId\n932G  158G  774G  17% /group/pi_netId\n4.6T     0  4.6T   0% /scratch/g/pi_netId\n</code></pre> <p>Finally, if you run jobs in OnDemand often, your home directory will fill with temporary files that are created every time you start an OnDemand job. If you primarily use OnDemand and your apps are failing to start, check your home directory limit with <code>mydisks</code>. If your home directory is full, look for files in <code>/home/netId/ondemand</code>, where <code>netId</code> is your username. You can safely clean out this folder and proceed with a logout/login on OnDemand.</p>"},{"location":"cluster/jobs/troubleshoot/#job-stopped-unexpectedly","title":"Job stopped unexpectedly","text":"<p>Jobs that start and run correctly can still stop or fail unexpectedly for a variety of reasons. Again, the output file is a good starting place and may contain a useful error for diagnosing the issue.</p>"},{"location":"cluster/jobs/troubleshoot/#memory","title":"Memory","text":"<p>A common failure is the job running out of memory. Jobs that fail with memory issues often produce normal, useful output until they stop abruptly. In this case, your job output file might reference an <code>OOM</code> or <code>Out-Of-Memory</code> error.</p> <p>Another way to diagnose memory issues is the <code>seff</code> command.</p> <pre><code>$ seff 250\nJob ID: 250\nCluster: cluster\nUser/Group: user/sg-group\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 5\nCPU Utilized: 00:00:01\nCPU Efficiency: 6.67% of 00:00:15 core-walltime\nJob Wall-clock time: 00:00:03\nMemory Utilized: 0.00 MB (estimated maximum)\nMemory Efficiency: 0.00% of 30.00 GB (30.00 GB/node)\n</code></pre> <p>If your job fails due to memory, the State: output will show OUT_OF_MEMORY. You will also see high memory utilization above what you allocated. To fix a memory issue, try to increase the amount of memory in the job script and re-submit.</p>"},{"location":"cluster/jobs/troubleshoot/#walltime","title":"Walltime","text":"<p>Another common failure of running jobs is a job timeout. Again, you will see an abrupt end to an otherwise well running job. A job can timeout if it tries to run longer than your requested walltime or the max walltime. Walltime tells the cluster how long you expect the job to run, and has a max value of 7 days. You can also use the <code>seff</code> command to diagnose this issue. If a job fails due to timeout, the State: output will show TIMEOUT. To fix a walltime issue, try to increase the walltime or shorten the simulation, and resubmit.</p>"},{"location":"cluster/jobs/troubleshoot/#other-resources","title":"Other resources","text":"<p>Research Computing provides a web portal with all job information called XDMoD. XDMoD collects job accounting data and node level metrics during all cluster jobs. This data can be used for troubleshooting in the event of a failed job. However, XDMoD is only useful for retrospective analysis. It collects and aggregates data once per day, rather in realtime. Jobs that run and finish one day will be available in XDMoD the following day.</p> <p>Please see the XDMoD guide for more info.</p>"},{"location":"cluster/jobs/troubleshoot/#getting-help","title":"Getting help","text":"<p>It is not possible to provide an exhaustive list of scenarios, reasons, and guidance here. When in doubt, please contact help-rcc@mcw.edu for expert help.</p>"},{"location":"cluster/jobs/xdmod/","title":"Job Metrics","text":""},{"location":"cluster/jobs/xdmod/#xdmod","title":"XDMoD","text":""},{"location":"cluster/jobs/xdmod/#overview","title":"Overview","text":"<p>XDMoD (XSEDE Metrics on Demand) is a NSF-funded open source tool that provides a wide range of metrics including utilization and performance of HPC resources. This tool can be used to monitor and troubleshoot your HPC jobs. It can also be used to track and document your RCC utilization. Please note that XDMoD uses your MCW login credentials. If you have not run jobs on RCC's HPC systems, you will not see any data after login. Please contact help-rcc@mcw.edu with questions.</p> <p>Metrics are not real-time</p> <p>XDMoD processes data overnight, rather than in real-time. This allows the application to be very responsive when displaying data. However, this means that job data will not display in XDMoD until the following day. Processing occurs every day at midnight.</p>"},{"location":"cluster/jobs/xdmod/#login","title":"Login","text":"<p>Access RCC's XDMoD site at https://xdmod.rcc.mcw.edu. Click Sign in in the upper left corner.</p> <p></p> <p>A login window will appear. Click the MCW logo.</p> <p></p> <p>You will be redirected to a login page. Enter your MCW credentials.</p> <p></p>"},{"location":"cluster/jobs/xdmod/#user-guide","title":"User Guide","text":"<p>Please see the XDMoD User Guide for details on site features and navigation.</p>"},{"location":"cluster/jobs/xdmod/#example-use-troubleshooting-a-job","title":"Example Use - Troubleshooting a Job","text":"<p>XDMoD collects job accounting data and node level metrics during all cluster jobs. This data can be used for troubleshooting in the event of a crashed job.</p> <p>To view job information in XDMoD:</p> <ol> <li>Select the Job Viewer tab and follow the on screen instructions for job lookup. Once you have selected and saved the job(s) of interest, new folders with the resource and job id appear in the Search History on the left.</li> <li>Select a job id to display job data. The Accounting data includes all information that XDMoD has gathered from the queuing system including job name, username, wait time, wall time, etc.</li> <li>The Executable information will show the job's node name and application if available.</li> <li>If your job shared the node with another job, the Peers tab will show the concurrent jobs (useful when memory issues occur).</li> <li>Summary metrics include the processor, disk, and network usage. Detailed metrics contain all available node level metrics from your job.</li> </ol>"},{"location":"learning/hpc_clusters/","title":"HPC Clusters Tutorials","text":""},{"location":"learning/hpc_clusters/#hpc-clusters-tutorials","title":"HPC Clusters Tutorials","text":"<p>In this section, you'll find links to our comprehensive cluster guides, designed to help you get started with using the system effectively. Additionally, we\u2019ve included helpful resources from other institutions that operate similar clusters, offering supplementary information and best practices.</p> <p>While we encourage you to explore these external guides, we recommend starting with our own documentation, as it is tailored specifically to our cluster. Please note that no two clusters are identical, so not all examples or instructions from other institutions may apply or function as expected in our environment.</p> <p>RCC Cluster Guide: This guide provides the essential steps to get started on the RCC, including instructions on how to write and submit a SLURM job. For more in-depth guidance and answers to advanced questions, you'll also find links to additional resources throughout the document.</p> <p>MCW RCC YouTube Channel: This channel features videos on creating scripts that are optimized for our cluster. We will continue to add content on a variety of topics, including containers, Python for data science, job submission and monitoring, and much more.</p> <p>Princeton  Research Computing: Princeton University operates a cluster that functions similarly to ours, including the use of the SLURM scheduler. Their guide provides a clear and detailed explanation of cluster fundamentals, including how clusters work and other key concepts.</p> <p>Department Of Defense: The Department of Defense operates a cluster that also uses SLURM, and their documentation covers how to create, submit, and monitor jobs. While much of this information is included in our own documentation, you may find additional examples, use cases, and topics in their guide. They also provide valuable insights on managing resources to optimize performance and maximize overall throughput.</p>"},{"location":"learning/learning/","title":"Overview","text":""},{"location":"learning/learning/#learning-tutorials-and-videos","title":"Learning Tutorials and Videos","text":"<p>Welcome to our comprehensive collection of video tutorials designed to guide you through using our HPC cluster. On our YouTube channel, you will find detailed videos covering everything from script writing to executing complex pipelines on the cluster. Whether you are new to high-performance computing or looking to sharpen your skills, these videos will provide you with the knowledge you need to get the most out of our system.</p> <p>Additionally, we\u2019ve curated a selection of external videos and tutorials from other institutions that use similar clusters, offering valuable insights and techniques to enhance your learning.</p> <p>This resource is continually evolving. If you have any suggestions for additional content or topics you'd like us to cover, please feel free to reach out to us at help-rcc@mcw.edu.</p>"},{"location":"learning/programming/","title":"Programming Guides","text":""},{"location":"learning/programming/#programming-guides","title":"Programming Guides","text":""},{"location":"learning/programming/#languages","title":"Languages","text":"<p>Bash and Python are among the most commonly used programming languages in data science. To run your pipelines and analyze data effectively, you'll likely need to learn or enhance your skills in these languages. Markdown is another language commonly used in science to present results and create documentation. The links provided here offer comprehensive guides tailored for both beginners learning the basics and advanced users seeking specific topics and practical how-tos.</p> <p>Bash guide: This guide covers a broad range of topics, from fundamental concepts like file manipulation, arithmetic operations, iteration, condition testing, and working with arrays to more advanced topics such as using databases, managing permissions, and working with remote or cluster resources.</p> <p>Python guide: This guide provides everything you need to know to become a Python programmer capable of analyzing, manipulating, and visualizing large datasets. Key topics widely used in data science are covered, including modules such as NumPy, pandas, SciPy, Django, and the Matplotlib library. You'll also find information on machine learning, working with databases, and much more.</p> <p>Markdown guide: This is a comprehensive guide that covers everything from how does it work and its most common applications to explanations of basic and extended syntax and some useful hacks.</p> <p>HTML guide: In this tutorial you will find anything you need about html. It is targeted to programmers with any level of experience as it covers a very wide range of topics.</p>"},{"location":"learning/programming/#tools","title":"Tools","text":""},{"location":"learning/programming/#ides","title":"IDEs","text":""},{"location":"learning/programming/#programming","title":"Programming","text":"<p>An IDE, or Integrated Development Environment, is an application for software development. It provides an interface with a code editor, a compiler, debugger and other tools that help streamline the coding process.</p> <p>Jupyter Guide: In this guide we explain you how to start using Jupyter in OnDemand, how to use languages other than python within your notebook, visualize documents, images and videos, export variables, and a couple other tips and tricks that will be very useful for beginners and more advanced users.</p> <p>Visual Studio Code: This tutorial provides all the necessary information to get you started with VS Code. You'll learn how to use and customize the user interface, write code with some editing features they provide, running and debugging code, and installing language extensions for different programming languages. To open VSCode in OnDemand, go to Interactive Apps, then click in Code Server and launch the application. Code Server is the web-based version of VS Code.</p> <p>RStudio: This is a very good starting point for someone new to R. You will find information from how to install and use RStudio, to installing and using packages, writing and running scripts, accessing build-in datasets, using R Markdown to do reports and much more. RStudio Server, which is the web-based version of RStudio, can also be found in the Interactive Apps in OnDemand.</p>"},{"location":"learning/programming/#designing-workflows","title":"Designing workflows","text":"<p>Workflow diagrams are very useful in research for designing processing pipelines. They help researchers understand the exact steps that will be taken, which aids to pinpoint potential problems, plan data collection, inform new collaborators, write research proposals and articles, and analyze data. Below is a list of free (or accessible to MCW employees) IDEs for designing workflows and pipelines:</p> <p>draw.io: This tool is very easy to use, free and can be exported in different formats. Additionally, it can be downloaded as an App for most operating systems.</p> <p>Microsoft Visio: Although usually not free, you can access it for free with your MCW Microsoft account. This tool will allow you to create very professional flowcharts that you can share with collaborators or use in your papers, presentations or posters.</p> <p>Canva: Canva has some beautiful templates for flowcharts that you just need to edit but can get you started quick. It has many options and add-ons and can also be downloaded in different formats.</p> <p>ClickUp: This tool will be explained in more detail in the Time management section because it is not only useful for creating diagrams but it can do many things to organize your research projects and team. This is a truly all-in-one tool.</p>"},{"location":"learning/programming/#version-control-tools","title":"Version Control Tools","text":"<p>The most commonly used version control tools by software developers are Git and GitHub. They can be used in research to organize and share scripts, recover from accidental loss of data, create excellent documentation on your projects, showcase your work, and keep track of changes in your code. It is important to note that Git and GitHub are different things even though they work hand by hand. On one hand, Git is a version control tool that can be used outside Github and allows you to track changes in files. On the other hand, GitHub is a web-based platform that hosts repositories and uses Git.</p> <p>Our Beginners Git and GitHub Guide: This guide will take you through the steps on how to configure git, create a new repository and host it on GitHub. It is a good place to start if you have no experience with git.</p> <p>Our Advanced Git and GitHub Guide: This guide covers how to create a GitHub page, use branches to edit your documentation and collaborate with others, and how to resolve merge conflicts and other issues.</p>"},{"location":"learning/programming/#time-management-tools","title":"Time Management Tools","text":"<p>When doing research you often have multiple projects going on, you share data with collaborators, and you have different deadlines for ech of these projects. Keeping track of everything can become difficult and stressful. It is specially hard to prioritize tasks and allocate time effectively in order to meet those deadlines. Time management tools will help you organize your day and your work, making you more productive and reducing stress. There are many available tools that provide the same or similar services for this purpose. However, it is important that you choose one that you feel comfortable with and that fits your way of thinking and organizing your day. Here we mention a few common ones that we suggest you try out.</p> <p>Trello: This tool allows you to organize and track projects or tasks both in your personal and professional life. It's organized in boards, lists and cards. In each card you can add to-do lists, tables, documents, images and much more. You can collaborate with other people, integrate with other tools and automate tasks and workflows. It is available to use in the web as well as through mobile apps.</p> <p>ClickUp: This is a highly customizable, all-in-one platform that includes to-do lists, documents, wikis, spreadsheets, chat, whiteboards, calendar, forms, etc. Moreover, ClickUp integrates with GitHub and offers relational database functionalities, making it a great tool for software development. It has a higher learning curve than others, but can replace most time management tools. It can also integrate with other tools and is available in the web and through a mobile app. You can make your ClickUp wikis public to others by sharing a public link or inviting specific people with view-only permissions.</p> <p>Nuclino: This tool has the user-friendly characteristics of Trello, but is also a multi-purpose tool like ClickUp. It aims to merge the best things of those two platforms. Here you can visualize organize your work in nested lists, boards, tables, or graphs in an intuitive and easy way. You can integrate Nuclino with Google drive, Microsoft and other tools. It is available in the web and through mobile apps and allows you to share your wikis or workspaces with others.</p> <p>Obsidian: This is a platform for taking notes and creating your personal wikis using Markdown syntax. With the Obsidian Canvas, you can visualize all your notes in one place. It also allows you to connect different pages and visualize these relationships through graphs. With the Excalidraw plugin you can also draw and take manual notes using your apple pen if you have an ipad.</p>"},{"location":"learning/slurm/","title":"SLURM Guides","text":""},{"location":"learning/slurm/#slurm-guides","title":"SLURM Guides","text":"<p>This section offers guides tailored to both beginners and experienced SLURM users. Whether you're learning how to write your first SLURM script or looking to optimize your jobs for more efficient resource management, these resources have you covered. you may be surprised by what you can achieve on the cluster with the right approach!</p> <p>Official SLURM guide: On this page, you will find a quick start guide along with a comprehensive list and detailed explanations of all SLURM options and commands for running and monitoring your jobs.</p> <p>Beginners guide to SLURM: This video is an excellent introduction for anyone new to high-performance computing and SLURM. It covers the basic concepts you need to get started.</p>"},{"location":"news/","title":"Recent News","text":""},{"location":"news/#recent-news","title":"Recent News","text":""},{"location":"news/2022/12/01/new-gpu-nodes-available/","title":"New GPU nodes available","text":""},{"location":"news/2022/12/01/new-gpu-nodes-available/#new-gpu-nodes-available","title":"New GPU nodes available","text":"<p>Two new GPU nodes are now available. Each new node has 48 cores, 480GB of memory, 4 A40 GPUs, and a 3.84TB local scratch disk. Along with this addition, we\u2019ve increased account and user limits for GPU utilization.</p> <p>The new GPU nodes are part of the <code>gpu</code> partition. No additional configuration is needed. Please see the SLURM guide for details.</p>"},{"location":"news/2023/07/28/august-maintenance/","title":"August maintenance","text":""},{"location":"news/2023/07/28/august-maintenance/#august-maintenance","title":"August maintenance","text":"<p>RCC maintenance will last from 9PM-12AM on Wednesday, August 2, 2023. Cluster jobs will be blocked during maintenance. Login nodes will be rebooted. Open OnDemand will be rebooted. Access to storage may be disrupted.</p>"},{"location":"news/2023/07/31/job-scheduling-and-maintenance/","title":"Job scheduling and maintenance","text":""},{"location":"news/2023/07/31/job-scheduling-and-maintenance/#job-scheduling-and-maintenance","title":"Job scheduling and maintenance","text":"<p>Why is my job blocked?</p> <p>This is the most common question before cluster maintenance. It has a simple, but hard to explain answer. Since we answer this question often, we wanted to share the answer, and provide an easy tool to help.</p> <p>Some background info...</p> <p>Each job has a walltime request. The walltime request sets the amount of time that your job will be allowed to run on the cluster. On the HPC cluster, the maximum allowed walltime is 7 days.</p> <p>If you submit a job before a maintenance window and the job is sitting in the queue, first check to see why with <code>squeue</code>. In the output, you'll see the last column <code>NODELIST(REASON)</code>. If your job is held for maintenance, you'll see <code>(ReqNodeNotAvail, Reserved for maintenance)</code>.</p> <p>Answering the question...</p> <p>If you submit your job with X hours walltime request, and the maintenance window starts in less than X hours, the job will be held until after the maintenance window closes. The reason is that the scheduler cannot guarantee your job will finish before maintenance begins.</p> <p>Fixing the issue...</p> <p>To make the job run, resubmit your job with a walltime request that ensures the job will finish before the maintenance window. We can use a simple formula to find that walltime.</p> <p><code>walltime = ( $maint_start_time - $current_time )</code></p> <p>To simplify the process, RCC created a new login node command, <code>maxwalltime</code>. This command will solve the above formula, and give you the maximum walltime request that will allow your job to run before maintenance.</p> <p>We hope this helps and stay tuned for more helpful tips and solutions posted here in the News section.</p>"},{"location":"news/2023/08/25/what-is-scratch-space/","title":"What is scratch space?","text":""},{"location":"news/2023/08/25/what-is-scratch-space/#what-is-scratch-space","title":"What is scratch space?","text":"<p>Every RCC user has access to scratch space (i.e., <code>/scratch</code>) on the cluster, but many do not use it properly, or might not understand its purpose. Here we'll talk about scratch space, why it exists, and how to use it properly.</p> <p>Overview</p> <p>Scratch space is traditionally the high-performance storage component in any cluster. It's purpose is to hold temporary files generated by running jobs. Many HPC jobs need to write large files, or many files, which are temporary and only used during the job. These files might include checkpoints (incremental save points), intermediate files, etc. These are files that may be needed to diagnose a failed job, but are not kept long term, and are not integral to publishable results.</p> <p>A common question is why do we separate scratch space and general file storage (i.e., /group)? Many users question why, since this may require copying data between storage, and extra data management steps. The simple answer is that high-performance storage is expensive. If we combined the general purpose storage and scratch space workloads into one storage system, some decisions must be made. For example, if the combined storage is all general purpose, the job workloads could crash or severely affect the slower general purpose storage system. Or if the combined space is all high-performance, general file data (which is most of our data) would be a wasted expense.</p> <p>Why talk about scratch space?</p> <p>Most users do not understand or properly use scratch space. We have 215TB of high-performance scratch space and approximately 50-60% is improperly utilized at any given time. Much of this improper use is old files that are not cleaned up or avoiding additional storage charges in <code>/group</code>.</p> <p>Why is this a problem?</p> <p>RCC staff are constantly having to contact users to ask them to clean-up their scratch space. This takes time and effort, and is not very effective. The scratch space remains unnecessarily full, which means new users will not have space to work. Moreover, existing users are forced to make use of a small amount of space, which hurts productivity.</p> <p>Proper use</p> <p>We have a simple workflow to follow:</p> <ol> <li>User copies job input/supporting files from an RGS directory within /group/{PI_NetID}/... to their scratch directory /scratch/g/{PI_NetID}</li> <li>User submits job that computes with the staged job input/supporting files</li> <li>Job finishes and user copies results from /scratch/g/{PI_NetID} back to /group/{PI_NetID}/...</li> <li>User continues with further computations with the job input/supporting files</li> <li>User finishes computations and deletes unneeded job input/supporting files from /scratch/g/{PI_NetID}</li> </ol> <p>Summary</p> <p>Try to remember that scratch space is shared. Proper use of scratch space benefits all users. So, clean up your scratch space by following a simple rule; when not running jobs, your scratch should be empty!</p>"},{"location":"news/2023/08/29/september-maintenance/","title":"September maintenance","text":""},{"location":"news/2023/08/29/september-maintenance/#september-maintenance","title":"September maintenance","text":"<p>RCC maintenance will last from 9PM-12AM on Wednesday, September 6, 2023. Cluster jobs will be blocked during maintenance. Login nodes will be rebooted. Open OnDemand will be rebooted. Access to storage may be disrupted.</p>"},{"location":"news/2023/09/03/recent-news-now-available-as-rss-feed/","title":"Recent news now available as RSS feed","text":""},{"location":"news/2023/09/03/recent-news-now-available-as-rss-feed/#recent-news-now-available-as-rss-feed","title":"Recent news now available as RSS feed","text":"<p>RCC recently converted to this built-in blog for news and announcements. If you're reading this, we hope you find the new format helpful. If you'd like to receive regular updates as we post new content, please consider following our RSS feed.</p> <p>To follow the feed using your Windows Outlook client, open your Account Settings and the RSS Feeds tab. Select new and enter <code>https://docs.rcc.mcw.edu/feed_rss_created.xml</code>.</p>"},{"location":"news/2023/09/18/new-cluster-software-list-available/","title":"New cluster software list available","text":""},{"location":"news/2023/09/18/new-cluster-software-list-available/#new-cluster-software-list-available","title":"New cluster software list available","text":"<p>We hope you enjoy the new cluster software list. This list will be regularly updated when new software is installed on the cluster. It uses a semi-automated process to read data from our modulefiles, which we then transform to a usable yaml format. We hope to add new lists for other package systems such as R. Please let us know if that is of interest.</p>"},{"location":"news/2023/09/19/updated-data-protection/","title":"Updated data protection","text":""},{"location":"news/2023/09/19/updated-data-protection/#updated-data-protection","title":"Updated data protection","text":"<p>Group storage directories are now protected by additional snapshots. You might be wondering what that is, and how does it help? We answer questions about data protection often, but this particular change is exciting and gives us an opportunity to discuss our file system, data protection, and specifically snapshots.</p>"},{"location":"news/2023/09/19/updated-data-protection/#overview","title":"Overview","text":"<p>A snapshot is a version of the file system data captured at a specific timestamp. This includes information about the file system objects (files and directories), including creation/modification timestamp, permissions, etc. Snapshots are taken at incremental times, and capture changes since the last snapshot timestamp. Using snapshots, we can compare versions of files over time, and possibly recover an older version of a file if needed.</p> <p>The time between snapshots determines the granularity of the information that is captured. For instance, a daily snapshot will capture all changes made in the previous 24-hour period, for files present at the time of the next snapshot. That last point is important. The timing of snapshots will determine what is and is not captured. For example, if you create and delete a file within the same 24-hour period, then the file will not be present for any snapshot. In that case you will not have any snapshot version of that particular file. Keep this in mind when working.</p> <p>Consider additional tools</p> <p>Snapshots may not be the best safety measure for your data types. For instance, when working with code, use a version control system such as git to capture your changes. Do not rely on snapshots to save your work.</p>"},{"location":"news/2023/09/19/updated-data-protection/#updates","title":"Updates","text":"<p>The previous snapshot policy for all lab group and home directories was 7 daily snapshots. That policy captured changes once per day, and kept those captured changes for 7 days.</p> <p>The new snapshot policy captures 14 daily and 6 weekly snapshots. This should provide additional time for users to recover from accidental file operations such as deletions.</p> <p>See the storage guide for additional details.</p>"},{"location":"news/2023/12/13/hpc-cluster-operating-system-upgrade/","title":"HPC Cluster Operating System Upgrade","text":""},{"location":"news/2023/12/13/hpc-cluster-operating-system-upgrade/#hpc-cluster-operating-system-upgrade","title":"HPC Cluster Operating System Upgrade","text":"<p>The HPC cluster operating system, CentOS 7, will be end-of-life in June 2024 and we are beginning a project to upgrade. This will allow us to maintain ongoing security patching and compatibility with new software through the cluster lifetime. To that end, we have chosen to upgrade the HPC cluster to Rocky Linux 8.</p>"},{"location":"news/2024/01/02/changes-to-storage-policies/","title":"Changes to storage policies","text":""},{"location":"news/2024/01/02/changes-to-storage-policies/#changes-to-storage-policies","title":"Changes to storage policies","text":"<p>Research Computing is making policy changes regarding storage organization and retention. This is in response to the increasing needs of users, and represent best practice.</p> <p>Policy updates</p> <ul> <li>User scratch directories (/scratch/u/...) will be deleted</li> <li>Files in scratch that have not been accessed or modified in the last 60 days will be removed daily</li> </ul>"},{"location":"news/2024/01/02/changes-to-storage-policies/#user-scratch-removal","title":"User scratch removal","text":"<p>Research Computing will no longer provide user scratch directories. For reference, these directories have been located at <code>/scratch/u/netid</code>. This is a response to user feedback about confusion over purpose of user vs. group scratch. Additionally, we have found that group scratch is more heavily utilized, and more actively utilized, i.e., less old data. We encourage all users to cleanup their user scratch data immediately. Research Computing will begin deleting these directories on March 6, 2024.</p>"},{"location":"news/2024/01/02/changes-to-storage-policies/#scratch-retention","title":"Scratch retention","text":"<p>Research Computing has had documented guidance for use of scratch storage in the HPC environment since the start of operations for the HPC cluster. This included a data deletion policy for old files in the scratch file system. Historically, we have not enforced this policy in any automated way. In fact, we have only emailed our MCW users to manually cleanup their scratch space when storage ran out. This has led to issues with users storing old data in scratch that is not needed and takes up valuable space meant for new jobs.</p> <p>To fix these issues, Research Computing will begin deleting old scratch data with an automated script. The script will daily scan all files and directories in your scratch space and move any objects that have not been modified or accessed in the last 60 days to a daily trash space. After 14 days, a daily trash space will expire and be deleted. This means that the scratch file system will be cleaned up every day, and you will have the opportunity to retrieve those old files for an additional 14 days.</p> <p>This scratch retention process is effective March 6, 2024. For details, please review the scratch retention information.</p>"},{"location":"news/2024/01/02/changes-to-storage-policies/#next-steps-for-you","title":"Next steps for you","text":"<p>Start cleaning up your scratch directories. If you have data in <code>/scratch/u</code>, move it to your lab group space <code>/group/pi_netid</code>, or <code>/scratch/g/pi_netid</code> if you have more jobs to run.</p> <p>We encourage all users to review the FAQ section below for answers to common questions. All further questions should be submitted to help-rcc@mcw.edu.</p>"},{"location":"news/2024/01/02/changes-to-storage-policies/#faq","title":"FAQ","text":"How do I know the age of my files? <p>To list files in a directory ordered by age from top to bottom:</p> <pre><code>ls -latr\n</code></pre> <p>To find all files in a directory that have not been modified in the last 60 days:</p> <pre><code>find /path/to/directory -mtime +60 \n</code></pre> <p>To find all files in a directory that have not been accessed in the last 60 days:</p> <pre><code>find /path/to/directory -atime +60\n</code></pre> <p>Remember, the scratch file system is not for regular data storage. It is only to be used for storing data while a SLURM job is running. When the job finishes, clean it up. There should never be a time where old data exists in your scratch folder. In this way, you can avoid a situation where you do not know or remember the age of a file.</p> My PI does not have space in their group storage. Where do I store my user scratch data? <p>You should discuss this issue with your PI. Group storage is available with higher quota limits for $60/TB/year. Contact help-rcc@mcw.edu if you require assistance in discussing this issue.</p> <p>Remember, it is best to avoid this situation. Do not use scratch storage as a free general data storage. Do not use the cluster to generate results if you are not willing to store that data either in group storage, or some other lab solution.</p> I am a PI. How do I know if my past/present lab members have data stored in scratch that could be deleted? <p>We suggest to ask those lab members directly. The best person to identify a piece of data is the person that generated it. If you cannot contact that lab member, contact help-rcc@mcw.edu to arrange a consultation.</p> I have data in scratch now. Will my data be deleted? <p>This depends on the age of your data. Any data older than 60 days will be moved to a trash folder for another 14 days. After that time, it will be deleted. Avoid that situation by cleaning up your scratch folder early and often.</p> Is my scratch data backed up before you delete it? <p>No, data in <code>/scratch</code> is not backed up, replicated, snapshotted, or protected by any other means. Again, this is temporary space only for data being processed in a SLURM job. Do not leave your data in scratch where it may be deleted or lost. Clean it up early and often by transferring results to group storage.</p>"},{"location":"news/2024/03/06/storage-policy-change-postponed/","title":"Storage policy change postponed","text":""},{"location":"news/2024/03/06/storage-policy-change-postponed/#storage-policy-change-postponed","title":"Storage policy change postponed","text":"<p>RCC is postponing the scratch storage policy changes that were supposed to take effect today, March 6, 2024 due to technical issues and lack of compliance.</p> <p>We currently have more than 59TB of data in user scratch directories. We also suspect, based on sampling, that much of the data in group scratch folders has not been accessed or modified in more than 60 days. This means that, according to these policy changes, RCC would have deleted at least 59TB of data, and probably closer to 100TB. We do not wish to delete ongoing research data and cause harm to any individual\u2019s research program. Therefore, RCC will postpone these policy changes.</p> <p>We will communicate further updates and a new schedule very soon. In the meantime, we require that you work towards compliance with these policies. This means that user scratch directories should be emptied, and any group scratch files not modified or accessed in previous 60 days should be deleted. Compliance with these policies is vitally important so that we can continue to support new initiatives and workloads within the RCC environment.</p>"},{"location":"news/2024/04/02/april-maintenance/","title":"April maintenance","text":""},{"location":"news/2024/04/02/april-maintenance/#april-maintenance","title":"April maintenance","text":"<p>RCC maintenance will last from 9PM-12AM on Wednesday, April 3, 2024. Cluster jobs will be blocked during maintenance. Login nodes will be rebooted. Open OnDemand will be rebooted. Access to storage may be disrupted. Most login and compute nodes will be upgraded to Rocky Linux 8.</p> <p>Potentially breaking change</p> <p>RCC will upgrade a majority of login and cluster nodes to Rocky Linux 8. If you have not participated in the first phase test, you are at risk of failing, or inaccurate jobs. We will continue to maintain a login node and small number of cluster nodes on the legacy CentOS 7 operating system. This will ensure backwards compatibility for those users that have not fixed software issues prior to upgrade. Contact help-rcc@mcw.edu with questions.</p>"},{"location":"news/2024/04/04/cluster-operating-system-upgrade/","title":"Cluster operating system upgrade","text":""},{"location":"news/2024/04/04/cluster-operating-system-upgrade/#cluster-operating-system-upgrade","title":"Cluster operating system upgrade","text":"<p>RCC is entering the second phase of a planned upgrade of the cluster operating system to Rocky Linux 8. Last night, the majority of login and cluster nodes were upgraded and now run Rocky Linux 8. A single login node and select compute nodes running CentOS 7 are still available via special queues. This will ensure backwards compatibility for those users that have not fixed software issues prior to upgrade. Please note, if you did not participate in the first phase test, your jobs may be at risk of failing, or inaccuracy.  Contact help-rcc@mcw.edu with questions.</p>"},{"location":"news/2024/04/24/final-cluster-os-upgrade-phase/","title":"Final cluster OS upgrade phase","text":""},{"location":"news/2024/04/24/final-cluster-os-upgrade-phase/#final-cluster-os-upgrade-phase","title":"Final cluster OS upgrade phase","text":"<p>RCC is in the second phase of a plan to upgrade the cluster operating system to Rocky Linux 8. We have answered a number of great questions and fixed a number of issues since the upgrade. We have also seen several weeks of stability, and so have decided to end this phase earlier than planned. To that end, RCC will upgrade the remaining systems on May 1. Please send any questions/concerns to help-rcc@mcw.edu.</p>"},{"location":"news/2024/05/01/cluster-os-upgrade-complete/","title":"Cluster OS upgrade complete","text":""},{"location":"news/2024/05/01/cluster-os-upgrade-complete/#cluster-os-upgrade-complete","title":"Cluster OS upgrade complete","text":"<p>RCC upgraded the cluster operating system to Rocky Linux 8 effect May 1, 2024. Please send questions/concerns to help-rcc@mcw.edu.</p>"},{"location":"news/2024/10/15/storage-price-increase-in-2025/","title":"Storage price increase in 2025","text":""},{"location":"news/2024/10/15/storage-price-increase-in-2025/#storage-price-increase-in-2025","title":"Storage price increase in 2025","text":"<p>Starting in 2025, the price for group storage will increase to $80/TB/year. However, because of the short notice, this new rate will apply only to new storage subscription payments made in 2025. All existing storage renewals processed in January 2025 will remain at the current rate of $60/TB/year.</p> <p>In FY22, RCC added replication storage to provide vital data protections for all group storage directories. This enhancement effectively doubled the cost of storage. At that time, RCC chose not to increase price until the end of the original storage lifetime. Now, as we work to renew and expand storage, we must adjust pricing to maintain a sustainable service for research. The new pricing structure will be implemented 2025-2027 as follows:</p> <p>2025 - $80/TB/year 2026 - $100/TB/year 2027 - $120/TB/year</p> <p>We understand that some investigators may question if replication can be optional to reduce cost. However, due to the nature of our storage solution, replication must be universal and cannot be optional. Research Computing will re-evaluate storage pricing again in FY27 to account for changing needs and functionality.</p> <p>Summary:</p> <ul> <li>Storage renewals in January 2025 will be at the current rate of $60/TB/year.</li> <li>All new (non-renewal) storage subscriptions in 2025 will be charged at the 2025 rate of $80/TB/year.</li> <li>Starting in 2026, both renewals and new subscriptions will be charged at the respective yearly rates shown above.</li> </ul> <p>Please contact help-rcc@mcw.edu with any questions/concerns.</p>"},{"location":"news/2024/11/13/expanded-storage-capacity/","title":"Expanded storage capacity","text":""},{"location":"news/2024/11/13/expanded-storage-capacity/#expanded-storage-capacity","title":"Expanded storage capacity","text":"<p>To meet growing storage needs, Research Computing recently expanded the <code>/group</code> file system. The total replicated capacity is now 2.6PB.</p>"},{"location":"news/2025/06/04/new-gpu-nodes-available/","title":"New GPU nodes available","text":""},{"location":"news/2025/06/04/new-gpu-nodes-available/#new-gpu-nodes-available","title":"New GPU nodes available","text":"<p>Two new GPU nodes are now available. Each new node has 128 cores, 750 GB of memory, 8 L40S GPUs, and a 7 TB local scratch disk. The new GPU nodes are part of the <code>gpu</code> partition and no special job configuration is needed. Please see the updated hardware and SLURM guides for details.</p>"},{"location":"secure-computing/mfa-data-transfer/","title":"Data Transfer","text":""},{"location":"secure-computing/mfa-data-transfer/#data-transfer","title":"Data Transfer","text":"<p>Transferring data to your ResHPC storage may be done via ResHPC OnDemand, or a multi-factor compatible SFTP client such as MobaXterm. For large data transfers, we recommend to use MobaXterm.</p>"},{"location":"secure-computing/mfa-data-transfer/#reshpc-ondemand","title":"ResHPC OnDemand","text":"<p>If you're new to using a cluster, ResHPC OnDemand offers web browser-based access to manage files. Please see ResHPC OnDemand for details.</p>"},{"location":"secure-computing/mfa-data-transfer/#mobaxterm","title":"MobaXterm","text":"<p>To enable MFA with MobaXterm SFTP client, please see the MobaXterm guide.</p>"},{"location":"secure-computing/mfa-login/","title":"Logging in","text":""},{"location":"secure-computing/mfa-login/#logging-in","title":"Logging in","text":"<p>ResHPC access requires a project user account, which is based on your MCW NetID and password. Your project username will be a combination of your NetID and a project identifier (example, jsmith.p1234). Your password and Duo MFA credentials are the same that you use to access Citrix and other MCW resources.</p> <p>Network Access</p> <p>ResHPC is available from Citrix and MCW-managed machines.</p>"},{"location":"secure-computing/mfa-login/#reshpc-ondemand","title":"ResHPC OnDemand","text":"<p>If you're new to using a cluster, ResHPC OnDemand offers web browser-based access. You can manage files and start a remote desktop session on the cluster. All of this is possible without logging in via a traditional SSH terminal.</p> <p>To login, point your browser to https://ood-reshpc.rcc.mcw.edu, enter your password, select a Duo MFA option, and complete the login.</p> <p>Additional info</p> <p>Much of the functionality of ResHPC OnDemand is shared with the standard Open OnDemand site. Please consult the OnDemand app guides for additional help and detail.</p>"},{"location":"secure-computing/mfa-login/#ssh-connection","title":"SSH Connection","text":"<p>ResHPC login requires Duo MFA. For Windows users, we recommend a multi-factor compatible terminal client such as MobaXterm. Mac and Linux users may use their OS provided terminal clients.</p> <p>To login, use the ResHPC login hostname <code>login-reshpc.rcc.mcw.edu</code>, enter your password, select a Duo MFA option, and complete the login process.</p> <p>Example login flow:</p> <pre><code>$ ssh jsmith.p1234@login-reshpc.rcc.mcw.edu\n(jsmith.p1234@login-reshpc.rcc.mcw.edu) Password:\n(jsmith.p1234@login-reshpc.rcc.mcw.edu) Duo two-factor login for jsmith\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-XXXX\n 2. Phone call to XXX-XXX-XXXX\n 3. SMS passcodes to XXX-XXX-XXXX\n\nPasscode or option (1-3):\n</code></pre>"},{"location":"secure-computing/project-account/","title":"Accounts & Projects","text":""},{"location":"secure-computing/project-account/#accounts-projects","title":"Accounts &amp; Projects","text":"<p>ResHPC access is based on projects. Each project corresponds to a protected dataset with a unique Data Use Agreement (DUA). Access is determined by the original DUA. Please note that only a MCW PI may establish a ResHPC project.</p> <p>one DUA, one project</p> <p>Each ResHPC project is specific to a single DUA. If you have more than one DUA, you will need a separate project for each.</p>"},{"location":"secure-computing/project-account/#project-id","title":"Project ID","text":"<p>Each ResHPC project is identified within the environment by a unique project ID beginning with the letter p followed by a unique 4-digit code.</p>"},{"location":"secure-computing/project-account/#project-user-account","title":"Project User Account","text":"<p>ResHPC access requires a project user account, which is an extension of your MCW NetID (username) account. For example, you may be a member of project p1234, with project user account jsmith.p1234. For login, the project user account uses the same password and Duo 2FA authentication as the MCW NetID (jsmith in our example).</p> <p>Multiple users may have access to the same project, each with their own unique project user account (i.e. jsmith.p1234, bstevens.p1234, mbradley.p1234). A MCW investigator may also be a member of multiple projects, with multiple corresponding project user accounts (i.e. jsmith.p1234, jsmith.p2345, jsmith.p3456).</p>"},{"location":"secure-computing/project-account/#project-access","title":"Project Access","text":"<p>Project access and accounts are based on the project's DUA. Access will only be granted to users that are part of the DUA.</p> <p>To add a user to a project, first you must amend the DUA to include this user. Consult the data provider and/or MCW Grants &amp; Contracts for details.</p>"},{"location":"secure-computing/project-account/#getting-an-account","title":"Getting an Account","text":"<p>We strongly suggest to email help-rcc@mcw.edu prior to requesting or signing a DUA for a new dataset. Research Computing will not guarantee that ResHPC is suitable for your dataset.</p> <p>If you're ready to proceed, use the link below.</p> <p>Request a ResHPC Project Account</p>"},{"location":"secure-computing/project-storage/","title":"Storage","text":""},{"location":"secure-computing/project-storage/#project-storage","title":"Project Storage","text":"<p>Each ResHPC project has project-specific storage directories in <code>/group/PINetID</code> and <code>/scratch/g/PINetID</code>, secured with a project-specific security group. Please note that ResHPC users do not have user scratch directories.</p> <p>For example, jsmith is PI for project p1234, with project directories <code>/group/jsmith/p1234</code> and <code>/scratch/g/jsmith/p1234</code>.</p> You can easily find your available project storage and current utilization with the <code>mydisks</code> command. <pre><code>$ mydisks\n=====My Storage=====\n Size  Used Avail Use% File\n 4.7G     0  4.7G   0% /home/user.p1234\n 932G  158G  774G  17% /group/PINetID/p1234\n 4.6T   62G  4.5T   2% /scratch/g/PINetID/p1234\n</code></pre>"},{"location":"secure-computing/project-storage/#encryption-requirement","title":"Encryption Requirement","text":"<p>Where noted below, encryption is required. This is an important step in ensuring security and integrity of restricted datasets.</p>"},{"location":"secure-computing/project-storage/#project-directory","title":"Project Directory","text":"<p>Quota: inherited from /group Snapshot: 14 daily @ 2PM, 6 weekly @ 3PM Sunday Replication: continuous with snapshot</p> Each project has a unique directory located at <code>/group/{PINetID}/{ProjID}</code>. Within each project's <code>/group</code> directory, source and results data must be kept separate. You will find the following spaces in your <code>/group/{PINetID}/{ProjID}</code> directory: <p><code>source</code> - used for protected dataset from data provider</p> <p><code>results</code> - used for derivative results data and anything non-source</p> <p>Users must keep source and results separate. Most DUAs are time limited and Research Computing may be required to delete the restricted source dataset at project closeout. Keeping source and results separate will avoid forcing Research Computing to delete all of your project data.</p> <p>Encryption required</p> <p>Restricted source data must be encrypted, and remain encrypted when stored in the project's group directory.</p>"},{"location":"secure-computing/project-storage/#home-directory","title":"Home Directory","text":"<p>Quota: 5 GB Snapshot: 14 daily @ 12PM, 6 weekly @ 1PM Sunday Replication: continuous with snapshot</p> <p>Every project user account has a home directory located at <code>/home/{NetID}.{ProjID}</code>. The home directory is a requirement of hte cluster's Linux operating system. You should not store data in your home directory.</p> <p>Restricted data</p> <p>Storing restricted data in your home directory is prohibited.</p>"},{"location":"secure-computing/project-storage/#scratch-directory","title":"Scratch Directory","text":"<p>Quota: inherited from /scratch Purge: files may be deleted by admin after 90 days</p> <p>Each project has a unique scratch directory located at <code>/scratch/g/{PINetID}/{ProjID}</code>. Please note, you must delete all data in your project scratch when your job is done.</p> <p>Encryption required</p> <p>Restricted source data may only be decrypted in the project's scratch directory, and only as needed for analysis. Do not leave unencrypted data in project's scratch directory. For example, if you're going on vacation, delete the unencrypted data.</p>"},{"location":"secure-computing/reshpc/","title":"Overview","text":""},{"location":"secure-computing/reshpc/#reshpc-overview","title":"ResHPC Overview","text":"<p>Restricted HPC (ResHPC) is a secure way to access and utilize the HPC cluster. It is specifically designed for restricted datasets that have a defined Data Use Agreement (DUA). The ResHPC service is built on the existing HPC cluster, but incorporates a separate, secure login method, and project specific accounts and directories. With ResHPC, you can work with familiar tools while also satisfying complex data provider requirements.</p> <p>NIH dbGaP security changes</p> <p>NIH has updated the security requirements in the Genomic Data Sharing (GDS) policy. Effective January 25, 2025, new or renewed dbGaP projects require NIST 800-171 security compliance. Please note, ResHPC is not NIST 800-171 compliant, and may not be used for dbGaP projects approved on or after January 25, 2025. Projects approved prior to January 25, 2025, utilize the old security policy and may continue on ResHPC until project close-out or renewal. There is no plan to make ResHPC NIST 800-171 compliant.</p>"},{"location":"secure-computing/reshpc/#project-eligibility","title":"Project Eligibility","text":"<p>ResHPC is reserved for projects that need elevated security. Research Computing will evaluate every project request to ensure that ResHPC is required and meets security requirements. For projects with known security requirements, this process is simple and fast. For more complex projects, Research Computing will work with you and may require additional information and discussion. Please note that ResHPC does not meet the security standards for dbGaP or HIPAA compliance.</p> <p>We strongly suggest to email help-rcc@mcw.edu prior to requesting or signing a DUA for a new dataset. Research Computing will not guarantee that ResHPC is suitable for your dataset.</p>"},{"location":"secure-computing/reshpc/#training","title":"Training","text":"<p>ResHPC security training is mandatory for every user. Training is provided by appointment via Zoom. Projects with multiple users may schedule group training. ResHPC access will not be granted until completion of training.</p>"},{"location":"secure-computing/reshpc/#compliance","title":"Compliance","text":"<p>It is part of your role as data steward to comply with any DUA or related policies. Please note that utilizing ResHPC may not satisfy your compliance requirements. Research Computing is not responsible for your compliance.</p> <p>Read and understand policies and procedures.</p> <p>Failure to understand and adhere to your DUA, and applicable policy, could result in loss of data access and other punitive measures as determined by the institution.</p>"},{"location":"secure-computing/reshpc/#support","title":"Support","text":"<p>Please email questions to help-rcc@mcw.edu or attend regular office hours.</p> <p>Do not share restricted information</p> <p>Please avoid sharing restricted information in your help requests.</p>"},{"location":"secure-computing/software/","title":"Software","text":""},{"location":"secure-computing/software/#software","title":"Software","text":"<p>ResHPC uses the same software stack and module system as the HPC cluster. Users may request software by email to help-rcc@mcw.edu. A key difference is that ResHPC users are prohibited from installing their own software.</p> <p>For details on using modules, please see the modules guide.</p>"},{"location":"secure-computing/workflow/","title":"Run a Job","text":""},{"location":"secure-computing/workflow/#running-jobs-with-restricted-data","title":"Running Jobs with Restricted Data","text":"<p>Job submission and management uses the same commands and syntax as the HPC cluster. The same partitions (queues), fairshare, and resource limits apply to jobs.  However, there are some key differences in workflow when using restricted datasets.</p> <p>Here we discuss running jobs, unique workflow features for ResHPC, and references to general documentation.</p>"},{"location":"secure-computing/workflow/#submitting-jobs","title":"Submitting Jobs","text":"<p>Job submission and management uses the same commands and syntax as the HPC cluster. For details on submitting and managing SLURM jobs, please see Submitting SLURM Jobs.</p>"},{"location":"secure-computing/workflow/#encrypting-restricted-data","title":"Encrypting Restricted Data","text":"<p>Restricted source data must be encrypted, and remain encrypted when stored in the project's <code>/group</code> directory. Source data may only be decrypted after it is copied to the project's <code>/scratch</code> directory, and only as needed for analysis.</p> <p>Please note that many restricted datasets will be delivered to you in an encrypted format. Some genomics software can work directly with these encrypted files. However, there are a variety of encryption programs available if needed.</p> <p>Please contact help-rcc@mcw.edu with questions.</p>"},{"location":"secure-computing/workflow/#data-staging-and-workflow","title":"Data Staging and Workflow","text":"<p>Staging data for a job is very similar to using the HPC cluster. A key difference is the need to encrypt restricted data when not in use.</p> <ol> <li>Copy encrypted data from project's <code>/group</code> source directory to <code>/scratch</code> directory.</li> <li>Decrypt files in project's <code>/scratch</code> directory and submits jobs that use the unencrypted data.</li> <li>Run jobs and copy results from <code>/scratch</code> directory back to project's <code>/group</code> results directory.</li> <li>Continue with further computations using the unencrypted data in project's <code>/scratch</code> directory.</li> <li>Finish workflow and delete files from project's <code>/scratch</code> directory.</li> </ol> <p>Warning</p> <p>Do not leave unencrypted data in project's <code>/scratch</code> directory. For example, if you're going on vacation, delete the unencrypted data.</p>"},{"location":"software/R/","title":"R","text":""},{"location":"software/R/#r","title":"R","text":"<p>R is a language and environment for statistical computing and graphics. The cluster has multiple versions installed with a variety of commonly used packages pre-installed.</p>"},{"location":"software/R/#package-installation","title":"Package Installation","text":"<p>R uses a central package library that contains many common packages. The location of this library is <code>$R_HOME/library</code>. Users may also install their own packages locally. The default location for local package installation is <code>$HOME/R/x86_64-pc-linux-gnu-library/4.5</code>.</p> <p>Check installed package list first!</p> <p>Your package installation command will not check the centrally installed packages. You should always check if your package is already installed before proceeding. RCC provides an up-to-date list of installed R packages for the most recent version.</p>"},{"location":"software/R/#user-package-install","title":"User Package Install","text":"<p>First load the R module:</p> <pre><code>module load R\n</code></pre> <p>Launch R:</p> <pre><code>R\n</code></pre> <p>Run the install command:</p> <pre><code>&gt; install.packages('SomePkg')\n</code></pre> <p>The first attempt will warn about writing to the central library. It will ask you to create and use a personal library. Answer yes to both.</p> <pre><code>Installing package into \u2018/hpc/apps/R/4.5.0/lib64/R/library\u2019\n(as \u2018lib\u2019 is unspecified)\nWarning in install.packages(\"ggplot2\") :\n  'lib = \"/hpc/apps/R/4.5.0/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel) yes\nWould you like to create a personal library\n\u2018~/R/x86_64-pc-linux-gnu-library/4.5\u2019\nto install packages into? (yes/No/cancel) yes\n</code></pre> <p>Your package will be installed to your home directory. This package can be removed or updated using the standard R commands.</p> <p>Did you check the installed package list first?</p> <p>Your package installation command will not check the centrally installed packages. You should always check if your package is already installed before proceeding. RCC provides an up-to-date list of installed R packages for the most recent version.</p>"},{"location":"software/R/#request-package-install","title":"Request Package Install","text":"<p>Some packages require system libraries and/or advanced dependencies in order to install correctly. If you see errors when you're installing a package, send a package install request to help-rcc@mcw.edu and RCC will update the central library.</p>"},{"location":"software/R/#running-r-jobs","title":"Running R Jobs","text":"<p>R can be run in batch or interactive jobs. Please do not run long or resource intensive R scripts on the login node.  </p>"},{"location":"software/R/#small-interactive-jobs","title":"Small Interactive Jobs","text":"<p>Small interactive jobs include light plotting, simple analysis of small data sets, etc. These jobs never take more than one core, a few GB of memory, and never last more than a few minutes. These small, fast jobs are allowed on a  login node. However, use caution when running these jobs and double-check that they will not use larger resources. If you need to run an interactive R workflow that is more resource intensive, please use an interactive cluster job.</p> <p>To get an interactive session to run R on the cluster:</p> <pre><code>srun --ntasks=1 --mem-per-cpu=4GB --time=01:00:00 --job-name=interactive --pty bash\n</code></pre>"},{"location":"software/R/#multi-core-jobs","title":"Multi-core Jobs","text":"<p>Multi-core jobs should be run on the cluster compute nodes using the Torque queuing system. There are several options for running these jobs in Torque, including the Rscript command and the BatchJobs library. Both methods interface R with Torque, however, their use cases are different. The Rscript command should be used when you have written an R program .r file and would like to run this script on the cluster. The BatchJobs library should be used when you would like to test individual functions in a semi-interactive way and submit this work to the cluster.</p> <p>Example SLURM submission script:</p> R-test.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=R-test\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:01:00\n#SBATCH --output=%x-%j.out\n\nmodule load R/4.5.0\n\nRscript Rtest.r  \n</code></pre> <p>Submit the job:</p> <pre><code>sbatch myRtest.sh\n</code></pre>"},{"location":"software/R/#help","title":"Help","text":"<p>If you have questions about running R on the HPC cluster, please contact help-rcc@mcw.edu for assistance.</p>"},{"location":"software/advanced_git/","title":"Git and GitHub (advanced)","text":""},{"location":"software/advanced_git/#git-and-github-advanced","title":"Git and GitHub (advanced)","text":"<p>In this guide you will learn how to create a GitHub page to host lab documentation and share scripts, work with branches and resolve merge conflicts. In order to follow this guide, it is a pre-requisite to have some basic knowledge in Git and GitHub. If you are new to these tools, please visit our Beginners guide.</p>"},{"location":"software/advanced_git/#github-pages","title":"GitHub Pages","text":"<p>We will start by creating a new repository to host the documentation for a research project. Log into the account that you created for your laboratory and go to create new repository. Choose a suitable name and make sure it is Public. Visibility must be public for a GitHub Page. Normal repositories, used only for storing scripts and other files, can be private. It is a good idea to always add a README file when creating a new repository, at minimum saying what is the purpose of the repository, but this is not mandatory. Adding a description is also optional. You can choose to add a license to inform people what they can or cannot do with your code. Finally, click on \"Create repository\" at the bottom of the page.</p> <p></p> <p>Now you can clone your repository in your local computer. The URL of your project will be of the form <code>https://github.com/username/repo_name.git</code>.</p> <p>You can do that in the command line:</p> <pre><code>git clone https://github.com/mcw-rcc/DTI_PIPELINE.git\n</code></pre> <p>Or in GitHub Desktop. Here you can select the URL tab and paste the URL of your GitHub repository or choose the GitHub.com tab and paste only the project name (in our example that would be <code>DTI_PIPELINE</code>):</p> <p></p> <p>Next, create an index.md or index.html file and any other files that will be part of the web page for your lab. Below is an example of what a very simple index.html file could look like. If you are new to Markdown or HTML, visit our programming guide section where we have links to very good programming tutorials for different languages.</p> index.htmlindex.md <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;title&gt;DTI pipeline&lt;/title&gt;\n&lt;body&gt;\n&lt;h2&gt;DTI pipeline&lt;/h2&gt;\n&lt;p&gt;In this page you will find information about our state of the art DTI processing pipeline.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <pre><code># DTI pipeline\n\nIn this page you will find information about our state of the art DTI processing pipeline.\n</code></pre> <p>To update the remote repository with the new file, we first add <code>index.html</code> to the Staging Environment using <code>git add</code>. This is where Git stores the list of changes that you want to commit. You can add or remove things from this environment before doing a commit:</p> <pre><code># Add all modified files to the Staging Environment\ngit add --all\n# Commit all changes in the Staging Environment\ngit commit -m \"created index file\"\n# This will synchronize the remote repository with the local changes\ngit push origin\n</code></pre> <p>Finally, enable GitHub Pages for the repository. Go to the Pages Settings of your repository: <code>https://github.com/username/repo_name/settings/pages</code>. Then, in Build and deployment, under Source, select Deploy from a branch. And under Branch, select the main branch. Click on Save.</p> <p></p> <p>your site should be live at <code>https://username.github.io/repo_name</code>. Repeat this process for any individual projects.</p> <p>Now you can create the main page for your lab and add links to the individual project pages. The process is the same as described above except that the name of the repository must be the same as your username. This main page will be hosted at <code>https://username.github.io</code>.</p>"},{"location":"software/advanced_git/#git-branches","title":"Git Branches","text":"<p>Git branches allow you to create isolated spaces to develop new features, modify code or fix bugs without affecting your main line of development. When the code in the new branch is stable and finished, you can merge it with the main branch. Each repository can have multiple branches and you can switch between them easily. Remember that the <code>git</code> commands that you execute will modify the repository in which you are located. To change repository, navigate to the folder of the corresponding repository. When you commit to a branch, those changes will not affect the main branch nor will be made public until you merge the two branches.</p> <p></p>"},{"location":"software/advanced_git/#updating-local-repository","title":"Updating local repository","text":"<p>In order to avoid potential merge conflicts, download the latest changes from the remote repository before creating a new branch. <code>git fetch</code> will not integrate those changes with your local repository, nor will modify your local files. It will just give you a view of what has happened in your the repository. <code>git status</code> will allow you to visualize the status of the local repository compared to the remote.</p> <pre><code>git fetch origin\ngit status\n</code></pre> <p>If your main branch is up to date with <code>origin/master</code> (the remote repository in GitHub), or if it's a few commits ahead but no commits behind, you can safely create a new branch and proceed with the instructions on how to create a new branch. If you are a few commits behind the remote branch, you can try to <code>pull</code> all changes from the remote repository in order to update the local. If there are no conflicts, your branch will be merged with the remote (locally) and you can now safely create a new branch and do any necessary edits to your code:</p> <pre><code>git pull origin\n</code></pre> <p>If there are any conflicts, you must resolve them manually before merging or creating a new branch. Please see the section below on how to resolve conflicts.</p>"},{"location":"software/advanced_git/#creating-a-new-branch","title":"Creating a new branch","text":"<pre><code># Create and switch to a new branch called test\ngit checkout -b test\n</code></pre>"},{"location":"software/advanced_git/#updating-remote-repository","title":"Updating remote repository","text":"<p>After editing your files, you can use <code>git status</code> to see the list of files that have been deleted, created or modified but not yet committed. At this point all changes are local and have not been updated in your remote repository. You might see a list of untracked files, these are those that are new (since the latest commit) and haven't been added yet to your remote repository. In order to commit any changes, you first must add them to the Staging Environment using <code>git add</code> and then commit everything that is in this environment. The Staging Environment tells Git what you want to commit. If you want to add all modified files to the Staging Environment, you should use <code>git add --all</code> as in the example below. To only add specific file(s) or directories, you list each of them like so: <code>git add myfile1.txt myfile2.txt mydirectory/</code>. If you change your mind and decide to remove one of those files from the Staging Environment before a commit, you can use <code>git reset HEAD myfile.txt</code>.</p> <pre><code># Add all modified files to the Staging Environment\n$ git add --all\n# Commit changes to your new branch\n$ git commit -m \"Added new functionality to the brain extraction script\"\n# Push your commit from the local repository to the remote\n$ git push origin test\n</code></pre> <p>If you get an authentication error, make sure that your personal access token is not expired. After successfully running the <code>push</code> command, your new branch is updated in the remote repository, but it's not yet merged with the main code.</p>"},{"location":"software/advanced_git/#merging-branches","title":"Merging branches","text":"<p>If the repository belongs to you and you don't need to have anybody review the changes that you made, you are now ready to merge your local and remote branches and publish your updates.</p> <pre><code># Switch to the main branch\ngit checkout main\n# Merge with test\ngit merge test\n# Delete test\ngit branch -d test\n# Check the status of the local repository compared to the remote\ngit status\n# Push the commit to synchronize the repositories\ngit push origin\n</code></pre> <p>On the other hand, if the repository doesn't belong to you or you have collaborators and you need them to review the changes, you should create a pull request. You can do this by following the link printed when you executed the <code>git push origin test</code> command. The link will have the following format, where <code>test</code> is the name of your working branch: <code>https://github.com/username/repo_name/pull/new/test</code>.</p>"},{"location":"software/advanced_git/#merge-conflicts","title":"Merge conflicts","text":"<p>You must resolve all conflicts before you can successfully merge your branch with the base branch.</p>"},{"location":"software/advanced_git/#manual-fix","title":"Manual fix","text":"<p>You can use <code>git status</code> to see the list of files that have conflicts. If only a few files have conflicts and they arose because two developers did different edits on the same file, fixing those conflicts is relatively easy. Open each of those files in any editor and you will see the lines that have conflict. Between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> and <code>=======</code> you will see the changes made in the base branch and between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; other_branch</code> you will see the changes made in <code>other_branch</code>. Delete those conflict markers and make the final changes that you want to keep.</p> <p>Then, add the changes to the Staging Environment, commit your changes, and synchronize the local and remote repositories:</p> <pre><code>git add .\ngit commit -m \"Resolved merge conflict\"\ngit push origin\n</code></pre> <p>If the conflict arose because a file in one of the two branches was edited and in the other was removed, the solution is also simple. If you want to keep the files, add it back to your repository:</p> <pre><code>git add deleted_file.md\ngit commit -m \"Resolved merge conflict by adding the file that was deleted\"\ngit push origin\n</code></pre> <p>If you decide to remove the file from the repository:</p> <pre><code>git rm deleted_file.md\ngit commit -m \"Resolved merge conflict by deleted the conflicting file\"\ngit push origin\n</code></pre>"},{"location":"software/advanced_git/#rebasing","title":"Rebasing","text":"<p>If the main branch progressed since you started working on your current branch, you will be some commits ahead and some commits behind the main branch. In this case, it will be more complicated to fix the conflicts using the method above and you will need to rebase your branch. This will allow you to obtain the latest changes done to the main branch, while keeping the ones you did in your current branch. Rebasing is moving the starting point of the commits in your new branch, so that the base of those changes is an updated version of the main branch.</p> <p></p> <p>In order to rebase your new branch and put all the commits to the head of <code>origin</code> (the remote main branch), you would type the following commands. <code>git branch</code> shows the branch that is activated with an asterisk. it is important to make sure that the correct branch is activated (the one you want to rebase, in this example <code>test</code>), before doing the rebase.</p> <pre><code>git branch\ngit rebase origin\n</code></pre> <p>You can rebase to any other branch, not necessarily has to be <code>origin</code>. It is not suggested to rebase if you already pushed your changes because it will modify your commit history. In that case, you will have to undo some of the changes you made, separate your branch in one or more branches. or do an interactive rebasing.</p>"},{"location":"software/advanced_git/#other-options","title":"Other options","text":"<p>As merge conflicts get more complicated to resolve, you might need to reset, checkout or revert some of the changes and commits that you did in your branch.</p> <p>Moving some uncommited changes to a new branch is another option that would allow you to merge some of the changes that you made and separate the more complicated conflicts into a different branch.</p> <p>Finally, to investigate further all conflicts, you can check the list of commits in the repository's history.</p>"},{"location":"software/advanced_git/#professional-documentation","title":"Professional documentation","text":"<p>You can create a professional static site like this one to host your documentation using MkDocs and MkDocs Material. These will allow you to use customizable themes with advanced navigation and search options.</p>"},{"location":"software/alphafold/","title":"AlphaFold","text":""},{"location":"software/alphafold/#alphafold","title":"AlphaFold","text":"<p>AlphaFold is an AI system developed by Google DeepMind that predicts a protein\u2019s 3D structure from its amino acid sequence.</p>"},{"location":"software/alphafold/#alphafold-2","title":"AlphaFold 2","text":"<p>The AlphaFold 2 workflow has two parts, MSA search which is CPU based, and inference/MD which is GPU based. To maximize resources, split your workflow into two jobs.</p>"},{"location":"software/alphafold/#interactive-job","title":"Interactive Job","text":"<p>You should only use interactive jobs for testing/debugging.  Please submit batch jobs otherwise.</p> <p>Start a CPU job for MSA:</p> <pre><code># start interactive job\nsrun --job-name=alphafold_test --ntasks=1 --cpus-per-task=12 --time=8:00:00 --pty bash\n\n# load required modules on the gpu node\nmodule load alphafold/2.3.2\nmodule load python/3.9.1\n\n# start alphafold software, replace PI_NetID with your username\nrun_alphafold.py -d $DOWNLOAD_DIR --cpus 12 -f /scratch/g/PI_NetID/alphatest/some.fasta -t 2020-05-14 -o /scratch/g/PI_NetID/alphatest/output --run_feature=1\n</code></pre> <p>The alphafold software environment is loaded using <code>module load alphafold/2.3.2</code> command. AlphaFold 2 reference files are located at <code>/hpc/refdata/alphafold/2.3</code>. This is set automatically by the module, so do not change <code>-d $DOWNLOAD_DIR</code>. Please make sure to edit <code>PI_NetID</code>. The <code>--run_feature=1</code> flag causes AlphaFold to run the MSA search only.</p> <p>Start a GPU job for model inference and MD:</p> <pre><code># start interactive job\nsrun --job-name=alphafold_test --ntasks=1 --cpus-per-task=12 --time=8:00:00 --partition=gpu --gres=gpu:1 --gpu_cmode=shared --pty bash\n\n# load required modules on the gpu node\nmodule load alphafold/2.3.2\nmodule load python/3.9.1\n\n# tell alphafold about the GPU\nexport NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\n\n# start alphafold software, replace netid with your username\nrun_alphafold.py -d $DOWNLOAD_DIR --cpus 12 -f /scratch/g/PI_NetID/alphatest/some.fasta -t 2020-05-14 -o /scratch/g/PI_NetID/alphatest/output --use_gpu_relax\n</code></pre> <p>This second job will use the results of first step. The <code>--use_gpu_relax</code> flag will ensure that MD uses the GPU.</p> <p>GPU Compute Mode</p> <p>The <code>#SBATCH --gpu_cmode=shared</code> slurm header is required or your job will fail halfway through. Alphafold 2.3.2 doesn't parallelize on GPUs so you should only run on 1 GPU.</p>"},{"location":"software/alphafold/#batch-job","title":"Batch Job","text":"<p>Create a file <code>/scratch/g/PI_NetID/alphatest/alphafold_step1.slurm</code> for the MSA step. Replace <code>PI_NetID</code> with your PI's Username.</p> alphafold_step1.slurm <pre><code>#!/bin/bash\n#SBATCH\u00a0--job-name=alphafold_test\n#SBATCH\u00a0--ntasks=1\n#SBATCH --cpus-per-task=12\n#SBATCH\u00a0--time=08:00:00\n#SBATCH\u00a0--output=%x-%j.out\n\nmodule\u00a0load\u00a0alphafold/2.3.2\nmodule load python/3.9.1\n\nexport\u00a0NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\nrun_alphafold.py\u00a0-d\u00a0$DOWNLOAD_DIR\u00a0--cpus 12 -f\u00a0/scratch/g/PI_NetID/alphatest/some.fasta\u00a0-t\u00a02020-05-14\u00a0-o\u00a0/scratch/g/PI_NetID/alphatest/output --run_feature=1\n</code></pre> <p>Submit the job:</p> <pre><code>cd\u00a0/scratch/g/PI_NetID/alphatest &amp;&amp;\u00a0sbatch\u00a0alphafold_step1.slurm\n</code></pre> <p>Create a file <code>/scratch/g/PI_NetID/alphatest/alphafold_step2.slurm</code> for the inference/MD step.</p> alphafold_step2.slurm <pre><code>#!/bin/bash\n#SBATCH\u00a0--job-name=alphafold_test\n#SBATCH\u00a0--ntasks=1\n#SBATCH --cpus-per-task=12\n#SBATCH\u00a0--time=08:00:00\n#SBATCH\u00a0--output=%x-%j.out\n#SBATCH\u00a0--gres=gpu:1\n#SBATCH --gpu_cmode=shared  \n#SBATCH\u00a0--partition=gpu\n\nmodule\u00a0load\u00a0alphafold/2.3.2\nmodule load python/3.9.1\n\nexport\u00a0NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\nrun_alphafold.py\u00a0-d\u00a0$DOWNLOAD_DIR\u00a0--cpus 12 -f\u00a0/scratch/g/PI_NetID/alphatest/some.fasta\u00a0-t\u00a02020-05-14\u00a0-o\u00a0/scratch/g/PI_NetID/alphatest/output --use_gpu_relax\n</code></pre> <p>Submit the job:</p> <pre><code>cd\u00a0/scratch/g/PI_NetID/alphatest &amp;&amp;\u00a0sbatch\u00a0alphafold_step2.slurm\n</code></pre>"},{"location":"software/alphafold/#alphafold-3","title":"AlphaFold 3","text":"<p>AlphaFold3 is an improvement in functionality and performance. Please note the new license requirements to access required model parameters and change in input file syntax.</p>"},{"location":"software/alphafold/#obtaining-model-parameters","title":"Obtaining Model Parameters","text":"<p>AlphaFold3 is installed on the cluster but does not include the required model parameters, which are subject to Terms of Use. In order to comply, each user wanting to use AlphaFold 3 must submit a request to DeepMind to obtain their own personal copy of the model parameters. Research Computing cannot obtain the model parameters for you. Please ensure that you read and fully understand the Terms of Use agreement prior to request and download.</p> <p>If approved, you will receive a download link to a ~1 GB file. Place that in your home directory, e.g. <code>$HOME/af3</code>. Per the Terms of Use, you may not share these files with anyone else, including other members of your lab.</p>"},{"location":"software/alphafold/#input-json","title":"Input JSON","text":"<p>AlphaFold 3 requires a JSON format input file. Use the following example from DeepMind to test the software.</p> fold_input.json <pre><code>{\n  \"name\": \"2PV7\",\n  \"sequences\": [\n    {\n      \"protein\": {\n        \"id\": [\"A\", \"B\"],\n        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\"\n      }\n    }\n  ],\n  \"modelSeeds\": [1],\n  \"dialect\": \"alphafold3\",\n  \"version\": 1\n}\n</code></pre> <p>Please review the AlphaFold 3 documentation for more info.</p>"},{"location":"software/alphafold/#interactive-job_1","title":"Interactive Job","text":"<p>You should only use interactive jobs for testing/debugging.  Please submit batch jobs otherwise.</p> <p>The syntax and input format has changed in AlphaFold 3. You are required to provide your own model parameters (<code>--model_dir</code>) as previously discussed. The <code>--db_dir</code> is provided with the install and should not be edited. The <code>--json_path</code> will be your JSON format input file as previously discussed. The <code>--output_dir</code> can be set to anything, with the default being the current working directory.</p> <p>Start a CPU job for MSA:</p> <pre><code># start interactive job\nsrun --job-name=alphafold_test --ntasks=1 --cpus-per-task=12 --time=8:00:00 --pty bash\n\n# load required modules\nmodule purge\nmodule load alphafold/3.0.1\n\n# start alphafold software, replace PI_NetID with your username\napptainer exec /hpc/containers/alphafold_3.0.1.sif python /app/alphafold/run_alphafold.py --model_dir=$HOME/af3 --db_dir=$DATABASES_DIR --json_path=fold_input.json --output_dir=$PWD --norun_inference\n</code></pre> <p>The <code>--norun_inference</code> flag causes AlphaFold to run the MSA search only.</p> <p>Start a GPU job for model inference and MD:</p> <pre><code># start interactive job\nsrun --job-name=alphafold_test --ntasks=1 --cpus-per-task=12 --time=8:00:00 --partition=gpu --gres=gpu:1 --gpu_cmode=shared --pty bash\n\n# load required modules\nmodule purge\nmodule load alphafold/3.0.1\n\n# for V100 compatibility\nexport APPTAINERENV_XLA_FLAGS=\"--xla_disable_hlo_passes=custom-kernel-fusion-rewriter\"\n\n# start alphafold software\n\napptainer exec --nv /hpc/containers/alphafold_3.0.1.sif python /app/alphafold/run_alphafold.py --model_dir=$HOME/af3 --db_dir=$DATABASES_DIR --json_path=fold_data.json --output_dir=$PWD --flash_attention_implementation=xla --norun_data_pipeline\n</code></pre> <p>This second job will use the results data json output from the first step.  The --flash_attention_implementation=xla flag is for V100 GPU compatibility.</p>"},{"location":"software/alphafold/#batch-job_1","title":"Batch Job","text":"<p>Start a CPU job for MSA:</p> alphafold_step1.slurm <pre><code>#!/bin/bash\n#SBATCH\u00a0--job-name=alphafold3_test\n#SBATCH\u00a0--ntasks=1\n#SBATCH --cpus-per-task=12\n#SBATCH\u00a0--time=08:00:00\n#SBATCH\u00a0--output=%x-%j.out\n\n# load required modules\nmodule purge\nmodule load alphafold/3.0.1\n\napptainer exec /hpc/containers/alphafold_3.0.1.sif python /app/alphafold/run_alphafold.py \\\n    --model_dir=$HOME/af3 \\\n    --db_dir=$DATABASES_DIR \\\n    --json_path=fold_input.json \\\n    --output_dir=$PWD \\\n    --norun_inference\n</code></pre> <p>Start a GPU job for model inference and MD:</p> alphafold_step2.slurm <pre><code>#!/bin/bash\n#SBATCH\u00a0--job-name=alphafold3_test\n#SBATCH\u00a0--ntasks=1\n#SBATCH --cpus-per-task=12\n#SBATCH\u00a0--time=08:00:00\n#SBATCH\u00a0--output=%x-%j.out\n#SBATCH\u00a0--gres=gpu:1\n#SBATCH --gpu_cmode=shared  \n#SBATCH\u00a0--partition=gpu\n\n# load required modules\nmodule purge\nmodule load alphafold/3.0.1\n\n# for V100 compatibility\nexport APPTAINERENV_XLA_FLAGS=\"--xla_disable_hlo_passes=custom-kernel-fusion-rewriter\"\n\napptainer exec --nv /hpc/containers/alphafold_3.0.1.sif python /app/alphafold/run_alphafold.py \\\n    --model_dir=$HOME/af3 \\\n    --db_dir=$DATABASES_DIR \\\n    --json_path=fold_data.json \\\n    --output_dir=$PWD \\\n    --flash_attention_implementation=xla \\\n    --norun_data_pipeline\n</code></pre>"},{"location":"software/ansys/","title":"ANSYS","text":""},{"location":"software/ansys/#ansys","title":"ANSYS","text":"<p>ANSYS is a simulation software with a variety of functions including finite-element analysis and computational fluid dynamics.</p> <p>Licensed Software</p> <p>ANSYS is restricted to licensed users. Please contact help-rcc@mcw.edu with questions.</p>"},{"location":"software/ansys/#multi-node-fluent-job","title":"Multi-node Fluent job","text":"<p>Fluent should be run from the ANSYS Workbench if possible, as this is the easiest method. However, if you need more resources than 1 node with 48 cores, you can run Fluent in a batch job on the cluster.</p> <p>To get started, setup some file structure. Every time we run a simulation, we should start with a new parent folder, which is <code>example-job</code> in this case. Make sure this is meaningful, since it will help you identify your simulations later.</p> <pre><code>cd /scratch/g/PI_NetID\nmkdir -p example-job/{cas-files,output}\n</code></pre> <p>We created a new folder for our first simulation <code>example-job</code> in our group scratch directory. Within that folder, we also created a place to hold our input and output files. Upload the input cas files to the <code>cas-files</code> directory.</p> <p>We need a journal file to tell Fluent how to proceed. Create a file <code>/scratch/g/PI_NetID/example-job/fluent-test.jou</code>.</p> fluent-test.jou <pre><code>/file/read-case-data \"/scratch/g/PI_NetID/example-job/cas-files/test_fluent_case\"\n/solve/iterate 1000\n/file/write-case \"/scratch/g/PI_NetID/example-job/output/test_fluent_case.cas\"\n/file/write-data \"/scratch/g/PI_NetID/example-job/output/test_fluent_case.data\"\n/parallel timer usage\n/exit\nyes\n</code></pre> <p>We also need a job script. Create a file <code>/scratch/g/PI_NetID/example-job/job-script.slurm</code>. Replace PI_NetID with your PI's username.</p> job-script.slurm <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=16\n#SBATCH --job-name=test-fluent-case\n#SBATCH --output=%x-%j.out\n#SBATCH --time=168:00:00\n\n# this is where we load software\nmodule load impi\nmodule load ansys/2022R1\n\nexport FLUENT_AFFINITY=0                             \nexport SLURM_ENABLED=1                               \nexport SCHEDULER_TIGHT_COUPLING=1                    \nFL_SCHEDULER_HOST_FILE=slurm.${SLURM_JOB_ID}.hosts  \n/bin/rm -rf ${FL_SCHEDULER_HOST_FILE}               \nscontrol show hostnames \"$SLURM_JOB_NODELIST\" &gt;&gt; $FL_SCHEDULER_HOST_FILE \ntime fluent -g 3ddp -t${SLURM_NTASKS} -i fluent-test.jou -cnf=${FL_SCHEDULER_HOST_FILE}\n</code></pre> <p>Submit the job:</p> <pre><code>cd\u00a0/scratch/g/PI_NetID/example-job\nsbatch\u00a0job-script.slurm\n</code></pre> <p>The example above is a job request for 2 nodes with 16 cores per node. Please make sure to insert your PI's username for <code>PI_NetID</code>.</p>"},{"location":"software/cluster-pkg-list/","title":"Cluster Packages","text":""},{"location":"software/cluster-pkg-list/#cluster-software-list","title":"Cluster software list","text":"<p>Last updated: October 28, 2025</p>"},{"location":"software/cluster-pkg-list/#categories","title":"Categories","text":"<p>We currently provide 424 software modules, in 6 categories, covering 62 fields of science:</p> <ul> <li><code>biology</code>      computational biology, cryo-em, genomics, imaging, molecular dynamics, neuroimaging, population genetics   </li> </ul> <ul> <li><code>chemistry</code>      computational chemistry, electrostatics, molecular dynamics, molecular modelling, molecular visualization, tools   </li> </ul> <ul> <li><code>devel</code>      compiler, data, debugger, language, lib, mpi, networking, profiling, text editor   </li> </ul> <ul> <li><code>math</code>      analytics, computational fluid dynamics, data analytics, deep learning, finite element analysis, gis, lib, machine learning, numerical analysis, numerical library, optimization, simulation, statistics   </li> </ul> <ul> <li><code>system</code>      benchmark, build, checkpointing, cloud interface, compression, containers, data management, database, document processing, file transfer, ide, imaging, lib, library, media, package manager, pipeline, profiling, remote display, scm, terminal, tools, utilities   </li> </ul> <ul> <li><code>viz</code>      graphics, graphs, imaging, molecular visualization   </li> </ul>"},{"location":"software/cluster-pkg-list/#biology","title":"biology","text":"Field Module Name Version(s) URL Description computational biology <code>alphafold</code> <code>2.3.2</code><code>3.0.0</code><code>3.0.1</code> Website AI system developed by DeepMind that predicts a protein\u2019s 3D structure from its amino acid sequence. computational biology <code>alphamissense</code> <code>3.18.2</code> Website AI system developed by DeepMind that predicts a protein\u2019s 3D structure from its amino acid sequence. computational biology <code>alphapulldown</code> <code>1.0.4</code> Website AlphaPulldown is a Python package that streamlines protein-protein interaction screens and high-throughput modelling of higher-order oligomers using AlphaFold-Multimer. computational biology <code>bcl</code> <code>3.4.0</code> Website The Bio Chemical Library (BCL) is a software package that provides unique tools for biological research, such as protein structure determination from sparse experimental data. computational biology <code>boltz</code> <code>0.4.1</code> Website Boltz-1 is the state-of-the-art open-source model to predict biomolecular structures containing combinations of proteins, RNA, DNA, and other molecules. computational biology <code>calibur</code> <code>20120117</code> Website Efficient tool for finding structural candidate decoys generated from Ab Initio protein structure prediction methods. computational biology <code>colabfold</code> <code>2.3.1</code> Website AI system developed by DeepMind that predicts a protein\u2019s 3D structure from its amino acid sequence. computational biology <code>cookhla</code> <code>1.0.1</code> Website CookHLA is an accurate and efficient HLA imputation method. computational biology <code>dssp</code> <code>4.4.0</code><code>4.4.7</code> Website DSSP is an algorithm originally designed by Wolfgang Kabsch and Chris Sander to standardise secondary structure assigment based on atomic coordinates. computational biology <code>ensembler</code> <code>1.0.5</code> Website Software pipeline for automating omics-scale protein modeling and simulation setup. computational biology <code>foldx</code> <code>4.0</code><code>5.0</code><code>5.1</code> Website FoldX provides a fast and quantitative estimation of the importance of the interactions contributing to the stability of proteins and protein complexes. computational biology <code>instanovo</code> <code>1.1.1</code> Website InstaNovo is a transformer neural network with the ability to translate fragment ion peaks into the sequence of amino acids that make up the studied peptides. computational biology <code>interproscan</code> <code>5.39-77.0</code><code>5.50-84.0</code> Website Functional analysis of proteins by classifying them into families and predicting domains and important sites. computational biology <code>modeller</code> <code>10.0</code><code>10.5</code> Website MODELLER is used for homology or comparative modeling of protein three-dimensional structures. computational biology <code>msmbuilder</code> <code>3.8.0</code> Website MSMBuilder is an application and python library. It builds statistical models for high-dimensional time-series. The particular focus of the package is on the analysis of atomistic simulations of biomolecular dynamics such as protein folding and conformational change. computational biology <code>pdbfixer</code> <code>1.7.0</code> Website The PDBFixer is an easy to use application for fixing problems in Protein Data Bank files computational biology <code>phenix</code> <code>1.21.2</code> Website Python-based Hierarchical ENvironment for Integrated Xtallography. computational biology <code>phylip</code> <code>3.697</code> Website PHYLIP (the PHYLogeny Inference Package) is a package of programs for inferring phylogenies (evolutionary trees). computational biology <code>rfdiffusion</code> <code>1.1.0</code> Website RFdiffusion is an open source method for structure generation, with or without conditional information (a motif, target etc). computational biology <code>tsl</code> <code>1.21</code> Website Two Sample Logo is a web-based application that calculates and visualizes differences between two sets of aligned samples of amino acids or nucleotides. computational biology <code>ucbshift</code> <code>2.0</code> Website UCBShift is a program for predicting chemical shifts for backbone and side chain atoms of a protein in solution. computational biology <code>weblogo</code> <code>3.9.0</code> Website Application designed to make the generation of sequence logos as easy and painless as possible. cryo-em <code>relion</code> <code>3.1.3</code><code>4.0.2</code> Website RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a software package that employs an empirical Bayesian approach for electron cryo-microscopy (cryo-EM) structure determination. genomics <code>agat</code> <code>1.4.0</code> Website Suite of tools to handle gene annotations in any GTF/GFF format. genomics <code>ampliconarchitect</code> <code>1.2</code> Website Identify one or more connected, focally amplified genomic regions to elucidate the architecture of focal amplifications such as ecDNA. genomics <code>ampliconsuite</code> <code>1.3.5</code> Website A quickstart tool for AmpliconArchitect. Performs all preliminary steps required prior to running AmpliconArchitect. Previously called PrepareAA. genomics <code>analyze_motifs</code> <code>0.0.1</code> Website Runs HOMER and kmer enrichment aspects from clip_analysis_legacy to generate useful motif analyses. genomics <code>annovar</code> <code>042018</code><code>25616</code><code>29711</code> Website ANNOVAR is an efficient software tool to utilize update-to-date information to functionally annotate genetic variants detected from diverse genomes. genomics <code>antismash</code> <code>7.1.0</code> Website antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. genomics <code>anvio</code> <code>8</code> Website Anvi\u2019o is an open-source, community-driven analysis and visualization platform for microbial \u2018omics. genomics <code>aracne-ap</code> <code>072018</code> Website ARACNe-AP (Algorithm for the Reconstruction of Accurate Cellular Networks with Adaptive Partitioning) genomics <code>bam-readcount</code> <code>0.8.0</code> Website Utility that runs on a BAM or CRAM file and generates low-level information about sequencing data at specific nucleotide positions. genomics <code>bamtools</code> <code>2.4.1</code><code>2.5.1</code> Website C++ API and a command-line toolkit for reading, writing, and manipulating BAM (genome alignment) files. genomics <code>basemount</code> <code>0.15.96</code> Website BaseMount is a tool to mount your BaseSpace Sequence Hub data as a Linux file system. genomics <code>basespace</code> <code>1.3.0</code> Website The BaseSpace Sequence Hub CLI supports scripting and programmatic access to BaseSpace Sequence Hub for automation, bulk operations, and other routine functions. genomics <code>bbmap</code> <code>38.82</code><code>39.01</code><code>39.11</code> Website Short read aligner for DNA and RNA-seq data. genomics <code>bcftools</code> <code>1.11</code><code>1.15</code><code>1.16</code><code>1.19</code><code>1.21</code> Website BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF. genomics <code>bcftools-mocha</code> <code>1.15</code> Website BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF. genomics <code>bcl2fastq2</code> <code>2.2</code> Website The Illumina bcl2fastq2 Conversion Software v2.20 demultiplexes sequencing data and converts base call (BCL) files into FASTQ files. genomics <code>bedops</code> <code>2.4.39</code><code>2.4.41</code> Website Open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale. genomics <code>bedtools2</code> <code>2.25.0</code><code>2.30.0</code> Website The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. genomics <code>bgen</code> <code>1.1.7</code> Website Reference implementation of the BGEN format, written in C++. genomics <code>bicseq2-norm</code> <code>0.2.4</code> Website BICseq2-norm is for normalizing potential biases in the sequencing data. genomics <code>bicseq2-seg</code> <code>0.7.2</code> Website BICseq2-seg is for detecting CNVs based on the normalized data given by BICseq2-norm. genomics <code>bioawk</code> <code>1.0</code> Website BWK awk modified for biological data genomics <code>biobambam2</code> <code>2.0.87</code> Website This package contains some tools for processing BAM files. genomics <code>biolibc</code> <code>0.2.4</code> Website Biolibc is a collection of high-quality bricks that can be used to build efficient, robust software applications to replace disposable scripts. Using biolibc, you can easily develop permanent solutions that are easy to use and install, with near-optimal performance, so that no one ever need reinvent that particular wheel. Biolibc also facilitates development of more complex applications by providing many commonly used building blocks, thus releasing you from low-level coding. genomics <code>bior</code> <code>5.0.0</code> Website The Biological Reference Repository (BioR) catalog format is a flexible, readable, indexable, and schema-free format for storing and rapidly accessing arbitrary structured data such as genomic features, diseases, conditions, genetic tests, and drugs. genomics <code>bismark</code> <code>0.16.3</code> Website A tool to map bisulfite converted sequence reads and determine cytosine methylation states. genomics <code>blast</code> <code>2.9.0</code><code>2.12.0</code> Website The Basic Local Alignment Search Tool (BLAST) finds regions of local similarity between sequences. genomics <code>bowtie</code> <code>2.3.4.3</code><code>2.5.0</code><code>2.5.4</code> Website Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. genomics <code>bracken</code> <code>2.6.0</code><code>2.8.0</code> Website Bracken (Bayesian Reestimation of Abundance with KrakEN) is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample. genomics <code>bwa</code> <code>0.7.17</code> Website BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. genomics <code>cadd</code> <code>1.6</code> Website CADD is a tool for scoring the deleteriousness of single nucleotide variants as well as insertion/deletions variants in the human genome. genomics <code>canu</code> <code>1.8</code> Website A single molecule sequence assembler for genomes large and small. genomics <code>cava</code> <code>1.2.3</code> Website CAVA (Clinical Annotation of VAriants) is a lightweight, fast, flexible and easy-to-use Next Generation Sequencing (NGS) variant annotation tool. genomics <code>cd-hit</code> <code>4.8.1</code> Website Program for clustering and comparing protein or nucleotide sequences. genomics <code>cellranger</code> <code>3.0.0</code><code>4.0.0</code><code>5.0.1</code><code>6.0.0</code><code>7.0.0</code><code>8.0.0</code><code>9.0.0</code> Website Set of analysis pipelines that process Chromium single cell data to align reads, generate feature-barcode matrices, perform clustering and other secondary analysis genomics <code>cellranger-arc</code> <code>1.0.1</code><code>2.0.1</code><code>2.0.2</code> Website Cell Ranger ARC is a set of analysis pipelines that process Chromium Single Cell Multiome ATAC + Gene Expression sequencing data to generate a variety of analyses pertaining to gene expression (GEX), chromatin accessibility, and their linkage. genomics <code>cellranger-atac</code> <code>2.1.0</code> Website Cell Ranger ARC is a set of analysis pipelines that process Chromium Single Cell Multiome ATAC + Gene Expression sequencing data to generate a variety of analyses pertaining to gene expression (GEX), chromatin accessibility, and their linkage. genomics <code>cellsnp-lite</code> <code>1.2.2</code> Website cellsnp-lite was initially designed to pileup the expressed alleles in single-cell or bulk RNA-seq data, which can be directly used for donor deconvolution in multiplexed single-cell RNA-seq data, particularly with vireo, which assigns cells to donors and detects doublets, even without genotyping reference. genomics <code>centrifuge</code> <code>1.0.3</code> Website Rapid and memory-efficient system for the classification of DNA sequences from microbial samples, with better sensitivity than and comparable accuracy to other leading systems. genomics <code>circexplorer2</code> <code>2.3.6</code> Website CIRCexplorer2 is a comprehensive and integrative circular RNA analysis toolset. genomics <code>circlefinder</code> <code>1.0</code> Website A method to identify Circular DNA (Micro DNA) from pair-end high-throughput sequencing data. genomics <code>circlemap</code> <code>1.1.4</code> Website Circle-Map takes as input an alignment of reads to a reference genome (e.g. a BWA-MEM generated BAM file) and like other methods, it will use those alignments to detect cases were the read has been split into two segments (e.g. split reads) to detect genomic rearrangements supporting a circular DNA structure. genomics <code>cite-seq-count</code> <code>1.4.4</code> Website A tool that allows to get UMI counts from a single cell protein assay. genomics <code>clara-parabricks</code> <code>4.0.1-1</code><code>4.4.0</code><code>4.5.0</code> Website Nvidia Clara Parabricks is an accelerated compute framework that supports applications across the genomics industry, primarily supporting analytical workflows for DNA, RNA, and somatic mutation detection applications. genomics <code>clipper</code> <code>2.0.1</code> Website CLIPper is a tool to define peaks in your CLIP-seq dataset. genomics <code>clustal-omega</code> <code>1.2.4</code> Website Clustal Omega is a new multiple sequence alignment program that uses seeded guide trees and HMM profile-profile techniques to generate alignments between three or more sequences. genomics <code>cnvkit</code> <code>0.9.12</code> Website CNVkit is a Python library and command-line software toolkit to infer and visualize copy number from high-throughput DNA sequencing data. genomics <code>cobra</code> <code>2.0</code> Website Containerized Bioinformatics workflow for Reproducible ChIP,ATAC-seq Analysis genomics <code>compass</code> <code>0.9.10.2</code> Website In-Silico Modeling of Metabolic Heterogeneity using Single-Cell Transcriptomes genomics <code>concoct</code> <code>1.1.0</code> Website CONCOCT \u201cbins\u201d metagenomic contigs. Metagenomic binning is the process of clustering sequences into clusters corresponding to operational taxonomic units of some level. genomics <code>crossmap</code> <code>0.6.6</code> Website CrossMap is a program for genome coordinates conversion between different assemblies (such as hg18 (NCBI36) &lt;=&gt; hg19 (GRCh37)). It supports commonly used file formats including BAM, CRAM, SAM, Wiggle, BigWig, BED, GFF, GTF, MAF VCF, and gVCF. genomics <code>cufflinks</code> <code>2.2.2</code> Website Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. genomics <code>cutadapt</code> <code>3.2</code><code>3.4</code><code>4.0</code> Website Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads. genomics <code>cytoscape</code> <code>3.10.3</code> Website Cytoscape is an open source software platform for visualizing complex networks and integrating these with any type of attribute data. With Cytoscape, you can load molecular and genetic interaction data sets in many standards formats, project and integrate global datasets and functional annotations, establish powerful visual mappings across these data, perform advanced analysis and modeling and visualize and analyze human-curated pathway datasets. genomics <code>dapars</code> <code>1.0.0</code> Website Dynamitic analysis of Alternative PolyAdenylation from RNA-seq genomics <code>dcc</code> <code>0.8.0</code> Website DCC is a python package intended to detect and quantify circRNAs with high specificity. DCC works with the STAR (Dobin et al., 2013) chimeric.out.junction files which contains chimerically aligned reads including circRNA junction spanning reads. genomics <code>dcca</code> <code>1.0.1</code> Website Deep cross-omics cycle attention model for joint analysis of single-cell multi-omics data. genomics <code>dchic</code> <code>2.1</code> Website dcHiC is a tool for differential compartment analysis of Hi-C datasets genomics <code>deeptools</code> <code>3.5.0</code><code>3.5.1</code><code>3.5.6</code> Website Suite of python tools particularly developed for the efficient analysis of high-throughput sequencing data, such as ChIPseq, RNAseq or MNaseseq. genomics <code>depict</code> <code>1.194</code> Website an integrative tool that employs predicted gene functions to systematically prioritize the most likely causal genes at associated loci, highlight enriched pathways and identify tissues/cell types where genes from associated loci are highly expressed. genomics <code>diamond</code> <code>0.9.14</code> Website DIAMOND is a sequence aligner for protein and translated DNA searches, designed for high performance analysis of big sequence data. genomics <code>dnanexus</code> <code>0.6.2</code> Website DNAnexus download agent. genomics <code>dorado</code> <code>0.9.6</code> Website Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. genomics <code>dram</code> <code>1.5.0</code> Website DRAM (Distilled and Refined Annotation of Metabolism) is a tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs. genomics <code>dropest</code> <code>0.8.5</code> Website Pipeline for estimating molecular count matrices for droplet-based single-cell RNA-seq measurements. genomics <code>dx-toolkit</code> <code>0.392.0</code> Website DNAnexus Platform SDK genomics <code>ea-utils</code> <code>1.04</code> Website Command-line tools for processing biological sequencing data. genomics <code>eclipanalyzer</code> <code>1.0</code> Website This repository contains a series of python based modules to automate the analysis of sequencing data generated from eCLIP (enhanced UV crosslinking and immunoprecipitation) experiments. genomics <code>eclipidrmergepeaks</code> <code>0.1.0</code> Website CWL-defined pipeline for using IDR to produce a set of peaks given two replicate eCLIP peaks. genomics <code>eclipregionnormalize</code> <code>0.0.4</code> Website Pipeline for region-based enrichment. This workflow is defined using CWL v1.0 spec. genomics <code>emg-viral-pipeline</code> <code>0.3.0</code><code>2.0.0</code> Website VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. genomics <code>enhancer-dissection</code> <code>1.0.0</code> Website The two programs in this project display variant data, motif data (via highlights). genomics <code>ensembl-vep</code> <code>103.1</code><code>111.0</code> Website VEP determines the effect of your variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions. genomics <code>esm</code> <code>2.0.0</code> Website Evolutionary Scale Modeling (esm). Pretrained language models for proteins. genomics <code>esmfold</code> <code>1.0.3</code> Website Evolutionary Scale Modeling (esm). Pretrained language models for proteins. genomics <code>express</code> <code>1.5.1</code> Website eXpress is a streaming tool for quantifying the abundances of a set of target sequences from sampled subsequences. genomics <code>fastq-pair</code> <code>1.0</code> Website Rewrite paired end fastq files to make sure that all reads have a mate and to separate out singletons.. genomics <code>fastq-screen</code> <code>0.15.2</code> Website FastQ Screen allows you to screen a library of sequences in FastQ format against a set of sequence databases so you can see if the composition of the library matches with what you expect. genomics <code>fastqc</code> <code>0.11.9</code> Website FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. genomics <code>fastvifi</code> <code>1.0</code> Website A software to detect viral infection and integration sites for different viruses. FastViFi relies on ViFi and Kraken2 tools. genomics <code>fiji</code> <code>2.16.0</code> Website Fiji is an image processing package\u2014a batteries-included distribution of ImageJ2, bundling a lot of plugins which facilitate scientific image analysis. genomics <code>fitsne</code> <code>1.2.1</code> Website FFT-accelerated Interpolation-based t-SNE (FIt-SNE) genomics <code>fluxsimulator</code> <code>1.2.1</code> Website The Flux Simulator aims at modeling RNA-Seq experiments in silico. Sequencing reads are produced from a reference genome according annotated transcripts. genomics <code>gatk</code> <code>3.8</code><code>3.8.1</code><code>4.0.11.0</code><code>4.1.7.0</code><code>4.4.0.0</code><code>4.5.0.0</code> Website The GATK is the industry standard for identifying SNPs and indels in germline DNA and RNAseq data. genomics <code>gcta</code> <code>1.94.1</code> Website a tool for Genome-wide Complex Trait Analysis genomics <code>gdc-client</code> <code>1.6.1</code> Website Illumina Array Analysis Platform cli genomics <code>geneformer</code> <code>0.1.0</code> Website Geneformer is a foundation transformer model pretrained on a large-scale corpus of ~30 million single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology. genomics <code>genomebrowse</code> <code>3.1.0</code> Website The free Golden Helix GenomeBrowse tool delivers stunning visualizations of your genomic data that give you the power to see what is occurring at each base pair in your samples. genomics <code>genrich</code> <code>0.6.1</code> Website Genrich is a peak-caller for genomic enrichment assays e.g. ChIP-seq, ATAC-seq. It analyzes alignment files generated following the assay and produces a file detailing peaks of significant enrichment. genomics <code>gethomologues</code> <code>3.7.2</code> Website Get_Homologues is a versatile software package for pan-genome analysis. genomics <code>gffcompare</code> <code>0.10.1</code> Website The program gffcompare can be used to compare, merge, annotate and estimate accuracy of one or more GFF files (the \u201cquery\u201d files), when compared with a reference annotation (also provided as GFF). genomics <code>gistic</code> <code>2.0.23</code> Website The GISTIC module identifies regions of the genome that are significantly amplified or deleted across a set of samples. genomics <code>glow</code> <code>2.0.0</code> Website Glow is an open-source toolkit for working with genomic data at biobank-scale and beyond. genomics <code>gmap-gsnap</code> <code>2016-05-01</code><code>2021-12-17</code> Website A genomic mapping and alignment program for mRNA and EST sequences. genomics <code>google-cloud-sdk</code> <code>335.0.0</code><code>393.0.0</code><code>525.0.0</code> Website gsutil is a command-line tool for interacting with Google Cloud Storage. genomics <code>gossamer</code> <code>1.0.0</code> Website Gossamer is an application for doing de novo assembly of high throughput sequencing data. genomics <code>gridss</code> <code>2.13.2</code> Website GRIDSS is a module software suite containing tools useful for the detection of genomic rearrangements. genomics <code>gsutil</code> <code>5.3.4</code> Website gsutil is a command-line tool for interacting with Google Cloud Storage. genomics <code>hail</code> <code>0.2.133</code> Website Hail is an open-source library for scalable data exploration and analysis, with a particular emphasis on genomics. genomics <code>haslr</code> <code>0.8a1</code> Website HASLR is a tool for rapid genome assembly of long sequencing reads. genomics <code>hicexplorer</code> <code>3.7.6</code> Website A set of tools for the analysis and visualization of chromosome conformation data genomics <code>hisat2</code> <code>2.0.5</code><code>2.2.1</code> Website HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes as well as to a single reference genome. genomics <code>hisatgenotype</code> <code>1.3.3</code> Website Next-generation genomic analysis software platform. genomics <code>hmmer</code> <code>3.3.2</code> Website HMMER is used for searching sequence databases for sequence homologs, and for making sequence alignments. genomics <code>homer</code> <code>4.9.1</code><code>4.11.1</code><code>5.1</code> Website HOMER (Hypergeometric Optimization of Motif EnRichment) is a suite of tools for Motif Discovery and next-gen sequencing analysis. genomics <code>htseq</code> <code>0.13.5</code> Website HTSeq is a Python package for analysis of high-throughput sequencing data. genomics <code>htslib</code> <code>1.9</code><code>1.10.2</code><code>1.11</code><code>1.15</code><code>1.2</code> Website HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data, and is the core library used by samtools and bcftools. genomics <code>humann</code> <code>3.0.0a4</code><code>3.9</code> Website HUMAnN 3.0 is the next generation of HUMAnN (HMP Unified Metabolic Analysis Network). genomics <code>iaap-cli</code> <code>1.1.0</code> Website Illumina Array Analysis Platform cli genomics <code>idr</code> <code>2.0.2</code> Website The IDR (Irreproducible Discovery Rate) framework is a uni\ufb01ed approach to measure the reproducibility of \ufb01ndings identi\ufb01ed from replicate experiments and provide highly stable thresholds based on reproducibility. genomics <code>igv</code> <code>2.17.4</code><code>2.19.4</code> Website The Integrative Genomics Viewer (IGV) is a high-performance, easy-to-use, interactive tool for the visual exploration of genomic data. genomics <code>illumina-utils</code> <code>2.1</code><code>2.11</code> Website Software to perform various operations on FASTQ files (such as demultiplexing raw Illumina files, merging partial or complete overlaps, and/or performing quality filtering). genomics <code>impute</code> <code>2.3.2</code> Website IMPUTE version 2 is a genotype imputation and haplotype phasing program. genomics <code>impute5</code> <code>1.1.5</code> Website IMPUTE 5 is a genotype imputation method that can scale to reference panels with millions of samples. genomics <code>jvarkit</code> <code>2021.10.13</code> Website Java utilities for bioinformatics. genomics <code>k2mem</code> <code>1.0.0</code> Website K2Mem is a variant of Kraken 2 taxonomic sequence classification system that can learn from the previous classifications. genomics <code>kaiju</code> <code>1.7.2</code> Website Kaiju is a program for the taxonomic classification of high-throughput sequencing reads, e.g., Illumina or Roche/454, from whole-genome sequencing of metagenomic DNA. genomics <code>kallisto</code> <code>0.46.1</code> Website kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. genomics <code>king</code> <code>2.1.2</code> Website Kinship-based INference for Gwas genomics <code>kneaddata</code> <code>0.10.0</code> Website KneadData is a tool designed to perform quality control on metagenomic and metatranscriptomic sequencing data, especially data from microbiome experiments. genomics <code>kobas</code> <code>3.0.3</code> Website KOBAS (KEGG Orthology-Based Annotation System) is a tool for the annotation of sequences by KEGG Orthology terms. KOBAS also identifies enriched pathways and uses KEGG Pathway, PID, BioCyc, Reactome, Panther and human data from OMIM, KEGG Disease, FunDO, GAD, NHGRI, and GWAS databases. genomics <code>kraken2</code> <code>2.1.1</code><code>2.1.3</code> Website Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. genomics <code>krakenuniq</code> <code>0.5.8</code> Website Metagenomics classifier with unique k-mer counting for more specific results. genomics <code>lefse</code> <code>1.1.2</code> Website LEfSe determines the features most likely to explain differences between classes by coupling standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. genomics <code>lep-map</code> <code>3</code> Website Lep-MAP3 (LM3) is a novel linkage map construction software suite. genomics <code>lumpy</code> <code>0.2.13</code> Website A probabilistic framework for structural variant discovery. genomics <code>macs2</code> <code>2.2.9.1</code> Website Model-based Analysis for ChIP-Seq genomics <code>macs3</code> <code>3.0.2</code> Website Model-based Analysis for ChIP-Seq genomics <code>maftools</code> <code>2.22.0</code> Website Provides a comprehensive set of functions for processing MAF files and to perform most commonly used analyses in cancer genomics. genomics <code>magicblast</code> <code>1.5.0</code> Website Magic-BLAST is a tool for mapping large next-generation RNA or DNA sequencing runs against a whole genome or transcriptome. genomics <code>magma</code> <code>1.1</code> Website MAGMA is a tool for gene analysis and generalized gene-set analysis of GWAS data. It can be used to analyse both raw genotype data as well as summary SNP p-values from a previous GWAS or meta-analysis. genomics <code>manorm</code> <code>1.1.4</code> Website A robust model for quantitative comparison of ChIP-Seq data sets. genomics <code>megahit</code> <code>1.1.1</code><code>1.2.9</code> Website MEGAHIT is an ultra-fast and memory-efficient NGS assembler. It is optimized for metagenomes, but also works well on generic single genome assembly (small or mammalian size) and single-cell assembly. genomics <code>meme</code> <code>5.3.1</code><code>5.4.1</code> Website The MEME Suite allows the biologist to discover novel motifs in collections of unaligned nucleotide or protein sequences, and to perform a wide variety of other motif-based analyses. genomics <code>metagenemark</code> <code>3.38</code> Website Metagenomic sequences can be analyzed by MetaGeneMark , the program optimized for speed. genomics <code>metaphlan</code> <code>4.1.1</code> Website MetaPhlAn is a computational tool for profiling the composition of microbial communites from metagenomic shotgun sequencing data with species level resolution. genomics <code>metilene</code> <code>0.2.6</code> Website Fast and sensitive detection of differential DNA methylation genomics <code>migec</code> <code>1.2.9</code> Website Molecular Identifier Guided Error Correction pipeline genomics <code>minpath</code> <code>1.5</code> Website MinPath (Minimal set of Pathways) is a parsimony approach for biological pathway reconstructions using protein family predictions, achieving a more conservative, yet more faithful, estimation of the biological pathways for a query dataset. genomics <code>mira</code> <code>4.0.2</code> Website MIRA is a whole genome shotgun and EST sequence assembler for Sanger, 454, Solexa (Illumina), IonTorrent data and PacBio. genomics <code>mitohpc</code> <code>1.0.0</code> Website MitoHPC is used for Calling Mitochondrial Homplasmies and Heteroplasmies, Mitochondrial High Performance Caller genomics <code>mixcr</code> <code>2.1.11</code><code>4.1.0</code> Website MiXCR is a universal software for fast and accurate analysis of raw T- or B- cell receptor repertoire sequencing data. genomics <code>mosaik</code> <code>0.9.0891</code> Website MOSAIK is a reference-guided aligner for next-generation sequencing technologies. genomics <code>mtoolbox</code> <code>1.2.1</code> Website A bioinformatics pipeline to analyze mtDNA from NGS data. genomics <code>multiqc</code> <code>1.11</code><code>1.17</code><code>1.18</code> Website MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples. genomics <code>mummer</code> <code>4.0.1</code> Website MUMmer is a system for rapidly aligning large DNA sequences to one another.. genomics <code>muscle</code> <code>3.8.1551</code> Website MUSCLE is widely-used software for making multiple alignments of biological sequences. genomics <code>mutsig2cv</code> <code>1.0</code> Website MutSig2CV analyzes somatic point mutations discovered in DNA sequencing, identifying genes mutated more often than expected by chance given inferred background mutation processes. genomics <code>ngmerge</code> <code>0.3</code> Website Merging paired-end reads and removing sequencing adapters genomics <code>octopus</code> <code>0.7.4</code> Website Haplotype-based variant calling genomics <code>oncoimpact</code> <code>0.9.4</code> Website OncoIMPACT is a first-in-class algorithmic framework that nominates patient-specific driver genes by integratively modeling genomic mutations (point, structural and copy-number) and the resulting perturbations in transcriptional programs via defined molecular networks. genomics <code>pacbioapps</code> <code>latest</code> Website PacBio Secondary Analysis Tools on Bioconda genomics <code>parsebiosciences</code> <code>1.4.2</code> Website Parse's single cell kits facilitate scalable single cell RNA-seq, enabling you to go all the way from samples in a single cell suspension to sequencing data. genomics <code>pgap</code> <code>2021-07-01</code> Website The NCBI Prokaryotic Genome Annotation Pipeline is designed to annotate bacterial and archaeal genomes (chromosomes and plasmids). genomics <code>phenolyzer</code> <code>0.4.0</code> Website Phenolyzer is a software that implements phenotype-based prioritization of candidate genes for human diseases. genomics <code>phispy</code> <code>4.2.21</code> Website PhiSpy identifies prophages in Bacterial (and probably Archaeal) genomes. Given an annotated genome it will use several approaches to identify the most likely prophage regions.. genomics <code>picard</code> <code>2.6.0</code><code>2.25.0</code> Website A set of command line tools (in Java) for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF. genomics <code>pignnemmenv</code> <code>1.0</code> Website Physics-Informed Graph Neural Network Emulation of Soft-Tissue Mechanics (CMAME 2023) genomics <code>piranha</code> <code>1.2.1</code> Website Piranha is a peak-caller for CLIP- and RIP-Seq data. It takes input in BED or BAM format and identifies regions of statistically significant read enrichment. genomics <code>plink</code> <code>1.9.0</code><code>2.0.0</code> Website PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. genomics <code>prodigal</code> <code>2.6.3</code> Website Fast, reliable protein-coding gene prediction for prokaryotic genomes. genomics <code>prokka</code> <code>1.14.6</code> Website Prokka is a software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files. genomics <code>prscs</code> <code>1.0.0</code> Website PRS-CS is a Python based command line tool that infers posterior SNP effect sizes under continuous shrinkage (CS) priors using GWAS summary statistics and an external LD reference panel. genomics <code>prsice2</code> <code>2.3.3</code> Website Polygenic Risk Score software. genomics <code>pureclip</code> <code>1.3.1</code> Website PureCLIP is a tool to detect protein-RNA interaction footprints from single-nucleotide CLIP-seq data, such as iCLIP and eCLIP.. genomics <code>pyani</code> <code>0.2.13.1</code> Website Application and Python module for whole-genome classification of microbes using Average Nucleotide Identity. genomics <code>pyega3</code> <code>5.2.0</code> Website The pyEGA3 download client is a python-based tool for viewing and downloading files from authorized EGA datasets. pyEGA3 uses the EGA Data API and has several key features. genomics <code>pyscenic</code> <code>0.12.1</code> Website pySCENIC is a lightning-fast python implementation of the SCENIC pipeline which enables biologists to infer transcription factors, gene regulatory networks and cell types from single-cell RNA-seq data. genomics <code>qctool</code> <code>2.0.8</code> Website QCTOOL is a command-line utility program for manipulation and quality control of gwas datasets and other genome-wide data. genomics <code>qiime2</code> <code>amplicon-2024.10</code><code>metagenome-2024.10</code><code>pathogenome-2024.10</code> Website Qiime2 is a next-generation microbiome bioinformatics platform. genomics <code>regenie</code> <code>3.1.1</code> Website regenie is a C++ program for whole genome regression modelling of large genome-wide association studies. genomics <code>repeatmasker</code> <code>4.1.7</code> Website RepeatMasker is a program that screens DNA sequences for interspersed repeats and low complexity DNA sequences. genomics <code>rgi</code> <code>6.0.2</code> Website This application is used to predict antibiotic resistome(s) from protein or nucleotide data based on homology and SNP models. genomics <code>rhapsody</code> <code>0.9.8</code> Website Python program, based on ProDy, for pathogenicity prediction of human missense variants. genomics <code>rsem</code> <code>1.3.3</code> Website RSEM RNA-Seq by Expectation-Maximization genomics <code>rseqc</code> <code>3.0.1</code><code>4.0.0</code> Website RNA Sequence Quality Control Package genomics <code>rvtests</code> <code>2.1.0</code> Website Rvtests, which stands for Rare Variant tests, is a flexible software package for genetic association analysis for sequence datasets. genomics <code>sai-10k-calc</code> <code>1.1</code> Website Calculator for the prediction of pseudoexonization, intron retention, and exon deletion. genomics <code>saige</code> <code>1.2.0</code> Website SAIGE is an R package developed with Rcpp for genome-wide association tests in large-scale data sets and biobanks. genomics <code>salmon</code> <code>1.5.2</code> Website Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. genomics <code>sambamba</code> <code>0.8.1</code><code>1.0.1</code> Website Sambamba is a high performance highly parallel robust and fast tool (and library), written in the D programming language, for working with SAM and BAM files. genomics <code>samblaster</code> <code>0.1.26</code> Website samblaster is a fast and flexible program for marking duplicates in read-id grouped1 paired-end SAM files. genomics <code>samtools</code> <code>1.11</code><code>1.15.1</code><code>1.2</code> Website Samtools is a suite of programs for interacting with high-throughput sequencing data. genomics <code>scgpt</code> <code>0.2.1</code> Website scGPT, a Python package for single-cell multi-omic data analysis using pretrained foundation models. genomics <code>scvelo</code> <code>0.3.3</code> Website scVelo is a scalable toolkit for RNA velocity analysis in single cells, based on Bergen et al. (Nature Biotech, 2020). genomics <code>seqkit</code> <code>0.15.0</code> Website A cross-platform and ultrafast toolkit for FASTA/Q file manipulation genomics <code>seqtk</code> <code>1.3</code> Website Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format genomics <code>shapeit</code> <code>2.904</code> Website SHAPEIT is a fast and accurate method for estimation of haplotypes (aka phasing) from genotype or sequencing data. genomics <code>shapeit4</code> <code>4.2.2</code> Website SHAPEIT4 is a fast and accurate method for estimation of haplotypes (aka phasing) for SNP array and high coverage sequencing data. The version 4 is a refactored and improved version of the SHAPEIT algorithm with multiple key additional features. genomics <code>shiftx2</code> <code>1.13</code> Website SHIFTX2 predicts both the backbone and side chain 1H, 13C and 15N chemical shifts for proteins using their structural (PDB) coordinates as input. genomics <code>snakemake</code> <code>8.27.1</code> Website The Snakemake workflow management system is a tool to create reproducible and scalable data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment. genomics <code>snpeff</code> <code>4.3</code><code>5.2</code> Website SnpEff is a variant annotation and effect prediction tool. genomics <code>snphylo</code> <code>20141127</code> Website SNPhylo users can construct a phylogenetic tree from a file containing huge SNP data. genomics <code>snptest</code> <code>2.5.6</code> Website SNPTEST is a program for the analysis of single SNP association in genome-wide studies. genomics <code>spaceranger</code> <code>1.2.2</code><code>1.3.1</code><code>2.0.0</code><code>2.0.1</code><code>2.1.0</code> Website Space Ranger is a set of analysis pipelines that process Visium Spatial Gene Expression data with brightfield and fluorescnce microscope images. genomics <code>spades</code> <code>3.15.0</code><code>3.15.2</code><code>3.15.4</code> Website SPAdes is an assembly toolkit containing various assembly pipelines. genomics <code>spliceai</code> <code>1.3.1</code> Website A deep learning-based tool to identify splice variants. genomics <code>sratoolkit</code> <code>3.0.0</code><code>3.0.10</code><code>3.1.0</code><code>3.1.1</code><code>3.2.0</code> Website Sequence Read Archive Toolkit(SRA) genomics <code>staphb-tk</code> <code>2.0.1</code> Website The StaPH-B ToolKit is a Python library of commonly used bioinformatics tools that help to inform public health action. genomics <code>star</code> <code>2.5.2</code><code>2.5.3a</code><code>2.6.1b</code><code>2.7.3a</code><code>2.7.9a</code><code>2.7.10b</code> Website Spliced Transcripts Alignment to a Reference genomics <code>star-fusion</code> <code>1.10.1</code> Website STAR-Fusion further processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set. genomics <code>stringtie</code> <code>2.1.4</code> Website High-performance read alignment, quantification and mutation discovery, next-gen sequencing data. genomics <code>subread</code> <code>2.0.1</code><code>2.0.6</code> Website High-performance read alignment, quantification and mutation discovery, next-gen sequencing data. genomics <code>suite2p</code> <code>0.14.4</code> Website Pipeline for processing two-photon calcium imaging data. genomics <code>svaba</code> <code>1.1.3</code> Website SvABA - Structural variation and indel analysis by assembly. genomics <code>taxonkit</code> <code>0.14.2</code> Website A Practical and Efficient NCBI Taxonomy Toolkit also supports creating NCBI style taxdump files for custom taxonomies. genomics <code>tophat</code> <code>2.1.1</code> Website TopHat is a fast splice junction mapper for RNA-Seq reads. genomics <code>trf</code> <code>4.09</code> Website Tandem Repeats Finder is a program to locate and display tandem repeats in DNA sequences. genomics <code>trimgalore</code> <code>0.6.6</code> Website Trim Galore is a perl wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data. genomics <code>trimmomatic</code> <code>0.38</code> Website A flexible read trimming tool for Illumina NGS data genomics <code>ucsctools</code> <code>1.0</code> Website Tools from the UCSC browser. genomics <code>umi_tools</code> <code>1.1.6</code> Website Tools for dealing with Unique Molecular Identifiers genomics <code>varscan</code> <code>2.4.0</code><code>2.4.1</code><code>2.4.2</code><code>2.4.3</code><code>2.4.4</code> Website VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data. genomics <code>varseq</code> <code>2.6.1</code> Website The free Golden Helix GenomeBrowse tool delivers stunning visualizations of your genomic data that give you the power to see what is occurring at each base pair in your samples. genomics <code>vcf-split</code> <code>0.1.5</code> Website vcf-split splits a combined-sample VCF stream into single-sample VCF files. genomics <code>vcftools</code> <code>0.1.16</code> Website A set of tools written in Perl and C++ for working with VCF files, such as those generated by the 1000 Genomes Project. genomics <code>velocyto</code> <code>0.17.17</code> Website Velocyto is a library for the analysis of RNA velocity. genomics <code>vicaller</code> <code>1.1</code> Website Viral Integration caller (VIcaller) is a bioinformatics tool designed for identifying viral integration events using high-throughput sequencing (HTS) data. genomics <code>vifi</code> <code>072018</code> Website ViFi is a tool for detecting viral integration and fusion mRNA sequences from Next Generation Sequencing data. genomics <code>virsorter</code> <code>2.2.4</code> Website Mining viral signal from microbial genomic data. genomics <code>virusseq</code> <code>072018</code> Website A new algorithmic tool for detecting known viruses and their integration sites using next-generation sequencing of human cancer tissue. imaging <code>deeplabcut</code> <code>3.0.0</code> Website Markerless pose estimation of user-defined features with deep learning for all animals including humans imaging <code>mipav</code> <code>7.3.0</code><code>11.0.3</code> Website The MIPAV (Medical Image Processing, Analysis, and Visualization) application enables quantitative analysis and visualization of medical images of numerous modalities such as PET, MRI, CT, or microscopy. imaging <code>mitograph</code> <code>3.0</code> Website MitoGraph is a fully automated image processing method and software dedicated to calculating the three-dimensional morphology of mitochondria in live cells. imaging <code>monai</code> <code>1.3.2</code> Website MONAI is a set of open-source, freely availalbe collaborative frameworks built for accelerating research ad clinical collaboration in Medical Imaging. imaging <code>openslide</code> <code>3.4.1</code> Website OpenSlide is a C library that provides a simple interface to read whole-slide images (also known as virtual slides). imaging <code>seacr</code> <code>1.3</code> Website SEACR. Sparse Enrichment Analysis for CUT&amp;RUN molecular dynamics <code>gmx_mmpbsa</code> <code>1.6.4</code> Website gmx_MMPBSA is a new tool based on AMBER's MMPBSA.py aiming to perform end-state free energy calculations with GROMACS files. neuroimaging <code>afni</code> <code>16.2.16</code><code>19.3.18</code><code>21.0.04</code><code>22.3.07</code><code>24.1.02</code><code>25.1.08</code> Website AFNI (Analysis of Functional NeuroImages) neuroimaging <code>ants</code> <code>2.2.0</code><code>2.4.0</code><code>2.5.2</code> Website ANTs is a medical image registration and segmentation toolkit. neuroimaging <code>biomedia</code> <code>1.1</code> Website The dHCP structural pipeline performs structural analysis of neonatal brain MRI images (T1 and T2) neuroimaging <code>camino</code> <code>1.0</code> Website Camino is an open-source software toolkit for diffusion MRI processing. neuroimaging <code>clara-train-sdk</code> <code>1.7.1</code> Website NVIDIA Clara Train SDK provides a training framework to help accelerate deep learning training and inference for medical imaging use cases. neuroimaging <code>dcm2nii</code> <code>122015</code> Website dcm2nii is a popular tool for converting images from the complicated formats used by scanner manufacturers (DICOM, PAR/REC) to the simple NIfTI format used by many scientific tools. neuroimaging <code>dcm2niix</code> <code>1.0.20210317</code><code>1.0.20240327</code> Website dcm2niix is designed to convert neuroimaging data from the DICOM format to the NIfTI format. neuroimaging <code>dcmtk</code> <code>3.6.1</code> Website This DICOM ToolKit (DCMTK) package consists of source code, documentation and installation instructions for a set of software libraries and applications implementing part of the DICOM/MEDICOM Standard. neuroimaging <code>dke</code> <code>2.6</code> Website Diffusional Kurtosis Estimator (DKE) is a software tool for post-processing diffusional kurtosis imaging (DKI) datasets that includes a suite of command-line programs along with a graphical user interface (GUI). neuroimaging <code>dsistudio</code> <code>latest</code> Website DSI Studio is a tractography software tool that maps brain connections and correlates findings with neuropsychological disorders. neuroimaging <code>fastsurfer</code> <code>2.4.2</code> Website Segmentation and surface reconstruction of MRI images neuroimaging <code>fix</code> <code>1.06</code> Website FIX attempts to auto-classify ICA components into good vs bad components, so that the bad components can be removed from the 4D FMRI data. neuroimaging <code>fmriprep</code> <code>20.2.3</code><code>21.0.2</code><code>22.0.1</code><code>22.0.2</code> Website fMRIPrep is a functional magnetic resonance imaging (fMRI) data preprocessing pipeline that is designed to provide an easily accessible, state-of-the-art interface that is robust to variations in scan acquisition protocols and that requires minimal user input, while providing easily interpretable and comprehensive error and output reporting. neuroimaging <code>freesurfer</code> <code>6.0.0</code><code>7.1.0</code><code>7.2.0</code><code>7.3.2</code><code>7.4.1</code><code>8.0.0-beta</code><code>8.0.0</code> Website An open source neuroimaging toolkit for processing, analyzing, and visualizing human brain MR images. neuroimaging <code>fsl</code> <code>5.0.11</code><code>6.0.4.singularity</code><code>6.0.4</code><code>6.0.7.11</code> Website FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. neuroimaging <code>fsleyes</code> <code>1.13.0</code> Website FSLeyes (pronounced fossilise) is the new FSL image viewer for 3D and 4D data (replacing FSLView). It does not perform any processing or analysis of images - that is done by separate tools. neuroimaging <code>gradunwarp</code> <code>1.2.0</code> Website gradunwarp is a Python/Numpy package used to unwarp the distorted volumes (due to the gradient field inhomogenities). neuroimaging <code>hdbet</code> <code>2.0.1</code> Website Brain Extraction Tool neuroimaging <code>infantfs</code> <code>20210109</code> Website An open source neuroimaging toolkit for processing, analyzing, and visualizing infant brain MR images. neuroimaging <code>ismrmrd</code> <code>1.0.1</code> Website The siemens_to_ismrmrd convertor is used to convert data from Siemens raw data format into ISMRMRD raw data format. neuroimaging <code>mne</code> <code>0.22.0</code> Website Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data; MEG, EEG, sEEG, ECoG, NIRS, and more. neuroimaging <code>mricrogl</code> <code>1.0.20181111</code> Website MRIcroGL is a cross-platform tool for viewing DICOM and NIfTI format images. neuroimaging <code>mrtrix</code> <code>3.0.4</code> Website MRtrix3 provides a set of tools to perform various types of diffusion MRI analyses. neuroimaging <code>mrtrix3tissue</code> <code>5.2.9</code> Website MRtrix3Tissue is a fork of the MRtrix3 project. It aims to add capabilities for 3-Tissue CSD modelling and analysis to a complete version of the MRtrix3 software. neuroimaging <code>pyalfe</code> <code>0.1.1</code> Website PyALFE is an open-source python pipeline for Automated Lesion and Feature Extraction. It can process a clinical brain MRI study consisting of various sequences such as T1, T1 post contrast, FLAIR, T2, ADC, and CBF. neuroimaging <code>pyradiomics</code> <code>3.1.0</code> Website Open-source python package for the extraction of Radiomics features from 2D and 3D images and binary masks. neuroimaging <code>simnibs</code> <code>4.1.0</code> Website SimNIBS is a free and open source software package for the Simulation of Non-invasive Brain Stimulation. neuroimaging <code>workbench</code> <code>1.4.2</code><code>2.0.0</code> Website Connectome Workbench is an open source, freely available visualization and discovery tool used to map neuroimaging data population genetics <code>eigensoft</code> <code>7.2.1</code><code>8.0</code> Website The EIGENSOFT package combines functionality from our population genetics methods (Patterson et al. 2006) and our EIGENSTRAT stratification correction method (Price et al. 2006)."},{"location":"software/cluster-pkg-list/#chemistry","title":"chemistry","text":"Field Module Name Version(s) URL Description computational chemistry <code>cyana</code> <code>3.98.15</code> Website CYANA is a program for automated structure calculation of biological macromolecules on the basis of conformational constraints from NMR. computational chemistry <code>gamess</code> <code>2023R2</code> Website The General Atomic and Molecular Electronic Structure System is a general ab initio quantum chemistry package. computational chemistry <code>garant</code> <code>2.2</code> Website GARANT is a free program that automatically assigns resonances in nuclear magnetic resonance spectra of proteins. computational chemistry <code>libcifpp</code> <code>7.0.9</code> Website Library containing code to manipulate mmCIF and PDB files computational chemistry <code>schrodinger</code> <code>2024-4</code><code>2025-1</code><code>2025-2</code> Website Small Molecule Drug Discovery Suite computational chemistry <code>xeasy</code> <code>1.2</code> Website Interactive computer support of the analysis of NMR spectra for three-dimensional structure determination of biological macromolecules. electrostatics <code>apbs</code> <code>1.5</code> Website APBS (Adaptive Poisson-Boltzmann Solver) solves the equations of continuum electrostatics for large biomolecular assemblages. electrostatics <code>pdb2pqr</code> <code>2.1.1</code> Website PDB2PQR - determining titration states, adding missing atoms, and assigning charges/radii to biomolecules. molecular dynamics <code>ambertools</code> <code>24</code> Website A suite that can be used to carry out complete molecular dynamics simulations. molecular dynamics <code>brer</code> <code>0.0.7</code> Website Python package for running bias-resampling ensemble refinement (BRER) simulations molecular dynamics <code>catdcd</code> <code>4.0</code> Website Catdcd functions much like the Unix cat command; it concatenates DCD files into a single DCD file. molecular dynamics <code>gromacs</code> <code>2024.2</code><code>2019.6</code><code>2021.1</code><code>2021.2</code><code>2022.5</code><code>2023.0</code><code>2025.2</code> Website A free and open-source software suite for high-performance molecular dynamics and output analysis. molecular dynamics <code>gromacs-ramd</code> <code>2020.5</code> Website Random Acceleration Molecular Dynamics (RAMD) molecular dynamics <code>mdanalysis</code> <code>1.0.0</code><code>2.8.0</code> Website MDAnalysis is an object-oriented Python library to analyze trajectories from molecular dynamics (MD) simulations in many popular formats. molecular dynamics <code>mdtraj</code> <code>1.10.2</code> Website MDTraj is a python library that allows users to manipulate molecular dynamics (MD) trajectories. molecular dynamics <code>namd2</code> <code>2.13</code><code>2.14-cuda-mpi</code><code>2.14-mpi</code><code>2.14</code> Website NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. molecular dynamics <code>namd3</code> <code>3.0.1-cuda</code><code>3.0.1</code> Website NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. molecular dynamics <code>plumed</code> <code>2.7.1</code><code>2.8.2</code><code>2.9.0</code><code>2.9.1</code> Website The community-developed PLUgin for MolEcular Dynamics molecular modelling <code>charm</code> <code>50</code> Website A molecular simulation program with broad application to many-particle systems with a comprehensive set of energy functions, a variety of enhanced sampling methods, and support for multi-scale techniques including QM/MM, MM/CG, and a range of implicit solvent models. molecular modelling <code>chimera</code> <code>1.15</code> Website UCSF Chimera is a program for the interactive visualization and analysis of molecular structures and related data, including density maps, trajectories, and sequence alignments. molecular modelling <code>chimerax</code> <code>1.8</code> Website ChimeraX is a visualization program for molecular structures and related data, including density maps, trajectories, and sequence alignments. molecular modelling <code>coot</code> <code>0.9.8.92</code> Website Coot is for macromolecular model building, model completion and validation, particularly suitable for protein modelling using X-ray data. molecular modelling <code>openmm</code> <code>8.2.0</code> Website A high-performance toolkit for molecular simulation. Use it as an application, a library, or a flexible programming environment. molecular visualization <code>pymol</code> <code>3.0.0</code><code>3.1.0</code> Website PyMOL is a user-sponsored molecular visualization system on an open-source foundation, maintained and distributed by Schr\u00f6dinger. molecular visualization <code>vmd</code> <code>1.9.3</code><code>2.0.0a4</code> Website VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. tools <code>openbabel</code> <code>2.4.1</code> Website Open Babel is a chemical toolbox designed to speak the many languages of chemical data."},{"location":"software/cluster-pkg-list/#devel","title":"devel","text":"Field Module Name Version(s) URL Description compiler <code>binutils</code> <code>2.37</code> Website The GNU Binary Utilities, or binutils, are a set of programming tools for creating and managing binary programs, object files, libraries, profile data, and assembly source code. compiler <code>gcc</code> <code>9.3.0</code><code>9.5.0</code><code>11.3.0</code><code>13.3.0</code> Website The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, Go, and D, as well as libraries for these languages. compiler <code>intel</code> <code>2021.1.1</code> Website Intel Compiler Family (C/C++/Fortran for x86_64) compiler <code>intel-oneapi</code> <code>2025.1.0</code> Website Intel Compiler Family (C/C++/Fortran for x86_64) data <code>netcdf/gcc/64</code> <code>4.7.4</code> Website NetCDF (Network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. data <code>netcdf/intel/64</code> <code>4.7.4</code> Website NetCDF (Network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. data <code>pnetcdf</code> <code>1.12.1</code> Website PnetCDF is a high-performance parallel I/O library for accessing files in format compatibility debugger <code>gdb</code> <code>gcc/64/12.1</code> Website GDB, the GNU Project debugger, allows you to see what is going on inside another program while it executes -- or what another program was doing at the moment it crashed. debugger <code>valgrind</code> <code>3.16.1</code> Website Dynamic memory testing and debugging tools language <code>cuda</code> <code>7.5</code><code>8.0</code><code>9.1</code><code>11.0.2</code><code>11.1.1</code><code>11.2.2</code><code>11.5.0</code><code>11.7.0</code><code>11.8.0</code><code>12.3</code> Website NVIDIA CUDA Toolkit for Linux language <code>go</code> <code>1.15.6</code><code>1.22.2</code> Website open source programming language language <code>jdk</code> <code>11.0.11</code><code>17.0.8</code><code>22.0.2</code> Website Java is a high-level, class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible. language <code>julia</code> <code>1.10.2</code><code>1.10.4</code> Website The Julia programming language is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. language <code>lua</code> <code>5.1.4.9</code><code>5.4.3</code> Website Lua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description. language <code>openjdk</code> <code>16.0.1</code><code>17.0.5.8</code><code>19.36.2238</code><code>21.0.2</code> Website Java\u2122 is the world's leading programming language and platform. AdoptOpenJDK uses infrastructure, build and test scripts to produce prebuilt binaries from OpenJDK\u2122 class libraries and a choice of either OpenJDK or the Eclipse OpenJ9 VM.. language <code>perl</code> <code>5.26.1</code> Website Perl is a highly capable, feature-rich programming language with over 30 years of development. language <code>python</code> <code>3.9.1</code><code>intel-2021.1</code><code>2.7.18</code><code>3.10.16</code><code>3.12.10</code> Website scientific scripting package language <code>ruby</code> <code>3.4.3</code> Website A dynamic, open source programming language with a focus on simplicity and productivity. lib <code>boost</code> <code>openmpi/gcc/1.75.0</code><code>impi/intel/1.75.0</code> Website Boost provides free peer-reviewed portable C++ source libraries. lib <code>hdf5/impi/intel/64</code> <code>1.12.0</code> Website General purpose library and file format for storing scientific data. lib <code>hdf5/openmpi/gcc/64</code> <code>1.12.0</code> Website General purpose library and file format for storing scientific data. lib <code>nccl</code> <code>2.8.4-11.1.1</code> Website The NVIDIA Collective Communication Library (NCCL) implements multi-GPU and multi-node communication primitives optimized for NVIDIA GPUs and Networking. lib <code>numba</code> <code>0.60.0</code> Website Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. lib <code>opencv</code> <code>4.5.1</code> Website Open Source Computer Vision Library mpi <code>impi</code> <code>2021.1.1</code><code>2021.15</code> Website Intel MPI Library (C/C++/Fortran for x86_64) mpi <code>openmpi</code> <code>gcc/64/4.1.0</code> Website OpenMPI Library (C/C++/Fortran for x86_64) networking <code>pmix</code> <code>3.1.6</code><code>5.0.2</code> Website Fabric communication services networking <code>ucx</code> <code>1.9.0</code><code>1.14.1</code> Website Fabric communication services profiling <code>intel-advisor</code> <code>2021.1.1</code> Website Intel Advisor text editor <code>emacs</code> <code>26.3</code><code>27.1</code> Website An extensible, customizable, free/libre text editor \u2014 and more."},{"location":"software/cluster-pkg-list/#math","title":"math","text":"Field Module Name Version(s) URL Description analytics <code>arrow</code> <code>4.0.0</code> Website Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. computational fluid dynamics <code>ansys</code> <code>2021R1</code><code>2022R1</code><code>2024R1</code><code>2024R2</code> Website CAE/multiphysics engineering simulation software for product design, testing and operation data analytics <code>spark</code> <code>hadoop3.2/3.1.2</code> Website Apache Spark\u2122 is a unified analytics engine for large-scale data processing deep learning <code>aif360</code> <code>0.6.1</code> Website The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. deep learning <code>cudnn</code> <code>8.0.5-11.0.2</code><code>8.0.5-11.1.1</code><code>8.1.0-11.2.2</code><code>8.3.0-11.5.0</code><code>8.6.0-11.x</code><code>8.9.3-11.x</code><code>8.9.7-12.x</code><code>9.1.1-12.x</code> Website GPU-accelerated library of primitives for deep neural networks. deep learning <code>libtensorflowcc</code> <code>1.14.0</code> Website TensorFlow is an end-to-end open source platform for machine learning. deep learning <code>mxnet</code> <code>1.8.0</code> Website A truly open source deep learning framework suited for flexible research prototyping and production. deep learning <code>pytorch</code> <code>2.5.1</code><code>2.6.0</code> Website PyTorch - An open source machine learning framework that accelerates the path from research prototyping to production deployment. deep learning <code>tensorflow</code> <code>2.16.1</code><code>2.18.0</code> Website TensorFlow is an end-to-end open source platform for machine learning. finite element analysis <code>febiostudio</code> <code>2.7</code> Website FEBio is a software tool for nonlinear finite element analysis in biomechanics and biophysics and is specifically focused on solving nonlinear large deformation problems in biomechanics and biophysics. finite element analysis <code>lsdyna</code> <code>8.0.0</code><code>8.1.0</code><code>9.0.1</code><code>9.3.1</code><code>10.2.0</code><code>11.2.0</code><code>12.0.0</code><code>13.0.0</code><code>13.1.1</code><code>14.1.0</code> Website Advanced general-purpose multiphysics simulation software. finite element analysis <code>lsprepost</code> <code>4.8.17</code> Website finite element analysis gis <code>gdal</code> <code>2.4.4</code><code>3.8.5</code> Website GDAL is a translator library for raster and vector geospatial data formats gis <code>geos</code> <code>3.9.1</code><code>3.13.0</code> Website GEOS is a C/C++ library for computational geometry with a focus on algorithms used in geographic information systems (GIS) software. gis <code>proj</code> <code>5.1.0</code><code>9.4.0</code> Website PROJ is a generic coordinate transformation software that transforms geospatial coordinates from one coordinate reference system (CRS) to another. gis <code>udunits</code> <code>2.2.28</code> Website The UDUNITS-2 package provides support for units of physical quantities. lib <code>fftw/impi/intel/64</code> <code>2.1.5</code><code>3.3.9</code> Website Numerical library, contains discrete Fourier transformation lib <code>fftw/openmpi/gcc/64</code> <code>3.3.9</code> Website Numerical library, contains discrete Fourier transformation machine learning <code>ollama</code> <code>0.3.14</code> Website Get up and running with large language models. numerical analysis <code>mcr</code> <code>v81</code><code>v83</code><code>v84</code><code>v93</code><code>v96</code><code>v97</code><code>v99</code><code>v713</code><code>v717</code><code>v914</code> Website The MATLAB Runtime is a standalone set of shared libraries that enables the execution of compiled MATLAB, Simulink applications, or components. numerical analysis <code>mosek</code> <code>8</code> Website MOSEK solves all your LPs, QPs, SOCPs, SDPs and MIPs. Includes interfaces to C, C++, Java, MATLAB, .NET, Python and R. numerical analysis <code>mplus</code> <code>8.3</code> Website Mplus is a latent variable modeling program numerical library <code>blas</code> <code>3.9.0</code> Website The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. numerical library <code>glpk</code> <code>4.65</code> Website The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. numerical library <code>gsl</code> <code>2.2</code><code>2.6</code><code>2.7</code> Website The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. numerical library <code>lapack</code> <code>3.9.0</code><code>3.10.1</code> Website LAPACK (Linear Algebra Package) is a standard software library for numerical linear algebra. numerical library <code>mkl</code> <code>2021.1.1</code> Website Intel oneAPI Math Kernel Library numerical library <code>openblas</code> <code>0.3.13</code><code>0.3.27</code> Website OpenBLAS is an optimized BLAS (Basic Linear Algebra Subprograms) library based on GotoBLAS2 1.13 BSD version. optimization <code>nlopt</code> <code>2.7.1</code> Website NLopt is a free/open-source library for nonlinear optimization. simulation <code>geant4</code> <code>10.4.2</code> Website Geant4 is a toolkit for the simulation of the passage of particles through matter. statistics <code>jags</code> <code>4.3.0</code><code>4.3.1</code><code>4.3.2</code> Website Just another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC). statistics <code>R</code> <code>4.0.4</code><code>4.1.1</code><code>4.2.1</code><code>4.3.1</code><code>4.3.3</code><code>4.4.0</code><code>4.4.2</code><code>4.5.0</code> Website statistical computing package statistics <code>rjags</code> <code>4.6</code> Website JAGS is Just Another Gibbs Sampler.  It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation  not wholly unlike BUGS. statistics <code>rstudio</code> <code>2022.02.1-461</code> Website RStudio is an integrated development environment (IDE) for R. statistics <code>rstudio-server</code> <code>2025.05.0-496</code><code>2025.05.1-513</code> Website RStudio Server enables you to provide a browser based interface to a version of R running on a remote Linux server, bringing the power and productivity of the RStudio IDE to server-based deployments of R. statistics <code>sas</code> <code>94-15.3</code> Website Providing a scalable, integrated software environment designed for data access, transformation, and reporting. statistics <code>spm</code> <code>12</code> Website The SPM software package has been designed for the analysis of brain imaging data sequences."},{"location":"software/cluster-pkg-list/#system","title":"system","text":"Field Module Name Version(s) URL Description benchmark <code>ior</code> <code>3.0.1</code> Website IOR is a parallel IO benchmark that can be used to test the performance of parallel storage systems using various interfaces and access patterns. build <code>cmake</code> <code>3.19.2</code><code>3.22.2</code><code>3.30.3</code> Website tool for generation of files from source build <code>patch</code> <code>2.7.6</code> Website Patch takes a patch file containing a difference listing produced by the diff program and applies those differences to one or more original files, producing patched versions. checkpointing <code>dmtcp</code> <code>2.6.0</code><code>3.1.2</code> Website DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space -- with no modifications to user code or to the O/S. cloud interface <code>awscli</code> <code>2.1.37</code><code>2.7.32</code><code>2.22.35</code> Website AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. compression <code>pigz</code> <code>2.6</code> Website pigz, which stands for parallel implementation of gzip, is a fully functional replacement for gzip that exploits multiple processors and multiple cores to the hilt when compressing data. containers <code>apptainer</code> <code>1.3.0</code> Website A tool for running containers on a hpc system. containers <code>singularity</code> <code>3.7.1</code> Website A tool for running containers on a hpc system. data management <code>mpifileutils</code> <code>0.11</code> Website mpiFileUtils provides both a library called libmfu and a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files. database <code>duckdb</code> <code>1.3.1</code> Website DuckDB is a fast portable in-process database system. document processing <code>texinfo</code> <code>6.8</code><code>7.1</code> Website Texinfo uses a single source file to produce output in a number of formats. document processing <code>texlive</code> <code>2020</code><code>2024</code> Website TeX Live is intended to be a straightforward way to get up and running with the TeX document production system. file transfer <code>aspera-connect</code> <code>3.11.2.63</code> Website High-performance transfer client file transfer <code>globus-cli</code> <code>1.0</code> Website The CLI provides an interface to Globus services from the shell, and is suited to both interactive and simple scripting use cases. It is open source and available at https://github.com/globus/globus-cli file transfer <code>globusconnect</code> <code>3.2.2</code><code>3.2.5</code> Website Globus Connect Personal turns your laptop or other personal computer into a Globus endpoint and allows you to transfer and share files easily. file transfer <code>rclone</code> <code>1.57.0</code><code>1.67.0</code><code>1.69.2</code> Website Rclone is a command line program to manage files on cloud storage. ide <code>code-server</code> <code>4.12.0</code><code>4.20.0</code><code>4.92.2</code><code>4.100.3</code> Website Run VS Code on any machine anywhere and access it in the browser. ide <code>cursor</code> <code>0.45.5</code> Website Cursor is a code editor built for programming with AI. ide <code>spyder</code> <code>6.0.3</code> Website Spyder is a free and open source scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts. ide <code>vscode</code> <code>1.99.3</code> Website VS Code is a free code editor imaging <code>giflib</code> <code>4.2.3</code> Website A library and utilities for processing GIFs. lib <code>libevent</code> <code>2.0.22</code> Website The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. lib <code>libfabric</code> <code>1.11.2</code> Website Libfabric OFI lib <code>openjpeg</code> <code>1.5.0</code> Website OpenJPEG is an open-source JPEG 2000 codec written in C language. library <code>hwloc</code> <code>2.3.0</code><code>2.9.3</code><code>2.11.2</code> Website Portable Hardware Locality (hwloc) library <code>readline</code> <code>6.2.p5</code> Website The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. media <code>ffmpeg</code> <code>4.4</code> Website A complete, cross-platform solution to record, convert and stream audio and video. package manager <code>miniconda3</code> <code>4.9.2</code> Website Miniconda is a small, bootstrap version of Anaconda. package manager <code>miniforge</code> <code>missing</code> Website Miniforge is a conda installer based off conda-forge. pipeline <code>nextflow</code> <code>24.10.4</code><code>25.04.4</code> Website Software pipeline for automating omics-scale protein modeling and simulation setup.  http://ensembler.readthedocs.org/ profiling <code>monitor</code> <code>2.1.3</code> Website A simple cross-platform system resource monitor. profiling <code>remora</code> <code>1.8.5-impi</code><code>1.8.5</code> Website REsource MOnitoring for Remote Applications remote display <code>virtualgl</code> <code>2.6.5</code><code>3.0</code> Website VirtualGL is an open source toolkit that gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration. scm <code>git</code> <code>2.30.0</code><code>2.48.1</code> Website Fast Version Control System terminal <code>tmux</code> <code>3.1c</code> Website tmux is a terminal multiplexer; it enables a number of terminals to be created, accessed, and controlled from a single screen. terminal <code>ttyd</code> <code>1.6.3</code> Website ttyd is a simple command-line tool for sharing terminal over the web. tools <code>imagemagick</code> <code>7.0.10</code><code>7.1.1</code> Website Use ImageMagick to create, edit, compose, or convert digital images. tools <code>lmod</code> <code>8.7.8</code> Website An environment module system tools <code>parallel</code> <code>20210122</code> Website GNU parallel is a shell tool for executing jobs in parallel using one or more computers. tools <code>povray</code> <code>3.7.0.0</code> Website The Persistence of Vision Raytracer is a high-quality, Free Software tool for creating stunning three-dimensional graphics. utilities <code>pandoc</code> <code>2.11.4</code> Website Pandoc is a universal document converter."},{"location":"software/cluster-pkg-list/#viz","title":"viz","text":"Field Module Name Version(s) URL Description graphics <code>blender</code> <code>2.79b</code><code>3.0.1</code> Website Blender is the free and open source 3D creation suite. graphics <code>ilastik</code> <code>1.3.3</code><code>1.4.0rc6</code> Website the interactive learning and segmentation toolkit graphics <code>qupath</code> <code>0.3.2</code><code>0.5.0</code><code>0.5.1</code> Website QuPath is open source software for bioimage analysis. graphs <code>graphviz</code> <code>2.40.1</code> Website Graph visualization is a way of representing structural information as diagrams of abstract graphs and networks. graphs <code>mcl</code> <code>14.137</code> Website The MCL algorithm is short for the Markov Cluster Algorithm, a fast and scalable unsupervised cluster algorithm for graphs (also known as networks) based on simulation of (stochastic) flow in graphs. graphs <code>orca</code> <code>1.3.1</code> Website Command line application for generating static images of interactive plotly charts imaging <code>kaleido</code> <code>0.2.1</code> Website Kaleido is a cross-platform library for generating static images (e.g. png, svg, pdf, etc.) for web-based visualization libraries, with a particular focus on eliminating external dependencies. imaging <code>netpbm</code> <code>10.73.30</code> Website A whole bunch of utilities for primitive manipulation of graphic images. molecular visualization <code>alevinfry</code> <code>0.11.1</code> Website alevin-fry is a suite of tools for the rapid, accurate and memory-frugal processing single-cell and single-nucleus sequencing data. molecular visualization <code>annotsv</code> <code>3.4.4</code> Website AnnotSV is a standalone program designed for annotating and ranking Structural Variations."},{"location":"software/conda/","title":"Conda","text":""},{"location":"software/conda/#conda","title":"Conda","text":"<p>Do not use Anaconda products without a license</p> <p>Anaconda products, including Anaconda Distribution and Miniconda, rely on proprietary package repositories by default. Anaconda's proprietary package repositories are not free to use, even for academic research, and RCC does not centrally license them for your use. While there are free and open-source package repositories (conda-forge, bioconda, etc.), Anaconda products do not use these by default. Research Computing is using Miniforge as an alternative and recommends all users do the same.</p> <p>Conda is a package management system that quickly installs many useful packages and their dependencies. We use a free and open-source version called Miniforge. It is mainly used for Python virtual environments, but includes many more open-source research software packages and dependencies.</p>"},{"location":"software/conda/#benefits-of-using-a-conda-environment","title":"Benefits of using a Conda environment","text":"<ul> <li>Creating isolated conda environments is a perfect way to manage dependencies. Some times you might need to use different programs with different dependencies that conflict with each other. With conda, you can isolate each of those programs with their corresponding dependencies.</li> <li>With conda it is very easy to keep track of what you have installed and remove, add or update any packages.</li> <li>You can create a \"perfect\" environment for a specific project and share it with others to make sure they are using the same version of the packages. This will also allow you to create reproducible pipelines, to ensure that you run and re-run your programs within the exact same environment.</li> <li>Conda has a large community of developers and is very easy to find help in forums whenever you run into any problems.</li> <li>It supports packages written in other languages, so, you can create dedicated environments for certain projects which depend on libraries written in other languages such as R.</li> </ul>"},{"location":"software/conda/#using-conda-on-the-cluster","title":"Using conda on the cluster","text":"<p>Research Computing provides a central installation of Miniforge. All users are encouraged to use this module to create custom environments as needed.</p> <p>To load Miniforge:</p> <pre><code>module load miniforge\n</code></pre>"},{"location":"software/conda/#create-an-environment","title":"Create an environment","text":"<p>To create a new environment (e.g., myenv):</p> <pre><code>conda create -n myenv\n</code></pre>"},{"location":"software/conda/#use-an-environment","title":"Use an environment","text":"<p>To use your environment:</p> <pre><code>module load miniforge\nconda activate myenv\n</code></pre>"},{"location":"software/conda/#install-packages-in-an-environment","title":"Install packages in an environment","text":"<p>To install packages in your conda virtual environment:</p> <pre><code>conda install numpy\n</code></pre> <p>To install a specific version of a package in your conda virtual environment:</p> <pre><code>conda activate myenv\nconda install python==3.8\n</code></pre> <p>To see all the packages that you have installed in your virtual environment:</p> <pre><code>conda activate myenv\nconda env list\n</code></pre> <p>To see what version of a package you have installed in your virtual environment:</p> <pre><code>conda activate myenv\nconda env list | grep package_name\n</code></pre> <p>To uninstall a package within your virtual environment:</p> <pre><code>conda activate myenv\nconda uninstall numpy\n</code></pre>"},{"location":"software/conda/#clone-an-environment","title":"Clone an environment","text":"<p>Suppose you are starting a new pipeline of a project for which you need a very similar environment to one you created before, with the exception of a few packages that use different versions. You could clone your previous environment and then make the desired changes. Make sure to deactivate the environment your are cloning, clone it and then activate your new environment.</p> <pre><code>conda deactivate myenv1\nconda create -n myenv1 --clone myenv2\nconda activate myenv2\n</code></pre> <p>You could also create a specification file to build an identical conda environment on the same OS either in the same or a different machine. This is perfect for sharing conda environments with collaborators. Make sure the people you share your specification file with are using your same platform. Otherwise, the packages that you installed might not be available to them or some dependencies could be missing. In this case, things might not work exactly the same as expected.</p> <p>To create a specification file:</p> <pre><code>conda activate myenv\nconda list --explicit &gt; spec-file.txt\n</code></pre> <p>To use a specification file to create an identical environment on the same or a different machine:</p> <pre><code>conda create --name myenv --file spec-file.txt\nconda activate myenv\n</code></pre> <p>To use a spec file to install packages into an existing environment:</p> <pre><code>conda install --name myenv \nconda activate myenv --file spec-file.txt\n</code></pre>"},{"location":"software/conda/#remove-an-environment","title":"Remove an environment","text":"<p>To completely remove a conda env:</p> <pre><code>conda remove --name myenv --all\n</code></pre> <p>To remove all packages within an environment:</p> <pre><code>conda remove -n myenv --all --keep-env\n</code></pre> <p>The <code>--keep-env</code> option leaves the environment intact. Use this option if you'd like to start over, but do not want to rename the environment.</p>"},{"location":"software/conda/#common-issues","title":"Common issues","text":"<p>Users should avoid mixing conda and non-conda modules in cluster jobs and workflows. For example, a pipeline requiring Python and TensorFlow should use <code>module load tensorflow</code> and not <code>module load tensorflow python</code>. The first loads the TensorFlow module which is conda based and includes Python already. The second loads the conda-based TensorFlow module and non-conda-based Python module, which may not give the desired version of Python. This simple example can be compounded with job scripts that load many modules.</p> <p>Users should also avoid using the <code>conda init</code> command. If you accidentally setup conda to auto-initialize via your <code>.bashrc</code> or <code>.bash_profile</code> scripts at login, unintended issues may occur. For example, subsequent module loads may not work, or system commands may conflict with packages within your auto-init conda env. Users that have this issue will see an env name in front of their username at login, i.e. <code>(myenv) [netid@server ~]</code>. You can fix this issue with <code>conda config --set auto_activate_base false</code>.</p>"},{"location":"software/conda/#additional-help","title":"Additional help","text":"<p>The conda docs provide much more information than is practical to include here. Consult the Managing Environments guide for more information on conda envs.</p>"},{"location":"software/containers/","title":"Software Containers","text":""},{"location":"software/containers/#software-containers","title":"Software Containers","text":""},{"location":"software/containers/#overview","title":"Overview","text":"<p>Apptainer (formerly Singularity) is a container solution designed for high-performance computing systems. A container is a collection of application and dependency files, which is packaged as a single portable image file. Containers are independent from the host operating system, allowing applications that are not natively supported to run on a variety of HPC resources. They are conceptually similar to Docker, but focus more on HPC. Apptainer containers can be shared and distributed to support reproducible research and can be run on most HPC systems without modification as long as Apptainer is installed.</p>"},{"location":"software/containers/#apptainer-commands","title":"Apptainer Commands","text":"<p>Apptainer uses a series of subcommands and options controlled by a wrapper script called <code>apptainer</code>. The <code>-h</code> option displays the command-line help documentation:</p> <pre><code>$ module load apptainer\n$ apptainer -h\n\nLinux container platform optimized for High Performance Computing (HPC) and\nEnterprise Performance Computing (EPC)\n\nUsage:\n  apptainer [global options...]\n\nDescription:\n  Apptainer containers provide an application virtualization layer enabling\n  mobility of compute via both application and environment portability. With\n  Apptainer one is capable of building a root file system that runs on any\n  other Linux system where Apptainer is installed.\n\nOptions:\n      --build-config    use configuration needed for building containers\n  -c, --config string   specify a configuration file (for root or\n                        unprivileged installation only) (default\n                        \"/hpc/apps/apptainer/1.3.0/etc/apptainer/apptainer.conf\")\n  -d, --debug           print debugging information (highest verbosity)\n  -h, --help            help for apptainer\n      --nocolor         print without color output (default False)\n  -q, --quiet           suppress normal output\n  -s, --silent          only print errors\n  -v, --verbose         print additional information\n      --version         version for apptainer\n\nAvailable Commands:\n  build       Build an Apptainer image\n  cache       Manage the local cache\n  capability  Manage Linux capabilities for users and groups\n  checkpoint  Manage container checkpoint state (experimental)\n  completion  Generate the autocompletion script for the specified shell\n  config      Manage various apptainer configuration (root user only)\n  delete      Deletes requested image from the library\n  exec        Run a command within a container\n  help        Help about any command\n  inspect     Show metadata for an image\n  instance    Manage containers running as services\n  key         Manage OpenPGP keys\n  keyserver   Manage apptainer keyservers\n  oci         Manage OCI containers\n  overlay     Manage an EXT3 writable overlay image\n  plugin      Manage Apptainer plugins\n  pull        Pull an image from a URI\n  push        Upload image to the provided URI\n  registry    Manage authentication to OCI/Docker registries\n  remote      Manage apptainer remote endpoints\n  run         Run the user-defined default command within a container\n  run-help    Show the user-defined help for an image\n  search      Search a Container Library for images\n  shell       Run a shell within a container\n  sif         Manipulate Singularity Image Format (SIF) images\n  sign        Add digital signature(s) to an image\n  test        Run the user-defined tests within a container\n  verify      Verify digital signature(s) within an image\n  version     Show the version for Apptainer\n\nExamples:\n  $ apptainer help &lt;command&gt; [&lt;subcommand&gt;]\n  $ apptainer help build\n  $ apptainer help instance start\n\n\nFor additional help or support, please visit https://apptainer.org/help/\n</code></pre> <p>For more information see the Apptainer Quick Start guide.</p>"},{"location":"software/containers/#download-pre-built-containers","title":"Download Pre-built Containers","text":"<p>Many software packages already have containers built by developers that you can download with the <code>pull</code> or <code>build</code> commands. These containers are available from external sites such as the Container Library or Docker Hub.</p> <p>The <code>apptainer pull</code> command will download a OCI image from a remote repository, combining the layers to a usable SIF image.</p> <pre><code>$ apptainer pull docker://ghcr.io/apptainer/lolcow\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nINFO:    Creating SIF file...\n$ ./lolcow_latest.sif\n ______________________________\n&lt;                              &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>The <code>build</code> command has many more uses for building containers from scratch or converting formats, which we cover later. For the purpose of downloading a pre-built container, <code>build</code> differs from <code>pull</code> by converting the image to the latest Apptainer image format after downloading it. Further, <code>build</code> allows for custom naming of the downloaded container.</p> <pre><code>$ apptainer build my_lolcow.sif docker://ghcr.io/apptainer/lolcow\nINFO:    Starting build...\nINFO:    Creating SIF file...\nINFO:    Build complete: my_lolcow.sif\n$ ./my_lolcow.sif\n ______________________________\n&lt;                              &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>Because these containers are already built, they are immutable. So, you can run things but not install things inside them. To do this, you would build a container, which is explained in the next section.</p>"},{"location":"software/containers/#build-a-container","title":"Build a Container","text":"<p>The <code>build</code> command is used to create new containers either from scratch or based on existing containers. It can also used to convert between container formats. As an example, we'll discuss the process of building custom Python container from scratch.</p> <p>Please see the Apptainer docs for more info.</p>"},{"location":"software/containers/#definition-file","title":"Definition File","text":"<p>First we write a definition file for the new container. A definition file defines the container build process including software installation, runtime functionality, etc. This file can be named how you like, but it is recommended to denote definition files with the <code>.def</code> extension. We will call our definition file <code>py_container.def</code>. See below for an explanation of each section.</p> py_container.def <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%help\n  This container includes Python.\n\n%environment\n    export PATH=/opt/python/bin:$PATH\n    export LD_LIBRARY_PATH=/opt/python/lib:$LD_LIBRARY_PATH\n\n%post\n    # Create useful bind points for needed file systems. The following are set by default in RCC and should always be included for containers used in RCC.\n    mkdir -p /scratch /hpc\n\n    # Install necessary packages.\n    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n        build-essential \\\n        gcc-multilib \\\n        ca-certificates \\\n        zlib1g-dev \\\n        libssl-dev \\\n        curl\n    apt-get clean\n    rm -rf /var/lib/apt/lists/*\n\n    # Install Python from source.\n    export PATH=/opt/python/bin:$PATH\n    curl https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz -o Python-2.7.15.tgz\n    tar -xvf Python-2.7.15.tgz &amp;&amp; cd Python-2.7.15\n    ./configure --prefix=/opt/python &amp;&amp; make &amp;&amp; make install\n    rm -rf Python-2.7.15*\n\n    # Install pip package manager.\n    curl https://bootstrap.pypa.io/get-pip.py | python\n\n    # Install some useful Python packages.\n    pip install --upgrade numpy scipy matplotlib \n\n%test\n    /opt/python/bin/python --version\n</code></pre> <p>As you can see in the previous example, a definition file has two parts. First is the header, which contains information about the base operating system for the container. <code>Bootstrap</code> is the first keyword and it's mandatory. It will specify the bootstrap agent. The most common ones are: <code>library</code>, <code>docker</code>, <code>shub</code>, or <code>localimage</code>. When using <code>library</code> as bootstrap agent, the next keyword must be <code>From</code>, followed by the operating system and the version. For example, <code>From: ubuntu:22.04</code>.</p> <p>The second part of a definition file is the sections, which execute commands at different times while the container is built or run. All sections are optional. The following table explains them in detail:</p> Section Explanation <code>%files</code> Use this section to copy files from the host into the container. Never copy files into /home, /tmp, or any other directories that are bound at runtime. <code>%environment</code> Specify variables that will be set at run time in this section (NOT available when building the container). Variables set here are global (available to all apps). <code>%post</code> In this section you can: specify variables that will be set at build time, download files from the internet with tools like git and wget, install new software and libraries, write configuration files, create new directories. Never install things into /home, /tmp, or any other directories that are bound at runtime. <code>%runscript</code> This section is executed when the container is run via the <code>apptainer run</code> command or by executing the container directly as a command. Any arguments passed to the container will be processed in this section. We can use here variables set in<code>%environment</code> or <code>%post</code>. <code>%test</code> This section runs at the end of the build process to validate the container using a method of your choice. <code>%labels</code> You can add metadata in this section with name-value format such as <code>Author Me</code> or <code>Version v1.0</code>. <code>%help</code> Use this section to explain to the user how to interact with the container. <p>Full documentation of definition files at Apptainer Docs.</p> <p>Examples of definition files: Apptainer examples.</p>"},{"location":"software/containers/#build","title":"Build","text":"<p>Build the container using the definition file:</p> <pre><code>sudo apptainer build py_container.sif py_container.def\n</code></pre> <p>The syntax is always <code>apptainer build</code> followed by the new container name, denoted by the <code>.sif</code> extension, and the corresponding definition file.</p> <p>root or sudo required</p> <p>Building from a definition file requires <code>root</code> or <code>sudo</code> permissions. This is not possible on the HPC cluster. Instead we recommend to use a virtual machine running on your desktop or send a container build request to help-rcc@mcw.edu.</p>"},{"location":"software/containers/#test","title":"Test","text":"<p>Copy your container file to your RCC home directory and run a test:</p> <pre><code>$ srun --ntasks=1 --mem-per-cpu=4GB --time=01:00:00 --job-name=interactive --account={PI_NetID} --pty bash\n$ module load apptainer\n$ apptainer exec py_container.sif python --version\nPython 2.7.15\n</code></pre> <p>Here we print the version of Python installed in the container. The exec command executes a command (i.e., <code>python --version</code>) within the container.</p>"},{"location":"software/containers/#containers-in-jobs","title":"Containers in Jobs","text":"<p>Apptainer containers were designed to be run on HPC systems. RCC maintains containers for several software packages on the clusters. The following examples show an RCC built container which can be run in an interactive or batch job.</p>"},{"location":"software/containers/#example-interactive-job","title":"Example Interactive job","text":"<p>Start an interactive job on the cluster:</p> <pre><code>srun --ntasks=1 --mem-per-cpu=4GB --time=01:00:00 --job-name=interactive --account={PI_NetID} --pty bash\n</code></pre> <p>Load the Apptainer module:</p> <pre><code>module load apptainer\n</code></pre> <p>Run a command in your container:</p> <pre><code>$ apptainer exec  py_container.sif python hello_world.py\nHello, World!\n</code></pre> <p>The exec command is used to execute the <code>hello_world.py</code> script within the container.</p> <p>Alternatively, shell into the container and run a command:</p> <pre><code>$ apptainer shell  py_container.sif \nApptainer&gt; python hello_world.py \nHello, World!\n</code></pre> <p>If your container has a %runscript section defined, then you could also use the <code>apptainer run</code> command. Choose the method that works best for you.</p>"},{"location":"software/containers/#example-batch-job","title":"Example Batch Job","text":"container.sh <pre><code>#!/bin/bash\n#SBATCH --job-name=Testing\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00\n#SBATCH --account=&lt;PI_NetID&gt;\n#SBATCH --output=%x-%j.out\n\nmodule load apptainer\napptainer exec py_container.sif python hello_world.py\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch container.sh\n</code></pre> <p>This job will execute the <code>hello_world.py</code> script within the <code>py_container.sif</code> container on a compute node. The output file should contain <code>Hello, World!</code>.</p>"},{"location":"software/containers/#apptainer-on-rcc","title":"Apptainer on RCC","text":"<p>RCC uses Apptainer to install and maintain software packages that would not otherwise be available on the compute clusters. This is often due to incompatible libraries or unsupported operating system. In some cases Apptainer is used to control access or functionality of an application. RCC-built containers are designed to interact with RCC clusters. They include necessary file mount points, special libraries (e.g., CUDA), and custom wrapper commands. Here we will discuss the elements needed to make your containers compatible with RCC systems.</p> <p>A set of useful directories are mounted by default on RCC clusters. The following file mounts should be included for non-GPU containers:</p> <pre><code>/hpc\n/scratch\n</code></pre> <p>Add to the <code>%post</code> section of your definition file:</p> <pre><code>mkdir -p /hpc /scratch\n</code></pre>"},{"location":"software/containers/#apptainer-and-docker","title":"Apptainer and Docker","text":"<p>Apptainer is conceptually similar to Docker and easily supports Docker images. We've shown above that Apptainer can be used to download and Apptainer-ize Docker images. It can also be used to build containers starting from a Docker image base (RCC does this). This allows RCC to support a wide variety of software that is already containerized with Docker. If you can find a container on Docker Hub, chances are that it will be supported through Apptainer. If you already use Docker to containerize your software, then you can continue developing in Docker knowing that Apptainer will easily import your images.</p> <p>Full information on Apptainer and Docker at Apptainer Docs.</p>"},{"location":"software/containers/#getting-help","title":"Getting Help","text":"<p>Contact help-rcc@mcw.edu for assistance with containers.</p>"},{"location":"software/git/","title":"Git and GitHub","text":""},{"location":"software/git/#git-and-github","title":"Git and GitHub","text":"<p>This is a beginner's guide to using git and GitHub. These are version control tools that are commonly used by software developers. We can use them in our computational research to better organize our scripts, collaborate with others, and prevent accidental loss of data. Here we will discuss step-by-step how to use git and GitHub.</p> <p>Tutorials</p> <p>You will need git installed if you want to follow along with the included tutorials. The HPC cluster login nodes have git installed. You can also install git on your MacOS or Windows computer.</p>"},{"location":"software/git/#version-control","title":"Version Control","text":"<p>Version control is a method to track changes on files as you work. The idea is similar to Microsoft Word track changes. For personal use, it allows you to manage and undo edits if needed. In a group setting, version control allows many users to contribute to the same files without conflict. If you've ever spent hours working on a script or document only to accidentally overwrite your changes, then you know the consequences of not using version control. Whether you're a full-stack developer or simply writing a one-off R script, version control will simplify and save your work.</p>"},{"location":"software/git/#about-git","title":"About git","text":"<p>Git is a version control software package. It can track changes to a file or set of files. To store the files and tracked changes, the files are grouped in a repository. Within a repository, we have the current version of all tracked files, as well as a full version history. With that information, we can use git to view old changes, compare versions, and undo unwanted edits.</p>"},{"location":"software/git/#configure-git","title":"Configure git","text":"<p>Set the following options before you proceed.</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@domain\"\ngit config --global init.defaultBranch main\n</code></pre>"},{"location":"software/git/#create-a-repository","title":"Create a Repository","text":"<p>To get started, we'll initialize an example git repository in a new directory.</p> <pre><code># create a new directory\nmkdir git-example\n\n# move to that directory\ncd git-example\n\n# initialize a repository\ngit init\n</code></pre> <p>Next we create a file to track. Save the <code>hello-world.txt</code> file in the <code>git-example</code> directory.</p> hello-world.txt <pre><code>Hello, World!\n</code></pre> <p>Use the <code>git status</code> command to view the repo status.</p> <pre><code>git status\n</code></pre> <p>The output will show untracked files. Use this same command after each of the following steps to watch the status of your changes through the process.</p> <p>Now we add the file to track changes.</p> <pre><code>git add hello-world.txt\n</code></pre> <p>And commit your changes.</p> <pre><code>git commit -m \"Initial commit\"\n</code></pre> <p>So far we have covered the git version control software. But everything we've done is local to the computer we're working on. In many cases this will be fine. However, it is sometimes necessary or desirable to share our files with the larger community. We can do this using a remote git repository host. There are several options, but here we will discuss GitHub.</p>"},{"location":"software/git/#about-github","title":"About GitHub","text":"<p>GitHub is a web service used to host and manage repositories. It offers public and private repository hosting for free. Another popular and free solution is GitLab.</p> <p> GitHub account</p> <p>You will need a free GitHub account for the tutorial below.</p>"},{"location":"software/git/#host-a-repository-on-github","title":"Host a repository on GitHub","text":"<p>To get started, create a new remote repository. Fill in the repository name, select public or private, and create repository.</p> <p>Now we connect your local repository to the remote GitHub repository and push our changes.</p> <pre><code>git remote add origin https://github.com/your_user_name/git-example.git\ngit branch -M main\ngit push -u origin main\n</code></pre> <p>You can also clone (i.e. download) a remote repository from GitHub to your local computer. This is useful if you created a remote repository first, or when using code from another entity.</p> <p>To clone a repository, locate the address. You can always find this using the green <code>&lt;&gt; Code</code> button on the repository's webpage. Use the HTTPS URL.</p> <pre><code>git clone https://github.com/octocat/Hello-World.git\n</code></pre>"},{"location":"software/git/#security-considerations","title":"Security considerations","text":"<p>You should not store sensitive information in any remote repository, including GitHub, GitLab, etc. In fact, you should avoid sensitive information in your research files at all times. This includes PHI, passwords, etc. Remember, most of the data you upload to GitHub will be publicly accessible. It is very easy to accidentally leave a password or sensitive file in your code, which you then upload.</p> <p>Also consider the code you might clone from other users on GitHub. Use caution to avoid downloading malicious software.</p>"},{"location":"software/git/#additional-info","title":"Additional Info","text":"<p>Git and GitHub are very well-documented and popular tools. There are many great tutorials to make using git easier. We encourage you to seek out more tutorials as needed.</p> <p>There are also many software tools to help work with Git and GitHub. These include GitHub Desktop and VS Code, among many others.</p>"},{"location":"software/jupyter/","title":"Jupyter","text":""},{"location":"software/jupyter/#jupyter","title":"Jupyter","text":""},{"location":"software/jupyter/#connect-to-ondemand","title":"Connect to OnDemand","text":"<p>In our cluster, Jupyter can only be run through OnDemand. To connect to Open OnDemand, please follow our Connecting Guide.</p>"},{"location":"software/jupyter/#open-the-jupyter-notebook","title":"Open the Jupyter Notebook","text":"<p>To open Jupyter Notebook, please follow the instructions in our Open OnDemand Guide.</p>"},{"location":"software/jupyter/#import-libraries","title":"Import libraries","text":"<p>The same way as when you're using Python in the Terminal or on a script, you can import libraries in Jupyter, you will have access to the same libraries that you have access to in the command line. If you need to install a new library, go to the Terminal and login to the cluster. Then, load the python module and install the corresponding library. For example:</p> <pre><code>module load python\npip install matplotlib --user\n</code></pre> <p>You will need to restart your Kernel in Jupyter Notebook to capture the changes. In the top menu, click Kernel and then Restart Kernel.</p>"},{"location":"software/jupyter/#custom-python-in-jupyter","title":"Custom Python in Jupyter","text":"<p>If you have installed a Python or Conda environment in your home directory, you can use it inside Jupyter Notebook. The following examples show creation of a kernel for use in Jupyter.</p>"},{"location":"software/jupyter/#python-virtual-environment","title":"Python Virtual Environment","text":"<p>To activate:</p> <pre><code># Activate your python environment\n$ source myenv/bin/activate\n</code></pre> <p>To add a Jupyter kernel:</p> <pre><code># Add a kernel\n(myenv) $ python -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n/home/user/myenv/bin/python: No module named ipykernel\n# Install ipykernel for your user if it isn't installed\n(myenv) $ pip install ipykernel --user\n# Install a Python (myenv) Kernel in your python environment\n(myenv) $ python -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\nInstalled kernelspec myenv in /home/user/.local/share/jupyter/kernels/myenv\n</code></pre> <p>To deactivate:</p> <pre><code># Deactivate your python environment\ndeactivate\n</code></pre> <p>Now you can open a new or an existing Jupyter Notebook. In the upper menu, click on Kernel, then Change Kernel, and select \"Python (myenv)\". Now Jupyter Notebook will use your selected Python environment. You may need to restart the kernel to use updated packages.</p>"},{"location":"software/jupyter/#conda-environment","title":"Conda Environment","text":"<p>To activate:</p> <pre><code># Activate your conda environment\n$ conda activate mycondaenv\n</code></pre> <p>To add a Jupyter kernel:</p> <pre><code># Make sure you have python in your conda environment\n$ conda list | grep python\n# If you don't have python there, install it\n$ conda install python\n$ conda list | grep python\npython                    3.13.2          hf636f53_101_cp313    conda-forge\npython_abi                3.13                    5_cp313    conda-forge\n# Install ipykernell\n$ conda install ipykernel\n# Install a Python (myenv) Kernel in your conda environment\n$ python -m ipykernel install --user --name mycondaenv --display-name \"Python (mycondaenv)\"\nInstalled kernelspec mycondaenv in /home/user/.local/share/jupyter/kernels/mycondaenv\n</code></pre> <p>To deactivate:</p> <pre><code># Deactivate your conda environment\n$ conda deactivate\n</code></pre> <p>Now you can open a new or an existing Jupyter Notebook. In the upper menu, click on Kernel, then Change Kernel, and select \"Python (mycondaenv)\". Now Jupyter Notebook will use your selected Python environment. You may need to restart the kernel to use updated packages.</p>"},{"location":"software/jupyter/#using-jupyter-notebook","title":"Using Jupyter Notebook","text":"<p>Once you are in Jupyter Notebook, you can type any Python commands into the cells and execute them with the play button or in the upper menu, selecting Run, and then Run Selected Cell, or using the keys shift+return while located over the cell that you want to execute. You can follow this complete Python guide to learn how to program in Python.</p> <p>One of the advantages of using Jupyter Notebook is to be able to visualize plots right next to the code. The Matplotlib User Guide has a lot of Useful Examples for doing just that.</p> <p>Please visit The Jupyter Notebook documentation for more details about Jupyter Notebook and how to use it.</p>"},{"location":"software/jupyter/#jupyter-notebook-shortcuts","title":"Jupyter Notebook Shortcuts","text":"<p>To enter Command Mode in the current cell, use the Esc key in your keyboard. While in Command Mode you can use the following keys:</p> Key Action a Inserts a new cell above the current one. b Inserts a new cell below the current one. c Copies the current cell. d + d Deletes the current cell. Shift + l Show/hide line numbers. m Converts the current cell to a markdown cell. Shift + m Merge selected cells. s Save the notebook. v Paste a copied or cut cell(s). x Cuts the current cell. y Converts the current cell to a code cell. z Undoes the deletion of a cell. up/down arrows Navigate through the cells. Shift + up/down arrows Select multiple cells. <p>To enter Edit Mode in the current cell, press Enter. While in Edit Mode you can use the following keys:</p> Key Action Alt + Enter Runs the current cell and inserts one below. Ctrl + Enter Runs the current cell and keeps it selected. Ctrl + Shift + up/down arrows Move current cell up/down. Shift + Control + c Open search menu Shift + Control + f Open/close file menu Shift + Enter Runs the current cell and moves to the next one."},{"location":"software/jupyter/#ipython-magic-commands","title":"IPython Magic Commands","text":"<p>IPython magic commands provide shortcuts for common tasks. To see the list of all magic commands, type <code>%lsmagic</code> in a cell and then run the cell or visit the IPython Documentation. Here are a few examples of useful things that can be done using the IPython Magic commands:</p> Command Use <code>%%bash</code> Run the content of the current cell as Bash. <code>%conda</code> Run the conda package manager. <code>%env</code> List all current environment variables. <code>%env VARIABLE_NAME=value</code> Set a new environment variable. <code>%%html</code> Render the current cell as html code. <code>%%javascript</code> Run the current cell as Javascript code. <code>%load path/to/script.py</code> Load the contents of a python script into the current cell. <code>print(os.environ['VARIABLE_NAME'])</code> Print the value of an environment variable. <code>%%markdown</code> Render the current cell as Markdown. <code>%run path/to/script.py</code> Execute a python script. <code>%store var</code> Save the value of a variable to use in a different cell or Jupyter Notebook. <code>%store -r var</code> Re-use a stored variable from a different Jupyter notebook. <code>del var</code> Remove a stored variable. <code>%%time</code> Print the execution time of a cell. <code>%time</code> Print the execution time of a line. <code>%who</code> See list of environment variables."},{"location":"software/jupyter/#shell-commands","title":"Shell commands","text":"<p>As we saw in the table above, you can execute a cell as Bash instead of Python using the <code>%%bash</code> magic command. But you can also execute a single line as bash by preceding the line with <code>!</code>.</p> <p>If the current Kernel is from a Conda environment, you might get <code>command not found</code> errors when trying to run binaries included in your conda environment. This is because the command is not available in the default shell environment used by Jupyter Notebook. Jupyter Notebook will not automatically activate your Conda environment when using shell commands with <code>!</code>. In order to do that you would have to add the following code to a cell and then execute it:</p> <pre><code>import os\nenv_path = \"/hpc/apps/miniforge/envs/alevinfry-0.11.1/bin\"\nos.environ['PATH'] = env_path + os.pathsep + os.environ['PATH']\n</code></pre> <p>After, you will be able to run the usual commands that you would run in the command line after activating the environment. For example, <code>!alevin-fry --version</code>.</p> <p>You can use the output of a shell command and save it in a Python variable:</p> <pre><code>myfiles = !ls\nprint(myfiles[2])\n</code></pre>"},{"location":"software/jupyter/#markdown-cells","title":"Markdown cells","text":"<p>You can run cells as Markdown using the magic command shown in the table above <code>%%markdown</code> or selecting Markdown as the language of the current cell. The Markdown Guide is a great place to start learning Markdown.</p>"},{"location":"software/jupyter/#latex-formulas","title":"Latex formulas","text":"<p>You can use Latex to display formulas within Markdown cells. To display Latex code within the current line, surround your expression with a dollar sign on each side. For example: <code>$x2+y2=8$</code>. To display the formula in a separate line, wrap it with two dollar signs before and after the Latex code.</p> <p>For example, the following Markdown cell:</p> <p></p> <p>Will generate the following output when run:</p> <p></p>"},{"location":"software/jupyter/#display-media","title":"Display media","text":"<p>You can display different type of media in your Jupyter Notebook, including images, audio, video or PDF files.</p> <p>The following example displays an image:</p> <pre><code>import os\nfrom IPython.display import display, Image\ndisplay(Image('myfile.png'))\n</code></pre> <p>The following example displays a pdf:</p> <pre><code>from IPython.display import IFrame\nIFrame(\"mypdf.pdf\", width=800, height=600)\n</code></pre> <p>You can display videos in a html cell:</p> <pre><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2TT1EKPV_hc\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"software/lsdyna/","title":"LS-DYNA","text":""},{"location":"software/lsdyna/#ls-dyna","title":"LS-DYNA","text":"<p>LS-DYNA is a general purpose finite element analysis software.</p> <p>Licensed Software</p> <p>LS-DYNA is restricted to licensed users. Please contact help-rcc@mcw.edu with questions.</p>"},{"location":"software/lsdyna/#running-ls-dyna","title":"Running LS-DYNA","text":"<p>LS-DYNA is installed on the HPC Cluster. RCC has provided the <code>sbatch-dyna</code> script which will build your submission file and submit it to the cluster.</p> <p>sbatch-dyna required</p> <p>You must use this script to submit LS-DYNA jobs on the cluster. Failure to do so will break the LS-DYNA licensing mechanism and result in failed jobs for users.</p> <pre><code>$ sbatch-dyna -h\nUsage: sbatch-dyna [-i:-j:-m:-n:-c:-w:-q:-l:-p:-s:-y]\n-i InputFile  (Enter input file, i.e. TestRun.k)\n-j JobName    (Enter job name, i.e. TestRun)\n-m Email      (Enter E-Mail for notification, i.e. user@mcw.edu)\n-n Nodes      (Number of nodes, default=1)\n-c Cores      (Number of cores, default=24)\n-w Walltime   (Walltime, format HH:MM:SS, default=48:00:00)\n-q Queue      (Queue, default=)\n-l Module     (Choose module, 0 for lsdyna/9.0.1, 1 for lsdyna/9.3.1, 2 for lsdyna/10.2.0, 3 for lsdyna/11.2.0, 4 for lsdyna/12.0.0, 5 for lsdyna/8.0.0, 6 for lsdyna/8.1.0)\n-p Precision  (Choose precision, i.e. [s]ingle or [d]ouble)\n-s Submit     (Enter 'false' to create pbs script. Default true)\n-y DynaArgs   (LS-Dyna args, i.e. -y memory1=200m memory2=100m, etc...)\n-h, --help    show this help message and exit\n</code></pre> <p>Please note that <code>InputFile</code>, <code>JobName</code>, and <code>DynaArgs</code> are required by the script.</p>"},{"location":"software/lsdyna/#memory-allocation","title":"Memory Allocation","text":"<p>It is very important to understand memory usage within LS-DYNA and how that corresponds to memory on a cluster node.</p> <p>When submitting a job, there are 3 memory values to keep in mind:</p> <ul> <li>SLURM job memory limit The standard compute node max memory is 360GB. Do not exceed the memory limit. If you need additional memory, use the large memory nodes.</li> <li>LS-DYNA memory1 This is the amount of memory dedicated to the master process to decompose the model. This value is specified in mega-words (m). A mega-word is 4 bytes in single precision, or 8 bytes in double precision. Memory1 is dependent on your model size.</li> <li>LS-DYNA memory2 This is the amount of memory used by each CPU core to solve the decomposed problem. Memory2 is dependent on the number of cores requested. More cores means less memory required per core.</li> </ul>"},{"location":"software/lsdyna/#cpu-allocation","title":"CPU Allocation","text":"<p>Every cluster node has 48 cores. Please remember that more cores does not always mean faster.</p>"},{"location":"software/lsdyna/#example","title":"Example","text":"<p>Simulation using 1 node, 48 cores, and 360GB.</p> <pre><code>sbatch-dyna -i testing.k -j testrun -m netid@mcw.edu -n 1 -c 48 -l 2 -p d -y \"memory1=16000m memory2=333m\"\n</code></pre> <ul> <li><code>memory1</code> We request 16000m (16,000 mega-word) for model decomposition, or 128GB in double precision.</li> </ul> \\[ 16000 \\text{mega-word} \\times \\frac{1e^6 \\text{word}}{1 \\text{mega-word}} \\times \\frac{8 \\text{byte}}{1 \\text{word}} \\times \\frac{1\\text{gigabyte}}{1e^9 \\text{byte}} = 128\\text{GB} \\] <ul> <li><code>memory2</code> We request 333m (333 mega-words) per CPU core, or ~2.67GB per CPU core. Total memory allocated to all CPU cores is ~128GB. The remaining 104GB is available for dynamic memory allocation.</li> </ul> \\[ 333 \\text{mega-word} \\times \\frac{1e^6 \\text{word}}{1 \\text{mega-word}} \\times \\frac{8 \\text{byte}}{1 \\text{word}} \\times \\frac{1\\text{gigabyte}}{1e^9 \\text{byte}} = 2.67\\text{GB} \\] <p>Additional memory guidance</p> <p>The example above is just an example. Please adjust for the memory needs of your simulation. You may need more or less memory depending on size of model. You may also need to substitute <code>4 bytes</code> into the calculations if you use single precision. The process of determining memory utilization should be iterative. For more advice on LS-DYNA memory, see please see https://www.d3view.com/a-few-words-on-memory-settings-in-ls-dyna/.</p>"},{"location":"software/lsdyna/#getting-help","title":"Getting Help","text":"<p>If you have questions about the <code>sbatch-dyna</code> command or the HPC cluster, please email help-rcc@mcw.edu. Please email the software vendor for LS-DYNA questions.</p>"},{"location":"software/matlab/","title":"MATLAB Parallel Server","text":""},{"location":"software/matlab/#matlab-parallel-server","title":"MATLAB Parallel Server","text":"<p>The MATLAB Parallel Server software is an extension of the Parallel Computing Toolbox. The software runs on the HPC Cluster and can be used to run MATLAB jobs that are too large to run on your personal desktop/laptop computer. RCC has a license for 256 workers.</p>"},{"location":"software/matlab/#requirements","title":"Requirements","text":"<ul> <li>RCC user account</li> <li>MATLAB R2024b (purchased from MCW)</li> </ul>"},{"location":"software/matlab/#setup","title":"Setup","text":"<p>Version requirement</p> <p>MATLAB requires that the Parallel Server version match the client version. Please make sure your client MATLAB version is R2024b.</p>"},{"location":"software/matlab/#download-plugin-files","title":"Download Plugin Files","text":"<p>Download the scheduler plugin files that help connect your MATLAB client to the HPC cluster. Unzip and save the folder <code>matlab-parallel-slurm-plugin-R2024b</code> to a location of your choice. Please note, this folder should be saved in a location that will not be moved or erased.</p>"},{"location":"software/matlab/#add-startup-script","title":"Add Startup Script","text":"<p>Save the following script as <code>startup.m</code> in a location on the MATLAB path that will not be deleted.</p> startup.m <pre><code>% startup.m\n% Startup script identifies available interfaces, \n% and selects correct interface.\n\ne = java.net.NetworkInterface.getNetworkInterfaces();\nwhile(e.hasMoreElements())\n    ee = e.nextElement().getInetAddresses();\n    while (ee.hasMoreElements())\n        i = ee.nextElement().getHostAddress().toString();\n        if contains(i,java.lang.String('141.106'))\n            pctconfig('hostname',char(i));\n        end\n    end\nend\n</code></pre>"},{"location":"software/matlab/#add-cluster-profile","title":"Add Cluster Profile","text":"<ol> <li>Launch the MATLAB application and select Home &gt; Parallel &gt; Create and Manage Clusters to open the Cluster Profile Manager window. Select Import, browse to the location of the <code>matlab-parallel-slurm-plugin-R2024b</code> folder, and select the <code>HPC_Cluster.mlsettings</code> file.</li> <li>Locate HPC Cluster profile in the Cluster Profile Manager and select Edit.<ul> <li>Locate the Scheduler Plugin section of the profile. Set the PluginScriptsLocation property to location of the MATLAB_R2024b_Client2Cluster folder.</li> <li>Locate the Additional Properties table. Set the RemoteJobStorageLocation property to <code>/scratch/g/PI_NetID</code>, where <code>PI_NetID</code> is your PI's username. Set the Username property to your MCW username.  </li> </ul> </li> <li>Select Done editing and set the new profile as default.</li> </ol>"},{"location":"software/matlab/#validation","title":"Validation","text":"<p>Select the Validation tab. Change the Number of workers to use to 1. Select Validate and enter your MCW password when prompted. All tests should pass.</p>"},{"location":"software/matlab/#upgrading","title":"Upgrading","text":"<p>RCC will periodically update the MATLAB Parallel Server software to the next B version. After you upgrade your client to match, follow the steps above to configure the new version.</p>"},{"location":"software/matlab/#using-the-cluster","title":"Using the Cluster","text":"<p>There are several ways to interact with the cluster using the Parallel Computing Toolbox. The parpool and batch commands can be used to create jobs to run your code on the cluster. Examples are provided below. More information is available on the MathWorks website for Batch Processing and Parpool.</p>"},{"location":"software/matlab/#batch","title":"Batch","text":"<p>The batch command also creates a pool of workers in a job on the cluster. It creates a remote pool of workers on the cluster that can run your script when workers are available. In comparison to the parpool option option, the batch command does not require that you wait for a pool of workers. Instead, your script is submitted to the cluster to be run in a batch pool when workers are available.</p> <p>The batch command can be submitted as a pool job, where N is the number of workers.</p> <pre><code>&gt;&gt; job = batch('mytest','Pool',N)\n</code></pre> <p>Your batch jobs are submitted to the HPC cluster with default max walltime of 8 hours. There are times (near maintenance windows) that this may not work for every job. You can add a unique time limit to your job by using AdditionalSubmitArgs.</p> <pre><code>&gt;&gt; % Select cluster\n&gt;&gt; c = parcluster(\"HPC Cluster\");\n&gt;&gt; % Set time\n&gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs = '--time=DD-HH:MM:SS';\n&gt;&gt; % Start batch job\n&gt;&gt; job = batch(c,\"mytest\");\n</code></pre> <p>A specific time limit can be added to any job. Max time is 7 days. For example, a job with a 10 hour time limit would set c.AdditionalProperties.AdditionalSubmitArgs = '--time=10:00:00';.</p> <p>Additional Information</p> <p>Please review the MathWorks tutorial explaining batch parallel jobs.</p>"},{"location":"software/matlab/#parpool","title":"Parpool","text":"<p>The parpool command creates a pool of workers in a job on the cluster. It creates an interactive session using remote cluster nodes to run the pool of workers. The parpool command does require that enough workers are available on the cluster before the pool will start. If you do not need to run commands interactively, and/or have your code in a script, please try the batch example.</p> <p>The parpool command can be submitted with variable pool size, where N is the number of workers. Please try to use parpool sizes that are multiples of 12.</p> <pre><code>&gt;&gt; parpool(N)\n</code></pre> <p>Each parpool job is submitted to the HPC cluster with default max walltime of 7 days There are times (near maintenance windows) that this may not work for every job. You can add a unique time limit to your job by using AdditionalSubmitArgs.</p> <pre><code>&gt;&gt; % Select cluster\n&gt;&gt; c = parcluster(\"HPC Cluster\");\n&gt;&gt; % Set time\n&gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs = '--time=DD-HH:MM:SS';\n&gt;&gt; % Open a pool of 12 workers on the cluster\n&gt;&gt; p = c.parpool(12);\n</code></pre> <p>A specific time limit can be added to any job. Max time is 7 days For example, a job with a 10 hour time limit would set <code>c.AdditionalProperties.AdditionalSubmitArgs = '--time=10:00:00';</code>.</p> <p>To start parpool on HPC Cluster with N workers:</p> <pre><code>&gt;&gt; parpool('HPC Cluster',N);\n</code></pre> <p>Start a parpool object on a cluster with N workers and attach a file <code>myfile.m</code>:</p> <pre><code>&gt;&gt; poolobj = parpool('HPC Cluster',N);\n&gt;&gt; addAttachedFiles(poolobj,{'mytest.m'});\n</code></pre> <p>Now that the parpool is started, you can run your code. The code can be run interactively or using a script.</p> <p>To run code interactively:</p> <pre><code>&gt;&gt; parfor i = 1:1024\n&gt;&gt;   A(i) = sin(i*2*pi/1024);\n&gt;&gt; end\n</code></pre> <p>Plot output:</p> <pre><code>&gt;&gt; plot(A);\n</code></pre> <p>To run code with a script, create a new script:</p> mytest.m <pre><code>parfor i = 1:1024\n  A(i) = sin(i*2*pi/1024);\nend\n</code></pre> <p>Execute script:</p> <pre><code>&gt;&gt; mytest\n</code></pre> <p>Plot output:</p> <pre><code>&gt;&gt; plot(A)\n</code></pre> <p>Please make sure to shutdown your parpool when you're done:</p> <pre><code>&gt;&gt; delete(gcp(poolobj))\n</code></pre>"},{"location":"software/matlab/#file-transfer-and-management","title":"File Transfer and Management","text":"<p>The Matlab Parallel Computing Toolbox has multiple ways to handle file transfer and access. For small files, the files can be auto-attached and transfer to the remote cluster at job submission. However, if your workflow requires large files, the transfer at job time becomes inconvenient. In this case, it is better to transfer files to the cluster before job submission, see File Transfer. For more information about using data in Batch jobs, please see Share Code with the Workers.</p>"},{"location":"software/matlab/#cluster-usage-policy","title":"Cluster Usage Policy","text":"<p>The HPC Cluster is a shared resource. Please only use whatever resources are needed for your computation. When you're done, please make sure to stop your processes and close any batch or parpool jobs. This is especially important to ensure that fair access is maintained.</p>"},{"location":"software/matlab/#getting-help","title":"Getting Help","text":"<p>Review Mathwork's Batch Processing and Parpool documentation.</p> <p>If you have questions/concerns please contact help-rcc@mcw.edu</p>"},{"location":"software/module-request/","title":"Requesting Software","text":""},{"location":"software/module-request/#requesting-software","title":"Requesting Software","text":"<p>All RCC users are eligible to request software installation. Read on to find out how we evaluate requests, and how to submit a request.</p>"},{"location":"software/module-request/#submitting-a-request","title":"Submitting a Request","text":"<p>To submit a software installation request, please email help-rcc@mcw.edu with the following information:</p> <ol> <li>Name of the software</li> <li>Link to the software files</li> <li>Link to the software documentation</li> <li>Brief description of need</li> </ol> <p>RCC will evaluate the software based on your request. This process is much faster if the above information is included in the original request.</p>"},{"location":"software/module-request/#installation","title":"Installation","text":"<p>RCC will evaluate your request to determine how to install the software. If approved, we will install the software in the <code>/hpc/apps</code> directory. Not all software requests will be centrally installed. For example, many R and Python packages can be installed without admin privileges. In that case, RCC admins would provide the proper installation command.</p>"},{"location":"software/module-request/#paid-license-software","title":"Paid License Software","text":"<p>Some software packages require a paid license. Paid license software packages have additional requirements that RCC must evaluate. Often this includes what type of license (not all paid license types are supported), and other contractual terms that MCW must abide.</p> <p>In order to avoid issues, please contact help-rcc@mcw.edu to setup a time to discuss your proposed installation. Please note that the PI funding the software purchase must attend this meeting.</p> <p>Do not buy software for the cluster without talking to RCC first.</p> <p>RCC does not guarantee that your software can or will be installed on the cluster. It is up to you to contact RCC prior to purchasing the software. Failure to do so may result in delays and/or denial of installation.</p>"},{"location":"software/modules/","title":"Using Modules","text":""},{"location":"software/modules/#software-modules","title":"Software Modules","text":"<p>We use Lmod to manage installed software. Each software package installed on the cluster has a corresponding modulefile, which contains information about an application's version, executable, libraries, and documentation. Using the module commands, users can add, remove, or switch versions of an application.</p>"},{"location":"software/modules/#module-help","title":"Module Help","text":"<p>Print help information for module command:</p> <pre><code>module help\n</code></pre> <p>Print help information for a module:</p> <pre><code>module help [modulefile]\n</code></pre>"},{"location":"software/modules/#show-loaded-modules","title":"Show Loaded Modules","text":"<p>Show currently loaded modules:</p> <pre><code>module list\n</code></pre>"},{"location":"software/modules/#show-available-modules","title":"Show Available Modules","text":"<p>Show currently available modules:</p> <pre><code>module avail\n</code></pre> <p>This will output a list of modules that represent installed software.</p>"},{"location":"software/modules/#show-module-information","title":"Show Module Information","text":"<p>Print information about a module:</p> <pre><code>module show [modulefile]\n</code></pre> <p>Print module contents:</p> <pre><code>module display [modulefile]\n</code></pre>"},{"location":"software/modules/#load-module","title":"Load Module","text":"<p>Add a module to the user environment:</p> <pre><code>module load [modulefile]\n</code></pre> <p>Example:</p> <pre><code> module load gcc\n</code></pre> <p>Adds the GCC compiler to the user environment.</p>"},{"location":"software/modules/#unload-module","title":"Unload Module","text":"<p>Remove a module from the user environment:</p> <pre><code>module unload [modulefile]\n</code></pre> <p>Example:</p> <pre><code>module unload gcc\n</code></pre> <p>Removes the GCC compiler from the user environment.</p>"},{"location":"software/modules/#swap-modules","title":"Swap Modules","text":"<p>Swap a loaded module for another module:</p> <pre><code>module swap [modulefile1] [modulefile2]\n</code></pre>"},{"location":"software/namd/","title":"NAMD","text":""},{"location":"software/namd/#namd","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code designed for simulation of large biomolecular systems.</p>"},{"location":"software/namd/#namd2-or-namd3","title":"NAMD2 or NAMD3","text":"<p>NAMD3 was recently released and is installed on the cluster. The command options and syntax may be different from NAMD2 and it is up to the user to decide the appropriate version.</p> <p>To list available NAMD versions:</p> NAMD2NAMD3 <pre><code>module avail namd2\n\n------------------------------------------------------------ /hpc/modulefiles -------------------------------------------------------------\nnamd2/2.14-cuda-mpi    namd2/2.14-mpi    namd2/2.14 (D)\n</code></pre> <pre><code>module avail namd3\n\n------------------------------------------------------------ /hpc/modulefiles -------------------------------------------------------------\nnamd3/3.0.1-cuda    namd3/3.0.1 (D)\n</code></pre> <p>Several versions are installed including parallel (mpi) and GPU (cuda) enabled options. The default module marked with <code>(D)</code> should be used in most cases for single-node simulations up to 48 cores.</p> <p>To load NAMD in a job script:</p> NAMD2NAMD3 <pre><code>module load namd2/2.14\n</code></pre> <pre><code>module load namd3/3.0.1\n</code></pre> <p>Remember to adjust the version of NAMD required for your type of simulation.</p>"},{"location":"software/namd/#batch-job","title":"Batch job","text":"<p>To run NAMD in a batch job, first create a job script.</p> namd2-job.slurmnamd3-job.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=single-node\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --time=72:00:00\n\nmodule load namd2/2.14\n\n## Single node.\n## This will start a simulation on 8 CPU cores with 72 hour time \n## limit. If you want to increase the number of CPU cores, adjust \n## the cpus-per-task parameter above. You need to update the namd \n## config and log output file names to suit your simulation.\n\nnamd2 +p$SLURM_CPUS_PER_TASK +idlepoll namd_config_file.conf &gt; namd_run_1.log\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=single-node\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --time=72:00:00\n\nmodule load namd3/3.0.1\n\n## Single node.\n## This will start a simulation on 8 CPU cores with 72 hour time \n## limit. If you want to increase the number of CPU cores, adjust \n## the cpus-per-task parameter above. You need to update the namd \n## config and log output file names to suit your simulation.\n\nnamd3 +p$SLURM_CPUS_PER_TASK +idlepoll namd_config_file.conf &gt; namd_run_1.log\n</code></pre>"},{"location":"software/nextflow/","title":"Nextflow","text":""},{"location":"software/nextflow/#nextflow","title":"Nextflow","text":"<p>Nextflow is a workflow software used to manage data and compute-intensive pipelines. It is commonly used in genomic workflows but is generally applicable. See nextflow for more information and tutorials.</p>"},{"location":"software/nextflow/#finding-pipelines","title":"Finding pipelines","text":"<p>Many ready-to-use pipelines are available from nf-core, which is a collection of community-built pipelines using Nextflow.</p>"},{"location":"software/nextflow/#anatomy-of-a-pipeline","title":"Anatomy of a pipeline","text":"<p>A Nextflow pipeline consists of an ordered list of tasks with resource requirements and software dependencies. Each task or step is run in a specific order to produce an end result, and a master process controls when and where these tasks are executed. When a Nextflow pipeline begins, the master process will download software dependencies using a software management system such as Docker, Singularity, Conda, etc. This provides a portable and reproducible software environment for each task.</p> <p>Task execution can be delegated to a workload manager like SLURM, which is used on the HPC cluster. With the SLURM executor, the master process has a dedicated SLURM job, and each task is run as a separate SLURM job. Tasks can also be run with a local executor, i.e. no outside workload manager, where pipeline tasks run in the same SLURM job as the master process. The choice of executor determines what resources are available to the master process and pipeline tasks. The local executor is fine for simple pipelines and testing. For more advanced pipelines including NGS workflows, the SLURM executor will generally be more efficient.</p>"},{"location":"software/nextflow/#running-a-pipeline","title":"Running a pipeline","text":"<p>We can use an interactive or batch job to start the pipeline master process. This guarantees that the master process has dedicated resources. Do not start a nextflow pipeline on a login node, even if you're using the SLURM executor. Admins will delete your pipeline process if it is running on a login node.</p> <p>Understand your pipeline!</p> <p>Take time to read the Nextflow and pipeline-specific documentation. A misconfigured pipeline could adversely affect the cluster scheduler and yield poor performance results for you. If you have any questions, please contact help-rcc@mcw.edu</p>"},{"location":"software/nextflow/#interactive-job","title":"Interactive job","text":"<p>We'll start with a simple <code>Hello world!</code> pipeline. To get started, launch an interactive job.</p> <pre><code>srun --job-name=nextflow --time=1:00:00 --ntasks=1 --pty bash\n</code></pre> <p>Next we load the nextflow module and start the pipeline with the default local executor.</p> <pre><code>module load nextflow\nnextflow run hello\n</code></pre> <p>The output below shows <code>Hello world!</code> printed in four languages. Notice that the executor was set to local by default, which launches the pipeline tasks in the same job as the master process. The local executor works for many workflows, but may not be appropriate in all cases. For resource-intensive workflows, such as NGS pipelines, use the SLURM executor.</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nNOTE: Your local project version looks outdated - a different revision is available in the remote repository [1d71f857bb]\nLaunching `https://github.com/nextflow-io/hello` [condescending_descartes] DSL2 - revision: 4eab81bd42 [master]\nexecutor &gt;  local (4)\n[d6/af64fc] process &gt; sayHello (4) [100%] 4 of 4 \u2714\nBonjour world!\n\nCiao world!\n\nHello world!\n\nHola world!\n</code></pre> <p>To run the same <code>Hello world!</code> workflow using the SLURM executor, add a <code>nextflow.config</code> file for the pipeline in your working directory.</p> nextflow.config <pre><code>process {\n    executor = 'slurm'\n    // time limit needed for this example\n    // most pipelines from nf-core have this built-in\n    time = '1:00:00'\n}\n</code></pre> <p>To use your new config file, add the <code>-c</code> flag.</p> <pre><code>nextflow run -c nextflow.config hello\n</code></pre> <p>Output will show the executor is SLURM. If you watch the SLURM queue with command <code>squeue</code>, you will see 4 additional jobs launch.</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nNOTE: Your local project version looks outdated - a different revision is available in the remote repository [1d71f857bb]\nLaunching `https://github.com/nextflow-io/hello` [nasty_lagrange] DSL2 - revision: 4eab81bd42 [master]\nexecutor &gt;  slurm (4)\n[92/338040] process &gt; sayHello (4) [100%] 4 of 4 \u2714\nCiao world!\n\nBonjour world!\n\nHello world!\n\nHola world!\n</code></pre>"},{"location":"software/nextflow/#batch-job","title":"Batch job","text":"<p>To run the same workflow with local executor in a batch job, use the <code>nf-hello-local-exec.slurm</code> job script.</p> nf-local-exec.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=nextflow\n#SBATCH --ntasks=1\n#SBATCH --time=1:00:00\n#SBATCH --output=%x-%j.out\n\nmodule load nextflow\nnextflow run hello\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch nf-local-exec.slurm\n</code></pre> <p>To use the SLURM executor with a batch job, use the <code>nf-slurm-exec.slurm</code> job script.</p> nf-slurm-exec.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=nextflow\n#SBATCH --ntasks=1\n#SBATCH --time=1:00:00\n#SBATCH --output=%x-%j.out\n\nmodule load nextflow\nnextflow run -c nextflow.config hello\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch nf-slurm-exec.slurm\n</code></pre>"},{"location":"software/nextflow/#examples","title":"Examples","text":"<p>Here we provide suggested configurations for various pipelines.</p> <p>Configuration issue</p> <p>The nf-core pipelines have a known bug with configuration files. If you attempt to combine your params and nextflow.config files, the pipeline will fail. To avoid this, put your params in <code>nf-params.json</code>, and general nextflow configuration in a <code>nextflow.config</code> file. You will pass the params file to nextflow on the command-line. The <code>nextflow.config</code> file is ready automatically when found in your working directory.</p>"},{"location":"software/nextflow/#nf-corernaseq","title":"nf-core/rnaseq","text":"<p>These configurations are the minimum required. You will need to customize and add additional parameters to the <code>nf-params.json</code> file as needed for your specific analysis. See https://nf-co.re/rnaseq/3.12.0/parameters for details.</p> nf-params.jsonnextflow.confignf-rnaseq.slurm <pre><code>{\n    \"input\": \"samplesheet.csv\",\n    \"outdir\": \".\\/exampleOut\"\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n}\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=nf-rnaseq\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --time=12:00:00\n#SBATCH --output=%x-%j.out\n\nmodule load nextflow\nmodule load singularity\n\nnextflow run nf-core/rnaseq -name batch1 -profile singularity -params-file nf-params.json\n</code></pre>"},{"location":"software/python/","title":"Python","text":""},{"location":"software/python/#python","title":"Python","text":"<p>Python is an object-oriented programming language that is easy to write and read. It is widely used in many areas of computational science and has many useful packages built-in or available for installation.</p> <p>Python2 Deprecated</p> <p>Python2 is deprecated. The developers are no longer maintaining the source code. It is recommended that all users migrate their scripts from Python2 to Python3. At the least, users should not write new scripts in for Python2.</p>"},{"location":"software/python/#using-installed-python","title":"Using Installed Python","text":"<p>The HPC cluster includes Python3.9 and Intel Python3. Use <code>module avail python</code> to see available versions. Python2.7 is deprecated but installed for backwards compatibility. You should not use it for any new scripting.</p> <p>To use Python3:</p> <pre><code>module load python/3.9.1\n</code></pre> <p>To use Intel Python3:</p> <pre><code>module load python/intel-2021.1\n</code></pre>"},{"location":"software/python/#list-installed-packages","title":"List Installed Packages","text":"<p>Many common scientific Python packages (i.e., NumPy, SciPy, Pandas) are already installed.</p> <p>To view all installed packages:</p> <pre><code>pip freeze\n</code></pre> <p>To search for a specific package (e.g., SciPy):</p> <pre><code>pip freeze | grep 'scipy'\n</code></pre>"},{"location":"software/python/#installing-packages","title":"Installing Packages","text":"<p>We recommend to install python packages in virtual environments or conda environments. Try to avoid installing with system Python, which will place the package files in your home directory. While this does work, it can be difficult to know what has been installed, and even more difficult to update or remove. Also, using conda will allow you to create different environments for specific software and share them with other people. For example, one program that you might need can have certain dependencies that conflict with the dependencies of another program. You can solve this by using different conda environments. For more information please visit our Conda Guide.</p>"},{"location":"software/python/#python-virtual-environment","title":"Python Virtual Environment","text":"<p>A Python virtual environment is an independent set of Python packages contained in a directory. To create an environment, we use the <code>venv</code> Python module. Within an environment, we use standard package installation methods. With Python virtual environments, everything is self-contained, reproducible, and easy to manage. For more information on Python virtual environments, see the venv guide.</p> <p>To get started, load Python:</p> <pre><code>module load python/3.9.1 \n</code></pre> <p>To create a new virtual environment:</p> <pre><code>python -m venv pythonEnv\n</code></pre> <p>To use your virtual environment:</p> <pre><code>source pythonEnv/bin/activate\n</code></pre> <p>To exit your Python virtual environment:</p> <pre><code>deactivate \n</code></pre> <p>To install a package using <code>pip</code>:</p> <pre><code>pip install numpy\n</code></pre> <p>The <code>pip</code> command will install packages to the site-packages directory in your virtual environment. Installed packages can be listed with the <code>pip freeze</code> command.</p> <p>Packages from the system-wide Python installations can also be included in your Python virtual environment. This must be done when creating the virtual environment.</p> <p>To create and use a Python virtual environment with system-wide packages:</p> <pre><code>python -m venv --system-site-packages pythonEnvSysPkgs\nsource pythonEnvSysPkgs/bin/activate\n</code></pre> <p>The Python virtual environment (e.g., pythonEnvSysPkgs) now includes all packages from the system-wide <code>python/3.9.1</code>. The <code>pip freeze</code> command shows the list of packages. You could now install new packages as shown above, creating a virtual extension to the system-wide Python installations.</p>"},{"location":"software/python/#python-in-a-job-script","title":"Python in a Job Script","text":"<p>You may want to use your personal Python environment in a SLURM job on the cluster. Please review the Writing a Job Script guide before proceeding. The following examples show various methods to use your personal Python environment.</p> py-venv.slurmconda-venv.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=Python\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:01:00\n#SBATCH --output=%x-%j.out\n\nmodule load python/3.9.1\nsource pythonEnv/bin/activate\npython script.py\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=Miniconda\n#SBATCH --ntasks=1\n#SBATCH --mem-per-cpu=1gb\n#SBATCH --time=00:01:00\n#SBATCH --output=%x-%j.out\n\nmodule load miniconda3\nconda activate myenv\npython myPythonScript.py\n</code></pre>"},{"location":"software/pytorch/","title":"PyTorch","text":""},{"location":"software/pytorch/#pytorch","title":"PyTorch","text":"<p>PyTorch can be run in batch, interactive, or Jupyter Notebook. For more information, check the module help information with <code>module help pytorch</code>.</p>"},{"location":"software/pytorch/#pytorch-job","title":"PyTorch job","text":"<p>The following example will use PyTorch to train a network on the MNIST data set.</p> <p>First, download the PyTorch examples:</p> <pre><code>git clone https://github.com/pytorch/examples.git\n</code></pre> <p>Now that you have the examples, use the following job script to train the network.</p> pytorch-mnist.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=pytorch\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n#SBATCH --account={PI_NetID}\n#SBATCH --partition=gpu\n#SBATCH --gres=gpu:1\n#SBATCH --output=%x-%j.out\n\nmodule load pytorch\npython examples/mnist/main.py &gt;&gt; output.log  \n</code></pre> <p>Submit the job:</p> <pre><code>cd /scratch/g/pi_netid/pytorch-test &amp;&amp; sbatch pytorch.sh\n</code></pre>"},{"location":"software/pytorch/#pytorch-jupyter-notebook","title":"PyTorch Jupyter Notebook","text":"<p>This functionality is now provided by Open OnDemand! Use the Jupyter Notebook app to start a PyTorch enabled kernel.</p>"},{"location":"software/r-pkg-list/","title":"R Packages","text":""},{"location":"software/r-pkg-list/#installed-r-packages","title":"Installed R packages","text":"<p>The following 1080 packages are installed with R 4.5.0.</p> <p>Last updated: October 28, 2025</p> Package Version Description abc 2.2.2 Tools for Approximate Bayesian Computation (ABC) abc.data 1.1 Data Only: Tools for Approximate Bayesian Computation (ABC) abind 1.4-8 Combine Multidimensional Arrays ade4 1.7-23 Analysis of Ecological Data: Exploratory and Euclidean Methods in Environmental Sciences admisc 0.38 Adrian Dusa's Miscellaneous afex 1.4-1 Analysis of Factorial Experiments affxparser 1.80.0 Affymetrix File Parsing SDK affy 1.86.0 Methods for Affymetrix Oligonucleotide Arrays affyio 1.78.0 Tools for parsing Affymetrix data files airr 1.5.0 AIRR Data Representation Reference Library alabaster.base 1.8.0 Save Bioconductor Objects to File alabaster.matrix 1.8.0 Load and Save Artifacts from File alabaster.ranges 1.8.0 Load and Save Ranges-related Artifacts from File alabaster.schemas 1.8.0 Schemas for the Alabaster Framework alabaster.se 1.8.0 Load and Save SummarizedExperiments from File alakazam 1.3.0 Immunoglobulin Clonal Lineage and Diversity Analysis amap 0.8-20 Another Multidimensional Analysis Package anndata 0.7.5.6 'anndata' for R annotate 1.86.0 Annotation for microarrays AnnotationDbi 1.70.0 Manipulation of SQLite-based annotations in Bioconductor AnnotationFilter 1.32.0 Facilities for Filtering Bioconductor Annotation Resources AnnotationHub 3.16.0 Client to access AnnotationHub resources anytime 0.3.11 Anything to 'POSIXct' or 'Date' Converter aod 1.3.3 Analysis of Overdispersed Data ape 5.8-1 Analyses of Phylogenetics and Evolution apeglm 1.30.0 Approximate posterior estimation for GLM coefficients aplot 0.2.5 Decorate a 'ggplot' with Associated Information argparse 2.2.5 Command Line Optional and Positional Argument Parser argparser 0.7.2 Command-Line Argument Parser arm 1.14-4 Data Analysis Using Regression and Multilevel/Hierarchical Models arrayhelpers 1.1-0 Convenience Functions for Arrays arrow 19.0.1.1 Integration to 'Apache' 'Arrow' ashr 2.2-63 Methods for Adaptive Shrinkage, using Empirical Bayes askpass 1.2.1 Password Entry Utilities for R, Git, and SSH assertthat 0.2.1 Easy Pre and Post Assertions assorthead 1.2.0 Assorted Header-Only C++ Libraries AUCell 1.30.0 AUCell: Analysis of 'gene set' activity in single-cell RNA-seq data (e.g. identify cells with specific gene signatures) babelgene 22.9 Gene Orthologs for Model Organisms in a Tidy Data Format backbone 2.1.4 Extracts the Backbone from Graphs backports 1.5.0 Reimplementations of Functions Introduced Since R-3.0.0 base 4.5.0 The R Base Package base64enc 0.1-3 Tools for base64 encoding base64url 1.4 Fast and URL-Safe Base64 Encoder and Decoder basefun 1.2-2 Infrastructure for Computing with Basis Functions basilisk 1.20.0 Freezing Python Dependencies Inside Bioconductor Packages basilisk.utils 1.20.0 Basilisk Installation Utilities batchelor 1.24.0 Single-Cell Batch Correction Methods bayesplot 1.12.0 Plotting for Bayesian Models bayestestR 0.15.3 Understand and Describe Bayesian Models and Posterior Distributions BBmisc 1.13 Miscellaneous Helper Functions for B. Bischl bbmle 1.0.25.1 Tools for General Maximum Likelihood Estimation bdsmatrix 1.3-7 Routines for Block Diagonal Symmetric Matrices beachmat 2.24.0 Compiling Bioconductor to Handle Each Matrix Type beeswarm 0.4.0 The Bee Swarm Plot, an Alternative to Stripchart bestglm 0.37.3 Best Subset GLM and Regression Utilities BgeeDB 2.34.0 Annotation and gene expression data retrieval from Bgee database. TopAnat, an anatomical entities Enrichment Analysis tool for UBERON ontology BH 1.87.0-1 Boost C++ Header Files bigD 0.3.1 Flexibly Format Dates and Times to a Given Locale biglm 0.9-3 Bounded Memory Linear and Generalized Linear Models bigmemory 4.6.4 Manage Massive Matrices with Shared Memory and Memory-Mapped Files bigmemory.sri 0.1.8 A Shared Resource Interface for Bigmemory Project Packages bio3d 2.4-5 Biological Structure Analysis Biobase 2.68.0 Biobase: Base functions for Bioconductor BiocFileCache 2.16.0 Manage Files Across Sessions BiocGenerics 0.54.0 S4 generic functions used in Bioconductor BiocIO 1.18.0 Standard Input and Output for Bioconductor Packages BiocManager 1.30.25 Access the Bioconductor Project Package Repository BiocNeighbors 2.2.0 Nearest Neighbor Detection for Bioconductor Packages BiocParallel 1.42.0 Bioconductor facilities for parallel evaluation BiocSingular 1.24.0 Singular Value Decomposition for Bioconductor Packages BiocVersion 3.21.1 Set the appropriate version of Bioconductor packages biocViews 1.76.0 Categorized views of R package repositories biomaRt 2.64.0 Interface to BioMart databases (i.e. Ensembl) Biostrings 2.76.0 Efficient manipulation of biological strings biovizBase 1.56.0 Basic graphic utilities for visualization of genomic data. bit 4.6.0 Classes and Methods for Fast Memory-Efficient Boolean Selections bit64 4.6.0-1 A S3 Class for Vectors of 64bit Integers bitops 1.0-9 Bitwise Operations blme 1.0-6 Bayesian Linear Mixed-Effects Models blob 1.2.4 A Simple S3 Class for Representing Vectors of Binary Data ('BLOBS') bmp 0.3 Read Windows Bitmap (BMP) Images boot 1.3-31 Bootstrap Functions (Originally by Angelo Canty for S) bootnet 1.6 Bootstrap Methods for Various Network Estimation Routines bread 0.4.1 Analyze Big Files Without Loading Them in Memory brew 1.0-10 Templating Framework for Report Generation bridgesampling 1.1-2 Bridge Sampling for Marginal Likelihoods and Bayes Factors brio 1.1.5 Basic R Input Output brms 2.22.0 Bayesian Regression Models using 'Stan' Brobdingnag 1.2-9 Very Large Numbers in R broom 1.0.8 Convert Statistical Objects into Tidy Tibbles broom.helpers 1.21.0 Helpers for Model Coefficients Tibbles bs4Dash 2.3.4 A 'Bootstrap 4' Version of 'shinydashboard' BSgenome 1.76.0 Software infrastructure for efficient representation of full genomes and their SNPs BSgenome.Hsapiens.UCSC.hg38 1.4.5 Full genomic sequences for Homo sapiens (UCSC genome hg38) bslib 0.9.0 Custom 'Bootstrap' 'Sass' Themes for 'shiny' and 'rmarkdown' ca 0.71.1 Simple, Multiple and Joint Correspondence Analysis cachem 1.1.0 Cache R Objects with Automatic Pruning Cairo 1.6-2 R Graphics Device using Cairo Graphics Library for Creating High-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG, PostScript) and Display (X11 and Win32) Output calibrate 1.7.7 Calibration of Scatterplot and Biplot Axes callr 3.7.6 Call R from R candisc 0.9.0 Visualizing Generalized Canonical Discriminant and Canonical Correlation Analysis car 3.1-3 Companion to Applied Regression carData 3.0-5 Companion to Applied Regression Data Sets cards 0.6.0 Analysis Results Data caret 7.0-1 Classification and Regression Training caTools 1.18.3 Tools: Moving Window Statistics, GIF, Base64, ROC AUC, etc CellChat 2.2.0 Inference and analysis of cell-cell communication from single-cell and spatially resolved transcriptomics data celldex 1.18.0 Index of Reference Cell Type Datasets cellranger 1.1.0 Translate Spreadsheet Cell Ranges to Rows and Columns checkmate 2.3.2 Fast and Versatile Argument Checks ChIPseeker 1.44.0 ChIPseeker for ChIP peak Annotation, Comparison, and Visualization chipseq 1.58.0 chipseq: A package for analyzing chipseq data chk 0.10.0 Check User-Supplied Function Arguments chromVAR 1.30.0 Chromatin Variation Across Regions chron 2.3-62 Chronological Objects which Can Handle Dates and Times circlize 0.4.16 Circular Visualization class 7.3-23 Functions for Classification classInt 0.4-11 Choose Univariate Class Intervals cli 3.6.5 Helpers for Developing Command Line Interfaces clipr 0.8.0 Read and Write from the System Clipboard clock 0.7.3 Date-Time Types and Tools clue 0.3-66 Cluster Ensembles cluster 2.1.8.1 \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. clustermole 1.1.1 Unbiased Single-Cell Transcriptomic Data Cell Type Identification clusterProfiler 4.16.0 A universal enrichment tool for interpreting omics data ClusterR 1.3.3 Gaussian Mixture Models, K-Means, Mini-Batch-Kmeans, K-Medoids and Affinity Propagation Clustering clustree 0.5.1 Visualise Clusterings at Different Resolutions cmprsk 2.2-12 Subdistribution Analysis of Competing Risks CNEr 1.43.0 CNE Detection and Visualization cocor 1.1-4 Comparing Correlations coda 0.19-4.1 Output Analysis and Diagnostics for MCMC codetools 0.2-20 Code Analysis Tools for R coin 1.4-3 Conditional Inference Procedures in a Permutation Test Framework colorRamp2 0.1.0 Generate Color Mapping Functions colorspace 2.1-1 A Toolbox for Manipulating and Assessing Colors and Palettes colourpicker 1.3.0 A Colour Picker Tool for Shiny and for Selecting Colours in Plots cols4all 0.8 Colors for all combinat 0.0-8 combinatorics utilities commonmark 1.9.5 High Performance CommonMark and Github Markdown Rendering in R comorbidity 1.1.0 Computing Comorbidity Scores compiler 4.5.0 The R Compiler Package ComplexHeatmap 2.24.0 Make Complex Heatmaps confintr 1.0.2 Confidence Intervals conflicted 1.2.0 An Alternative Conflict Resolution Strategy conquer 1.3.3 Convolution-Type Smoothed Quantile Regression coro 1.1.0 'Coroutines' for R corpcor 1.6.10 Efficient Estimation of Covariance and (Partial) Correlation corrplot 0.95 Visualization of a Correlation Matrix covr 3.6.4 Test Coverage for Packages cowplot 1.1.3 Streamlined Plot Theme and Plot Annotations for 'ggplot2' coxme 2.2-22 Mixed Effects Cox Models cpp11 0.5.2 A C++11 Interface for R's C Interface crayon 1.5.3 Colored Terminal Output credentials 2.0.2 Tools for Managing SSH and Git Credentials crossmatch 1.4-0 The Cross-Match Test crosstalk 1.2.1 Inter-Widget Interactivity for HTML Widgets crul 1.5.0 HTTP Client csaw 1.42.0 ChIP-Seq Analysis with Windows cubature 2.1.1 Adaptive Multivariate Integration over Hypercubes curl 6.2.2 A Modern and Flexible Web Client for R cvAUC 1.1.4 Cross-Validated Area Under the ROC Curve Confidence Intervals data.table 1.17.0 Extension of <code>data.frame</code> data.tree 1.1.0 General Purpose Hierarchical Data Structure datasets 4.5.0 The R Datasets Package datawizard 1.0.2 Easy Data Wrangling and Statistical Transformations date 1.2-42 Functions for Handling Dates DBI 1.2.3 R Database Interface dbparser 2.0.3 Drugs Databases Parser dbplyr 2.5.0 A 'dplyr' Back End for Databases dbscan 1.2.2 Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Related Algorithms DDRTree 0.1.5 Learning Principal Graphs with DDRTree decor 1.0.2 Retrieve Code Decorations DelayedArray 0.34.1 A unified framework for working transparently with on-disk and in-memory array-like datasets DelayedMatrixStats 1.30.0 Functions that Apply to Rows and Columns of 'DelayedMatrix' Objects deldir 2.0-4 Delaunay Triangulation and Dirichlet (Voronoi) Tessellation dendextend 1.19.0 Extending 'dendrogram' Functionality in R densityClust 0.3.3 Clustering by Fast Search and Find of Density Peaks densvis 1.18.0 Density-Preserving Data Visualization via Non-Linear Dimensionality Reduction DEoptimR 1.1-3-1 Differential Evolution Optimization in Pure R Deriv 4.1.6 Symbolic Differentiation desc 1.4.3 Manipulate DESCRIPTION Files DescTools 0.99.60 Tools for Descriptive Statistics DESeq2 1.48.0 Differential gene expression analysis based on the negative binomial distribution devtools 2.4.5 Tools to Make Developing R Packages Easier diagram 1.6.5 Functions for Visualising Simple Graphs (Networks), Plotting Flow Diagrams DiagrammeR 1.0.11 Graph/Network Visualization dials 1.4.0 Tools for Creating Tuning Parameter Values DiceDesign 1.10 Designs of Computer Experiments dichromat 2.0-0.1 Color Schemes for Dichromats diffobj 0.3.6 Diffs for R Objects digest 0.6.37 Create Compact Hash Digests of R Objects dir.expiry 1.16.0 Managing Expiration for Cache Directories DirichletMultinomial 1.50.0 Dirichlet-Multinomial Mixture Model Machine Learning for Microbiome Data distributional 0.5.0 Vectorised Probability Distributions DNAcopy 1.82.0 DNA Copy Number Data Analysis doBy 4.6.26 Groupwise Statistics, LSmeans, Linear Estimates, Utilities docopt 0.7.2 Command-Line Interface Specification Language doFuture 1.0.2 Use Foreach to Parallelize via the Future Framework doMC 1.3.8 Foreach Parallel Adaptor for 'parallel' doParallel 1.0.17 Foreach Parallel Adaptor for the 'parallel' Package doRNG 1.8.6.2 Generic Reproducible Parallel Backend for 'foreach' Loops DOSE 4.2.0 Disease Ontology Semantic and Enrichment analysis doSNOW 1.0.20 Foreach Parallel Adaptor for the 'snow' Package dotCall64 1.2 Enhanced Foreign Function Interface Supporting Long Vectors DoubleExpSeq 1.1 Differential Exon Usage Test for RNA-Seq Data via Empirical Bayes Shrinkage of the Dispersion Parameter downlit 0.4.4 Syntax Highlighting and Automatic Linking downloader 0.4.1 Download Files over HTTP and HTTPS dplyr 1.1.4 A Grammar of Data Manipulation dqrng 0.4.1 Fast Pseudo Random Number Generators DT 0.33 A Wrapper of the JavaScript Library 'DataTables' dtplyr 1.3.1 Data Table Back-End for 'dplyr' dygraphs 1.1.1.6 Interface to 'Dygraphs' Interactive Time Series Charting Library dynamicTreeCut 1.63-1 Methods for Detection of Clusters in Hierarchical Clustering Dendrograms e1071 1.7-16 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien edgeR 4.6.1 Empirical Analysis of Digital Gene Expression Data in R egg 0.4.5 Extensions for 'ggplot2': Custom Geom, Custom Themes, Plot Alignment, Labelled Panels, Symmetric Scales, and Fixed Panel Size eigenmodel 1.11 Semiparametric Factor and Regression Models for Symmetric Relational Data ellipse 0.5.0 Functions for Drawing Ellipses and Ellipse-Like Confidence Regions ellipsis 0.3.2 Tools for Working with ... emdbook 1.3.13 Support Functions and Data for \"Ecological Models and Data\" emmeans 1.11.0 Estimated Marginal Means, aka Least-Squares Means emoa 0.5-3 Evolutionary Multiobjective Optimization Algorithms EnhancedVolcano 1.26.0 Publication-ready volcano plots with enhanced colouring and labeling EnrichedHeatmap 1.38.0 Making Enriched Heatmaps enrichplot 1.28.0 Visualization of Functional Enrichment Result EnsDb.Hsapiens.v86 2.99.0 Ensembl based annotation package ensembldb 2.32.0 Utilities to create and use Ensembl-based annotation databases EnvStats 3.1.0 Package for Environmental Statistics, Including US EPA Guidance estimability 1.5.1 Tools for Assessing Estimability of Linear Predictions etrunct 0.1 Computes Moments of Univariate Truncated t Distribution eulerr 7.0.2 Area-Proportional Euler and Venn Diagrams with Ellipses evaluate 1.0.3 Parsing and Evaluation Tools that Provide More Details than the Default Exact 3.3 Unconditional Exact Test exactRankTests 0.8-35 Exact Distributions for Rank and Permutation Tests ExperimentHub 2.16.0 Client to access ExperimentHub resources expm 1.0-0 Matrix Exponential, Log, 'etc' extrafont 0.19 Tools for Using Fonts extrafontdb 1.0 Package for holding the database for the extrafont package factoextra 1.0.7 Extract and Visualize the Results of Multivariate Data Analyses FactoMineR 2.11 Multivariate Exploratory Data Analysis and Data Mining fANCOVA 0.6-1 Nonparametric Analysis of Covariance fansi 1.0.6 ANSI Control Sequence Aware String Functions farver 2.1.2 High Performance Colour Space Manipulation fastcluster 1.2.6 Fast Hierarchical Clustering Routines for R and 'Python' fastDummies 1.7.5 Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables fastICA 1.2-7 FastICA Algorithms to Perform ICA and Projection Pursuit fastmap 1.2.0 Fast Data Structures fastmatch 1.1-6 Fast 'match()' Function fdrtool 1.2.18 Estimation of (Local) False Discovery Rates and Higher Criticism feather 0.3.5 R Bindings to the Feather 'API' ff 4.5.2 Memory-Efficient Storage of Large Data on Disk and Fast Access Functions fftwtools 0.9-11 Wrapper for 'FFTW3' Includes: One-Dimensional, Two-Dimensional, Three-Dimensional, and Multivariate Transforms fgsea 1.34.0 Fast Gene Set Enrichment Analysis fields 16.3.1 Tools for Spatial Data filelock 1.0.3 Portable File Locking findpython 1.0.9 Functions to Find an Acceptable Python Binary fitdistrplus 1.2-2 Help to Fit of a Parametric Distribution to Non-Censored or Censored Data flashClust 1.01-2 Implementation of optimal hierarchical clustering flextable 0.9.7 Functions for Tabular Reporting fmsb 0.7.6 Functions for Medical Statistics Book with some Demographic Data FNN 1.1.4.1 Fast Nearest Neighbor Search Algorithms and Applications fontawesome 0.5.3 Easily Work with 'Font Awesome' Icons fontBitstreamVera 0.1.1 Fonts with 'Bitstream Vera Fonts' License fontLiberation 0.1.0 Liberation Fonts fontquiver 0.2.1 Set of Installed Fonts forcats 1.0.0 Tools for Working with Categorical Variables (Factors) foreach 1.5.2 Provides Foreach Looping Construct foreign 0.8-90 Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... formatR 1.14 Format R Code Automatically formattable 0.2.1 Create 'Formattable' Data Structures Formula 1.2-5 Extended Model Formulas formula.tools 1.7.1 Programmatic Utilities for Manipulating Formulas, Expressions, Calls, Assignments and Other R Objects freesurferformats 0.1.18 Read and Write 'FreeSurfer' Neuroimaging File Formats fresh 0.2.1 Create Custom 'Bootstrap' Themes to Use in 'Shiny' fs 1.6.6 Cross-Platform File System Operations Based on 'libuv' fsbrain 0.5.5 Managing and Visualizing Brain Surface Data fslr 2.25.3 Wrapper Functions for 'FSL' ('FMRIB' Software Library) from Functional MRI of the Brain ('FMRIB') fst 0.9.8 Lightning Fast Serialization of Data Frames fstcore 0.10.0 R Bindings to the 'Fstlib' Library furrr 0.3.1 Apply Mapping Functions in Parallel using Futures futile.logger 1.4.3 A Logging Utility for R futile.options 1.0.1 Futile Options Management future 1.40.0 Unified Parallel and Distributed Processing in R for Everyone future.apply 1.11.3 Apply Function to Elements in Parallel using Futures gam 1.22-5 Generalized Additive Models gamm4 0.2-7 Generalized Additive Mixed Models using 'mgcv' and 'lme4' gargle 1.5.2 Utilities for Working with Google APIs gbm 2.2.2 Generalized Boosted Regression Models gclus 1.3.3 Clustering Graphics gcrma 2.80.0 Background Adjustment Using Sequence Information gdata 3.0.1 Various R Programming Tools for Data Manipulation gdsfmt 1.44.0 R Interface to CoreArray Genomic Data Structure (GDS) Files gdtools 0.4.2 Utilities for Graphical Rendering and Fonts Management geeM 0.10.1 Solve Generalized Estimating Equations geepack 1.3.12 Generalized Estimating Equation Package genefilter 1.90.0 genefilter: methods for filtering genes from high-throughput experiments geneplotter 1.86.0 Graphics related functions for Bioconductor generics 0.1.3 Common S3 Generics not Provided by Base R Methods Related to Model Fitting GENIE3 1.30.0 GEne Network Inference with Ensemble of trees genio 1.1.2 Genetics Input/Output Functions GenomeInfoDb 1.44.0 Utilities for manipulating chromosome names, including modifying them to follow a particular naming style GenomeInfoDbData 1.2.14 Species and taxonomy ID look up tables used by GenomeInfoDb GenomicAlignments 1.44.0 Representation and manipulation of short genomic alignments GenomicFeatures 1.60.0 Query the gene models of a given organism/assembly GenomicRanges 1.60.0 Representation and manipulation of genomic intervals GenSA 1.1.14.1 R Functions for Generalized Simulated Annealing geojsonsf 2.0.3 GeoJSON to Simple Feature Converter GEOmetadb 1.70.0 A compilation of metadata from NCBI GEO geometries 0.2.4 Convert Between R Objects and Geometric Structures GEOquery 2.76.0 Get data from NCBI Gene Expression Omnibus (GEO) geos 0.2.4 Open Source Geometry Engine ('GEOS') R API gert 2.1.5 Simple Git Client for R getopt 1.20.4 C-Like 'getopt' Behavior GetoptLong 1.0.5 Parsing Command-Line Arguments and Simple Variable Interpolation gfonts 0.2.0 Offline 'Google' Fonts for 'Markdown' and 'Shiny' ggalluvial 0.12.5 Alluvial Plots in 'ggplot2' GGally 2.2.1 Extension to 'ggplot2' ggbeeswarm 0.7.2 Categorical Scatter (Violin Point) Plots ggbio 1.56.0 Visualization tools for genomic data ggdendro 0.2.0 Create Dendrograms and Tree Diagrams Using 'ggplot2' ggdist 3.3.3 Visualizations of Distributions and Uncertainty ggforce 0.4.2 Accelerating 'ggplot2' ggformula 0.12.0 Formula Interface to the Grammar of Graphics ggfun 0.1.8 Miscellaneous Functions for 'ggplot2' ggnetwork 0.5.13 Geometries to Plot Networks with 'ggplot2' ggnewscale 0.5.1 Multiple Fill and Colour Scales in 'ggplot2' ggplot2 3.5.2 Create Elegant Data Visualisations Using the Grammar of Graphics ggplotify 0.1.2 Convert Plot to 'grob' or 'ggplot' Object ggpmisc 0.6.1 Miscellaneous Extensions to 'ggplot2' ggpp 0.5.8-1 Grammar Extensions to 'ggplot2' ggpubr 0.6.0 'ggplot2' Based Publication Ready Plots ggraph 2.2.1 An Implementation of Grammar of Graphics for Graphs and Networks ggrastr 1.0.2 Rasterize Layers for 'ggplot2' ggrepel 0.9.6 Automatically Position Non-Overlapping Text Labels with 'ggplot2' ggridges 0.5.6 Ridgeline Plots in 'ggplot2' ggsci 3.2.0 Scientific Journal and Sci-Fi Themed Color Palettes for 'ggplot2' ggsignif 0.6.4 Significance Brackets for 'ggplot2' ggstance 0.3.7 Horizontal 'ggplot2' Components ggstats 0.9.0 Extension to 'ggplot2' for Plotting Stats ggtangle 0.0.6 Draw Network with Data ggtext 0.1.2 Improved Text Rendering Support for 'ggplot2' ggthemes 5.1.0 Extra Themes, Scales and Geoms for 'ggplot2' ggtree 3.16.0 an R package for visualization of tree and annotation data ggVennDiagram 1.5.2 A 'ggplot2' Implement of Venn Diagram gh 1.4.1 'GitHub' 'API' Giotto 4.2.1 Spatial Single-Cell Transcriptomics Toolbox GiottoClass 0.4.7 Giotto Suite Object Definitions and Framework GiottoUtils 0.2.4 Giotto Suite Utilities GiottoVisuals 0.2.12 Visuals for the Giotto spatial biology analysis ecosystem gistr 0.9.0 Work with 'GitHub' 'Gists' gitcreds 0.1.2 Query 'git' Credentials from 'R' glasso 1.11 Graphical Lasso: Estimation of Gaussian Graphical Models gld 2.6.7 Estimation and Use of the Generalised (Tukey) Lambda Distribution Glimma 2.18.0 Interactive visualizations for gene expression analysis glmGamPoi 1.20.0 Fit a Gamma-Poisson Generalized Linear Model glmmTMB 1.1.11 Generalized Linear Mixed Models using Template Model Builder glmnet 4.1-8 Lasso and Elastic-Net Regularized Generalized Linear Models GlobalOptions 0.1.2 Generate Functions to Get or Set Global Options globals 0.17.0 Identify Global Objects in R Expressions glue 1.8.0 Interpreted String Literals gmp 0.7-5 Multiple Precision Arithmetic GO.db 3.21.0 A set of annotation maps describing the entire Gene Ontology goftest 1.2-3 Classical Goodness-of-Fit Tests for Univariate Distributions googledrive 2.1.1 An Interface to Google Drive googlesheets4 1.1.1 Access Google Sheets using the Sheets API V4 GOSemSim 2.34.0 GO-terms Semantic Similarity Measures gower 1.0.2 Gower's Distance GPArotation 2025.3-1 Gradient Projection Factor Rotation GPfit 1.0-9 Gaussian Processes Modeling gplots 3.2.0 Various R Programming Tools for Plotting Data gprofiler2 0.2.3 Interface to the 'g:Profiler' Toolset graph 1.86.0 graph: A package to handle graph data structures graphics 4.5.0 The R Graphics Package graphite 1.54.0 GRAPH Interaction from pathway Topological Environment graphlayouts 1.2.2 Additional Layout Algorithms for Network Visualizations grDevices 4.5.0 The R Graphics Devices and Support for Colours and Fonts grid 4.5.0 The Grid Graphics Package gridBase 0.4-7 Integration of base and grid graphics gridExtra 2.3 Miscellaneous Functions for \"Grid\" Graphics gridGraphics 0.5-1 Redraw Base Graphics Using 'grid' Graphics gridtext 0.1.5 Improved Text Rendering Support for 'Grid' Graphics grpreg 3.5.0 Regularization Paths for Regression Models with Grouped Covariates grr 0.9.5 Alternative Implementations of Base R Functions GSA 1.03.3 Gene Set Analysis GSEABase 1.70.0 Gene set enrichment data structures and methods gson 0.1.0 Base Class and Methods for 'gson' Format gstat 2.1-3 Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation gsubfn 0.7 Utilities for Strings and Function Arguments GSVA 2.2.0 Gene Set Variation Analysis for Microarray and RNA-Seq Data gt 1.0.0 Easily Create Presentation-Ready Display Tables gtable 0.3.6 Arrange 'Grobs' in Tables gtools 3.9.5 Various R Programming Tools gtsummary 2.2.0 Presentation-Ready Data Summary and Analytic Result Tables GWASExactHW 1.2 Exact Hardy-Weinburg Testing for Genome Wide Association Studies GWASTools 1.54.0 Tools for Genome Wide Association Studies gwasurvivr 1.26.0 gwasurvivr: an R package for genome wide survival analysis gypsum 1.4.0 Interface to the gypsum REST API h2o 3.44.0.3 R Interface for the 'H2O' Scalable Machine Learning Platform h5mread 1.0.0 A fast HDF5 reader hardhat 1.4.1 Construct Modeling Packages harmony 1.2.3 Fast, Sensitive, and Accurate Integration of Single Cell Data hash 2.2.6.3 Full Featured Implementation of Hash Tables/Associative Arrays/Dictionaries haven 2.5.4 Import and Export 'SPSS', 'Stata' and 'SAS' Files HDF5Array 1.36.0 HDF5 datasets as array-like objects in R hdf5r 1.3.12 Interface to the 'HDF5' Binary Data Format HDO.db 1.0.0 A set of annotation maps describing the entire Human Disease Ontology heatmaply 1.5.0 Interactive Cluster Heat Maps Using 'plotly' and 'ggplot2' heplots 1.7.4 Visualizing Hypothesis Tests in Multivariate Linear Models here 1.0.1 A Simpler Way to Find Your Files hexbin 1.28.5 Hexagonal Binning Routines HGNChelper 0.8.15 Identify and Correct Invalid HGNC Human Gene Symbols and MGI Mouse Gene Symbols HiddenMarkov 1.8-14 Hidden Markov Models highr 0.11 Syntax Highlighting for R Source Code Hmisc 5.2-3 Harrell Miscellaneous hms 1.1.3 Pretty Time of Day hrbrthemes 0.8.7 Additional Themes, Theme Components and Utilities for 'ggplot2' HSMMSingleCell 1.28.0 Single-cell RNA-Seq for differentiating human skeletal muscle myoblasts (HSMM) htmlTable 2.4.3 Advanced Tables for Markdown/HTML htmltools 0.5.8.1 Tools for HTML htmlwidgets 1.6.4 HTML Widgets for R httpcode 0.3.0 'HTTP' Status Code Helper httpuv 1.6.16 HTTP and WebSocket Server Library httr 1.4.7 Tools for Working with URLs and HTTP httr2 1.1.2 Perform HTTP Requests and Process the Responses hwriter 1.3.2.1 HTML Writer - Outputs R Objects in HTML Format ica 1.0-3 Independent Component Analysis ids 1.0.1 Generate Random Identifiers igraph 2.1.4 Network Analysis and Visualization imager 1.0.3 Image Processing Library Based on 'CImg' impute 1.82.0 impute: Imputation for microarray data infer 1.0.8 Tidy Statistical Inference infercnv 1.24.0 Infer Copy Number Variation from Single-Cell RNA-Seq Data influenceR 0.1.5 Software Tools to Quantify Structural Importance of Nodes in a Network ini 0.3.1 Read and Write '.ini' Files inline 0.3.21 Functions to Inline C, C++, Fortran Function Calls from R insight 1.2.0 Easy Access to Model Information for Various Model Objects interactiveDisplayBase 1.46.0 Base package for enabling powerful shiny web displays of Bioconductor objects interp 1.1-6 Interpolation Methods intervals 0.15.5 Tools for Working with Points and Intervals invgamma 1.1 The Inverse Gamma Distribution iotools 0.3-5 I/O Tools for Streaming ipred 0.9-15 Improved Predictors IRanges 2.42.0 Foundation of integer range manipulation in Bioconductor IRdisplay 1.1 'Jupyter' Display Machinery IRkernel 1.3.2 Native R Kernel for the 'Jupyter Notebook' irlba 2.3.5.1 Fast Truncated Singular Value Decomposition and Principal Components Analysis for Large Dense and Sparse Matrices IsingFit 0.4 Fitting Ising Models Using the ELasso Method IsingSampler 0.2.3 Sampling Methods and Distribution Functions for the Ising Model isoband 0.2.7 Generate Isolines and Isobands from Regularly Spaced Elevation Grids iterators 1.0.14 Provides Iterator Construct ITKR 0.6.0.0.2 ITK in R janitor 2.2.1 Simple Tools for Examining and Cleaning Dirty Data JASPAR2020 0.99.10 Data package for JASPAR database (version 2020) jomo 2.7-6 Multilevel Joint Modelling Multiple Imputation jpeg 0.1-11 Read and write JPEG images jquerylib 0.1.4 Obtain 'jQuery' as an HTML Dependency Object jsonify 1.2.2 Convert Between 'R' Objects and Javascript Object Notation (JSON) jsonlite 2.0.0 A Simple and Robust JSON Parser and Generator for R jsonvalidate 1.5.0 Validate 'JSON' Schema juicyjuice 0.1.0 Inline CSS Properties into HTML Tags Using 'juice' kableExtra 1.4.0 Construct Complex Table with 'kable' and Pipe Syntax KEGGgraph 1.68.0 KEGGgraph: A graph approach to KEGG PATHWAY in R and Bioconductor KEGGREST 1.48.0 Client-side REST access to the Kyoto Encyclopedia of Genes and Genomes (KEGG) kernlab 0.9-33 Kernel-Based Machine Learning Lab KernSmooth 2.23-26 Functions for Kernel Smoothing Supporting Wand &amp; Jones (1995) kinship2 1.9.6.1 Pedigree Functions km.ci 0.5-6 Confidence Intervals for the Kaplan-Meier Estimator KMsurv 0.1-5 Data sets from Klein and Moeschberger (1997), Survival Analysis knitr 1.50 A General-Purpose Package for Dynamic Report Generation in R ks 1.14.3 Kernel Smoothing kutils 1.73 Project Management Tools labeling 0.4.3 Axis Labeling labelled 2.14.0 Manipulating Labelled Data laeken 0.5.3 Estimation of Indicators on Social Exclusion and Poverty lambda.r 1.2.4 Modeling Data with Functional Programming later 1.4.2 Utilities for Scheduling Functions to Execute Later with Event Loops lattice 0.22-7 Trellis Graphics for R latticeExtra 0.6-30 Extra Graphical Utilities Based on Lattice lava 1.8.1 Latent Variable Models lavaan 0.6-19 Latent Variable Analysis lazyeval 0.2.2 Lazy (Non-Standard) Evaluation leafem 0.2.3 'leaflet' Extensions for 'mapview' leafgl 0.2.2 High-Performance 'WebGl' Rendering for Package 'leaflet' leaflegend 1.2.1 Add Custom Legends to 'leaflet' Maps leaflet 2.2.2 Create Interactive Web Maps with the JavaScript 'Leaflet' Library leaflet.providers 2.0.0 Leaflet Providers leafpop 0.1.0 Include Tables, Images and Graphs in Leaflet Pop-Ups leafsync 0.1.0 Small Multiples for Leaflet Web Maps leaps 3.2 Regression Subset Selection leiden 0.4.3.1 R Implementation of Leiden Clustering Algorithm leidenbase 0.1.35 R and C/C++ Wrappers to Run the Leiden find_partition() Function lfa 2.8.0 Logistic Factor Analysis for Categorical Data lhs 1.2.0 Latin Hypercube Samples libcoin 1.0-10 Linear Test Statistics for Permutation Inference libgeos 3.11.1-3 Open Source Geometry Engine ('GEOS') C API lifecycle 1.0.4 Manage the Life Cycle of your Package Functions limma 3.64.0 Linear Models for Microarray and Omics Data linkcomm 1.0-14 Tools for Generating, Visualizing, and Analysing Link Communities in Networks lisrelToR 0.3 Import Output from LISREL into R listenv 0.9.1 Environments Behaving (Almost) as Lists litedown 0.7 A Lightweight Version of R Markdown lme4 1.1-37 Linear Mixed-Effects Models using 'Eigen' and S4 lmerTest 3.1-3 Tests in Linear Mixed Effects Models lmodel2 1.7-4 Model II Regression lmom 3.2 L-Moments lmtest 0.9-40 Testing Linear Regression Models lobstr 1.1.2 Visualize R Data Structures with Trees locfdr 1.1-8 Computes Local False Discovery Rates locfit 1.5-9.12 Local Regression, Likelihood and Density Estimation logger 0.4.0 A Lightweight, Modern and Flexible Logging Utility logistf 1.26.1 Firth's Bias-Reduced Logistic Regression loo 2.8.0 Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models lubridate 1.9.4 Make Dealing with Dates a Little Easier lwgeom 0.2-14 Bindings to Selected 'liblwgeom' Functions for Simple Features magick 2.8.6 Advanced Graphics and Image-Processing in R magrittr 2.0.3 A Forward-Pipe Operator for R manhattanly 0.3.0 Interactive Q-Q and Manhattan Plots Using 'plotly.js' manipulateWidget 0.11.1 Add Even More Interactivity to Interactive Charts mapproj 1.2.11 Map Projections maps 3.4.2.1 Draw Geographical Maps mapview 2.11.2 Interactive Viewing of Spatial Data in R marginaleffects 0.25.1 Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests markdown 2.0 Render Markdown with 'commonmark' MASS 7.3-65 Support Functions and Datasets for Venables and Ripley's MASS mathjaxr 1.6-0 Using 'Mathjax' in Rd Files Matrix 1.7-3 Sparse and Dense Matrix Classes and Methods matrixcalc 1.0-6 Collection of Functions for Matrix Calculations MatrixGenerics 1.20.0 S4 Generic Summary Statistic Functions that Operate on Matrix-Like Objects MatrixModels 0.5-4 Modelling with Sparse and Dense Matrices matrixStats 1.5.0 Functions that Apply to Rows and Columns of Matrices (and to Vectors) maxLik 1.5-2.1 Maximum Likelihood Estimation and Related Tools maxstat 0.7-25 Maximally Selected Rank Statistics mbend 1.3.1 Matrix Bending MCL 1.0 Markov Cluster Algorithm mclogit 0.9.6 Multinomial Logit Models, with or without Random Effects or Overdispersion mclust 6.1.1 Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation memisc 0.99.31.8.3 Management of Survey Data and Presentation of Analysis Results memoise 2.0.1 'Memoisation' of Functions memuse 4.2-3 Memory Estimation Utilities MESS 0.5.12 Miscellaneous Esoteric Statistical Scripts metadat 1.4-0 Meta-Analysis Datasets metafor 4.8-0 Meta-Analysis Package for R metapod 1.16.0 Meta-Analyses on P-Values of Differential Analyses methods 4.5.0 Formal Methods and Classes Metrics 0.1.4 Evaluation Metrics for Machine Learning mets 1.3.6 Analysis of Multivariate Event Times mgcv 1.9-3 Mixed GAM Computation Vehicle with Automatic Smoothness Estimation mgm 1.2-15 Estimating Time-Varying k-Order Mixed Graphical Models mi 1.1 Missing Data Imputation and Model Checking mice 3.17.0 Multivariate Imputation by Chained Equations microbenchmark 1.5.0 Accurate Timing Functions mime 0.13 Map Filenames to MIME Types miniUI 0.1.2 Shiny UI Widgets for Small Screens minqa 1.2.8 Derivative-Free Optimization Algorithms by Quadratic Approximation misc3d 0.9-1 Miscellaneous 3D Plots miscTools 0.6-28 Miscellaneous Tools and Utilities mitml 0.4-5 Tools for Multiple Imputation in Multilevel Modeling mitools 2.4 Tools for Multiple Imputation of Missing Data mixsqp 0.3-54 Sequential Quadratic Programming for Fast Maximum-Likelihood Estimation of Mixture Proportions mixtools 2.0.0.1 Tools for Analyzing Finite Mixture Models mlr 2.19.2 Machine Learning in R mlrMBO 1.1.5.1 Bayesian Optimization and Model-Based Optimization of Expensive Black-Box Functions mnormt 2.1.1 The Multivariate Normal and t Distributions, and Their Truncated Versions modeldata 1.4.0 Data Sets Useful for Modeling Examples modelenv 0.2.0 Provide Tools to Register Models for Use in 'tidymodels' ModelMetrics 1.2.2.2 Rapid Calculation of Model Metrics modelr 0.1.11 Modelling Functions that Work with the Pipe modeltools 0.2-24 Tools and Classes for Statistical Models monocle 2.36.0 Clustering, differential expression, and trajectory analysis for single- cell RNA-Seq monocle3 1.3.7 Clustering, Differential Expression, and Trajectory Analysis for Single-Cell RNA-Seq mosaic 1.9.1 Project MOSAIC Statistics and Mathematics Teaching Utilities mosaicCore 0.9.4.0 Common Utilities for Other MOSAIC-Family Packages mosaicData 0.20.4 Project MOSAIC Data Sets motifmatchr 1.30.0 Fast Motif Matching in R msigdbr 10.0.2 MSigDB Gene Sets for Multiple Organisms in a Tidy Data Format multcomp 1.4-28 Simultaneous Inference in General Parametric Models multcompView 0.1-10 Visualizations of Paired Comparisons multicool 1.0.1 Permutations of Multisets in Cool-Lex Order multicross 2.1.0 A Graph-Based Test for Comparing Multivariate Distributions in the Multi Sample Framework multtest 2.64.0 Resampling-based multiple hypothesis testing munsell 0.5.1 Utilities for Using Munsell Colours muscat 1.22.0 Multi-sample multi-group scRNA-seq data analysis tools mutoss 0.1-13 Unified Multiple Testing Procedures mvtnorm 1.3-3 Multivariate Normal and t Distributions nabor 0.5.0 Wraps 'libnabo', a Fast K Nearest Neighbour Library for Low Dimensions nanoarrow 0.6.0-1 Interface to the 'nanoarrow' 'C' Library nbpMatching 1.5.6 Functions for Optimal Non-Bipartite Matching ncdf4 1.24 Interface to Unidata netCDF (Version 4 or Earlier) Format Data Files network 1.19.0 Classes for Relational Data NetworkComparisonTest 2.2.2 Statistical Comparison of Two Networks Based on Several Invariance Measures networkD3 0.4.1 D3 JavaScript Network Graphs from R NetworkToolbox 1.4.2 Methods and Measures for Brain, Cognitive, and Psychometric Network Analysis networktools 1.6.0 Tools for Identifying Important Nodes in Networks neurobase 1.32.4 'Neuroconductor' Base Package with Helper Functions for 'nifti' Objects nleqslv 3.3.5 Solve Systems of Nonlinear Equations nlme 3.1-168 Linear and Nonlinear Mixed Effects Models nloptr 2.2.1 R Interface to NLopt NMF 0.28 Algorithms and Framework for Nonnegative Matrix Factorization (NMF) nnet 7.3-20 Feed-Forward Neural Networks and Multinomial Log-Linear Models nnls 1.6 The Lawson-Hanson Algorithm for Non-Negative Least Squares (NNLS) nortest 1.0-4 Tests for Normality numDeriv 2016.8-1.1 Accurate Numerical Derivatives NxtIRFdata 1.14.0 Data for NxtIRF odbc 1.6.1 Connect to ODBC Compatible Databases (using the DBI Interface) officer 0.6.8 Manipulation of Microsoft Word and PowerPoint Documents oligo 1.72.0 Preprocessing tools for oligonucleotide arrays oligoClasses 1.70.0 Classes for high-throughput arrays supported by oligo and crlmm ompBAM 1.12.0 C++ Library for OpenMP-based multi-threaded sequential profiling of Binary Alignment Map (BAM) files OpenMx 2.21.13 Extended Structural Equation Modelling openssl 2.3.2 Toolkit for Encryption, Signatures and Certificates Based on OpenSSL openxlsx 4.2.8 Read, Write and Edit xlsx Files operator.tools 1.6.3 Utilities for Working with R's Operators optimx 2025-4.9 Expanded Replacement and Extension of the 'optim' Function optparse 1.7.5 Command Line Option Parser orca 1.1-3 Computation of Graphlet Orbit Counts in Sparse Graphs ordinal 2023.12-4.1 Regression Models for Ordinal Data org.Hs.eg.db 3.21.0 Genome wide annotation for Human org.Mm.eg.db 3.21.0 Genome wide annotation for Mouse OrganismDbi 1.50.0 Software to enable the smooth interfacing of different database packages oro.nifti 0.11.4 Rigorous - 'NIfTI' + 'ANALYZE' + 'AFNI' : Input / Output orthopolynom 1.0-6.1 Collection of Functions for Orthogonal and Orthonormal Polynomials outliers 0.15 Tests for Outliers packrat 0.9.2 A Dependency Management System for Projects and their R Package Dependencies pak 0.8.0.2 Another Approach to Package Installation pals 1.10 Color Palettes, Colormaps, and Tools to Evaluate Them pammtools 0.7.3 Piece-Wise Exponential Additive Mixed Modeling Tools for Survival Analysis pan 1.9 Multiple Imputation for Multivariate Panel or Clustered Data parallel 4.5.0 Support for Parallel Computation in R parallelDist 0.2.6 Parallel Distance Matrix Computation using Multiple Threads parallelly 1.43.0 Enhancing the 'parallel' Package parallelMap 1.5.1 Unified Interface to Parallelization Back-Ends ParamHelpers 1.14.2 Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning paran 1.5.4 Horn's Test of Principal Components/Factors parsnip 1.3.1 A Common API to Modeling and Analysis Functions patchwork 1.3.0 The Composer of Plots pathview 1.48.0 a tool set for pathway based data integration and visualization pbapply 1.7-2 Adding Progress Bar to '*apply' Functions pbdZMQ 0.3-14 Programming with Big Data -- Interface to 'ZeroMQ' pbivnorm 0.6.0 Vectorized Bivariate Normal CDF pbkrtest 0.5.4 Parametric Bootstrap, Kenward-Roger and Satterthwaite Based Methods for Test in Mixed Models pbmcapply 1.5.1 Tracking the Progress of Mc*pply with Progress Bar pcaMethods 2.0.0 A collection of PCA methods pdist 1.2.1 Partitioned Distance Function pec 2023.04.12 Prediction Error Curves for Risk Prediction Models in Survival Analysis performance 0.13.0 Assessment of Regression Models Performance permute 0.9-7 Functions for Generating Restricted Permutations of Data pfamAnalyzeR 1.8.0 Identification of domain isotypes in pfam data pheatmap 1.0.12 Pretty Heatmaps phia 0.3-1 Post-Hoc Interaction Analysis phyclust 0.1-34 Phylogenetic Clustering (Phyloclustering) pillar 1.10.2 Coloured Formatting for Columns pinfsc50 1.3.0 Sequence ('FASTA'), Annotation ('GFF') and Variants ('VCF') for 17 Samples of 'P. Infestans\" and 1 'P. Mirabilis' pixmap 0.4-13 Bitmap Images / Pixel Maps pkgbuild 1.4.7 Find Tools Needed to Build R Packages pkgconfig 2.0.3 Private Configuration for 'R' Packages pkgdown 2.1.2 Make Static HTML Documentation for a Package pkgfilecache 0.1.5 Download and Manage Optional Package Data pkgload 1.4.0 Simulate Package Installation and Attach PKI 0.1-14 Public Key Infrastucture for R Based on the X.509 Standard plink 1.5-1 IRT Separate Calibration Linking Methods plogr 0.2.0 The 'plog' C++ Logging Library plot3D 1.4.1 Plotting Multi-Dimensional Data plotly 4.10.4 Create Interactive Web Graphics via 'plotly.js' plotrix 3.8-4 Various Plotting Functions pls 2.8-5 Partial Least Squares and Principal Component Regression plyr 1.8.9 Tools for Splitting, Applying and Combining Data png 0.1-8 Read and write PNG images polspline 1.1.25 Polynomial Spline Routines polyclip 1.10-7 Polygon Clipping polylabelr 0.3.0 Find the Pole of Inaccessibility (Visual Center) of a Polygon polynom 1.4-1 A Collection of Functions to Implement a Class for Univariate Polynomial Manipulations pool 1.0.4 Object Pooling posterior 1.6.1 Tools for Working with Posterior Distributions poweRlaw 1.0.0 Analysis of Heavy Tailed Distributions ppcor 1.1 Partial and Semi-Partial (Part) Correlation pracma 2.4.4 Practical Numerical Math Functions praise 1.0.0 Praise Users preprocessCore 1.70.0 A collection of pre-processing functions presto 1.0.0 Fast Functions for Differential Expression using Wilcox and AUC prettyunits 1.2.0 Pretty, Human Readable Formatting of Quantities pROC 1.18.5 Display and Analyze ROC Curves processx 3.8.6 Execute and Control System Processes prodlim 2025.04.28 Product-Limit Estimation for Censored Event History Analysis profileplyr 1.24.0 Visualization and annotation of read signal over genomic ranges with profileplyr profvis 0.4.0 Interactive Visualizations for Profiling R Code progress 1.2.3 Terminal Progress Bars progressr 0.15.1 An Inclusive, Unifying API for Progress Updates promises 1.3.2 Abstractions for Promise-Based Asynchronous Programming proteoDA 1.0.1 Streamlined Differential Abundance Analysis of Proteomic Data ProtGenerics 1.40.0 Generic infrastructure for Bioconductor mass spectrometry packages proto 1.0.0 Prototype Object-Based Programming proxy 0.4-27 Distance and Similarity Measures pryr 0.1.6 Tools for Computing on the Language ps 1.9.1 List, Query, Manipulate System Processes pscl 1.5.9 Political Science Computational Laboratory psych 2.5.3 Procedures for Psychological, Psychometric, and Personality Research Publish 2023.01.17 Format Output of Various Routines in a Suitable Way for Reports and Publication purrr 1.0.4 Functional Programming Tools pwalign 1.4.0 Perform pairwise sequence alignments pwr 1.3-0 Basic Functions for Power Analysis qap 0.1-2 Heuristics for the Quadratic Assignment Problem (QAP) qdapRegex 0.7.10 Regular Expression Removal, Extraction, and Replacement Tools qgraph 1.9.8 Graph Plotting Methods, Psychometric Data Visualization and Graphical Model Estimation qqconf 1.3.2 Creates Simultaneous Testing Bands for QQ-Plots qqman 0.1.9 Q-Q and Manhattan Plots for GWAS Data quadprog 1.5-8 Functions to Solve Quadratic Programming Problems quantreg 6.1 Quantile Regression quantsmooth 1.74.0 Quantile smoothing and genomic visualization of array data questionr 0.8.0 Functions to Make Surveys Processing Easier QuickJSR 1.7.0 Interface for the 'QuickJS' Lightweight 'JavaScript' Engine qvalue 2.40.0 Q-value estimation for false discovery rate control R.cache 0.16.0 Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations R.matlab 3.7.0 Read and Write MAT Files and Call MATLAB from Within R R.methodsS3 1.8.2 S3 Methods Simplified R.oo 1.27.0 R Object-Oriented Programming with or without References R.utils 2.13.0 Various Programming Utilities R2HTML 2.3.4 HTML Exportation for R Objects R2jags 0.8-9 Using R to Run 'JAGS' R2WinBUGS 2.1-22.1 Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' R6 2.6.1 Encapsulated Classes with Reference Semantics ragg 1.4.0 Graphic Devices Based on AGG randomForest 4.7-1.2 Breiman and Cutlers Random Forests for Classification and Regression randomForestSRC 3.3.3 Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC) ranger 0.17.0 A Fast Implementation of Random Forests RANN 2.6.2 Fast Nearest Neighbour Search (Wraps ANN Library) Using L2 Metric rapidjsonr 1.2.0 'Rapidjson' C++ Header Files rappdirs 0.3.3 Application Directories: Determine Where to Save Data, Caches, and Logs raster 3.6-32 Geographic Data Analysis and Modeling RBGL 1.84.0 An interface to the BOOST graph library rbibutils 2.3 Read 'Bibtex' Files and Convert Between Bibliography Formats rbokeh 0.5.2 R Interface for Bokeh RcisTarget 1.28.0 RcisTarget Identify transcription factor binding motifs enriched on a list of genes or genomic regions rcmdcheck 1.4.0 Run 'R CMD check' from 'R' and Capture Results RColorBrewer 1.1-3 ColorBrewer Palettes Rcpp 1.0.14 Seamless R and C++ Integration RcppAnnoy 0.0.22 'Rcpp' Bindings for 'Annoy', a Library for Approximate Nearest Neighbors RcppArmadillo 14.4.2-1 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library RcppEigen 0.3.4.0.2 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library RcppGSL 0.3.13 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices RcppHNSW 0.6.0 'Rcpp' Bindings for 'hnswlib', a Library for Approximate Nearest Neighbors RcppML 0.3.7 Rcpp Machine Learning Library RcppNumerical 0.6-0 'Rcpp' Integration for Numerical Computing Libraries RcppParallel 5.1.10 Parallel Programming Tools for 'Rcpp' RcppProgress 0.4.2 An Interruptible Progress Bar with OpenMP Support for C++ in R Packages RcppRoll 0.3.1 Efficient Rolling / Windowed Operations RcppTOML 0.2.3 'Rcpp' Bindings to Parser for \"Tom's Obvious Markup Language\" RcppZiggurat 0.1.8 'Rcpp' Integration of Different \"Ziggurat\" Normal RNG Implementations RCurl 1.98-1.17 General Network (HTTP/FTP/...) Client Interface for R RCy3 2.28.0 Functions to Access and Control Cytoscape Rdpack 2.6.4 Update and Manipulate Rd Documentation Objects reactable 0.4.4 Interactive Data Tables for R reactome.db 1.92.0 A set of annotation maps for reactome ReactomePA 1.52.0 Reactome Pathway Analysis reactR 0.6.1 React Helpers readbitmap 0.1.5 Simple Unified Interface to Read Bitmap Images (BMP,JPEG,PNG,TIFF) readr 2.1.5 Read Rectangular Text Data readxl 1.4.5 Read Excel Files recipes 1.3.0 Preprocessing and Feature Engineering Steps for Modeling reformulas 0.4.0 Machinery for Processing Random Effect Formulas registry 0.5-1 Infrastructure for R Package Registries relsurv 2.3-2 Relative Survival remaCor 0.0.18 Random Effects Meta-Analysis for Correlated Test Statistics rematch 2.0.0 Match Regular Expressions with a Nicer 'API' rematch2 2.1.2 Tidy Output from Regular Expression Matching remotes 2.5.0 R Package Installation from Remote Repositories, Including 'GitHub' rentrez 1.2.3 'Entrez' in R renv 1.1.4 Project Environments repr 1.1.7 Serializable Representations reprex 2.1.1 Prepare Reproducible Example Code via the Clipboard reshape 0.8.9 Flexibly Reshape Data reshape2 1.4.4 Flexibly Reshape Data: A Reboot of the Reshape Package ResidualMatrix 1.18.0 Creating a DelayedMatrix of Regression Residuals restfulr 0.0.15 R Interface to RESTful Web Services reticulate 1.42.0 Interface to 'Python' rex 1.2.1 Friendly Regular Expressions Rfast 2.1.5.1 A Collection of Efficient and Extremely Fast R Functions rgl 1.3.18 3D Visualization Using OpenGL Rgraphviz 2.52.0 Provides plotting capabilities for R graph objects rGREAT 2.10.0 GREAT Analysis - Functional Enrichment on Genomic Regions rhandsontable 0.3.8 Interface to the 'Handsontable.js' Library rhdf5 2.52.0 R Interface to HDF5 rhdf5filters 1.20.0 HDF5 Compression Filters Rhdf5lib 1.30.0 hdf5 library as an R package RhpcBLASctl 0.23-42 Control the Number of Threads on 'BLAS' Rhtslib 3.4.0 HTSlib high-throughput sequencing library as an R package RInside 0.2.19 C++ Classes to Embed R in C++ (and C) Applications rintrojs 0.3.4 Wrapper for the 'Intro.js' Library rio 1.2.3 A Swiss-Army Knife for Data I/O RISCA 1.0.7 Causal Inference and Prediction in Cohort-Based Analyses riskRegression 2023.12.21 Risk Regression Models and Prediction Scores for Survival Analysis with Competing Risks RITAN 1.32.0 Rapid Integration of Term Annotation and Network resources RITANdata 1.32.0 This package contains reference annotation and network data sets rjags 4-17 Bayesian Graphical Models using MCMC rJava 1.0-11 Low-Level R to Java Interface rjson 0.2.23 JSON for R RJSONIO 2.0.0 Serialize R Objects to JSON, JavaScript Object Notation rlang 1.1.6 Functions for Base Types and Core R and 'Tidyverse' Features rlist 0.4.6.2 A Toolbox for Non-Tabular Data Manipulation RMariaDB 1.3.4 Database Interface and MariaDB Driver rmarkdown 2.29 Dynamic Documents for R rmeta 3.0 Meta-Analysis Rmisc 1.5.1 Ryan Miscellaneous Rmpi 0.7-3.3 Interface (Wrapper) to MPI (Message-Passing Interface) rms 8.0-0 Regression Modeling Strategies RMySQL 0.11.1 Database Interface and 'MySQL' Driver for R Rnanoflann 0.0.3 Extremely Fast Nearest Neighbor Search rngtools 1.5.2 Utility Functions for Working with Random Number Generators RNifti 1.8.0 Fast R and C++ Access to NIfTI Images robustbase 0.99-4-1 Basic Robust Statistics rockchalk 1.8.157 Regression Estimation and Presentation ROCR 1.0-11 Visualizing the Performance of Scoring Classifiers rootSolve 1.8.2.4 Nonlinear Root Finding, Equilibrium and Steady-State Analysis of Ordinary Differential Equations ROpenCVLite 4.90.2 Helper Package for Installing OpenCV with R roxygen2 7.3.2 In-Line Documentation for R rpart 4.1.24 Recursive Partitioning and Regression Trees rpf 1.0.14 Response Probability Functions RPresto 1.4.7 DBI Connector to Presto rprojroot 2.0.4 Finding Files in Project Subdirectories rsample 1.3.0 General Resampling Infrastructure Rsamtools 2.24.0 Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import rsconnect 1.3.4 Deploy Docs, Apps, and APIs to 'Posit Connect', 'shinyapps.io', and 'RPubs' RSpectra 0.16-2 Solvers for Large-Scale Eigenvalue and SVD Problems RSQLite 2.3.9 SQLite Interface for R rstan 2.32.7 R Interface to Stan rstantools 2.4.0 Tools for Developing R Packages Interfacing with 'Stan' rstatix 0.7.2 Pipe-Friendly Framework for Basic Statistical Tests rstudioapi 0.17.1 Safely Access the RStudio API Rsubread 2.22.1 Mapping, quantification and variant analysis of sequencing data rsvd 1.0.5 Randomized Singular Value Decomposition rtracklayer 1.68.0 R interface to genome annotation files and the UCSC genome browser Rtsne 0.17 T-Distributed Stochastic Neighbor Embedding using a Barnes-Hut Implementation Rttf2pt1 1.3.12 'ttf2pt1' Program RUnit 0.4.33 R Unit Test Framework rvcheck 0.2.1 R/Package Version Check RVenn 1.1.0 Set Operations for Many Sets rversions 2.1.2 Query 'R' Versions, Including 'r-release' and 'r-oldrel' rvest 1.0.4 Easily Harvest (Scrape) Web Pages s2 1.1.7 Spherical Geometry Operators Using the S2 Geometry Library S4Arrays 1.8.0 Foundation of array-like containers in Bioconductor S4Vectors 0.46.0 Foundation of vector-like and list-like containers in Bioconductor safetensors 0.1.2 Safetensors File Format samr 3.0 SAM: Significance Analysis of Microarrays sandwich 3.1-1 Robust Covariance Matrix Estimators sass 0.4.10 Syntactically Awesome Style Sheets ('Sass') satellite 1.0.5 Handling and Manipulating Remote Sensing Data satuRn 1.16.0 Scalable Analysis of Differential Transcript Usage for Bulk and Single-Cell RNA-sequencing Applications ScaledMatrix 1.16.0 Creating a DelayedMatrix of Scaled and Centered Values scales 1.4.0 Scale Functions for Visualization scam 1.2-18 Shape Constrained Additive Models scater 1.36.0 Single-Cell Analysis Toolkit for Gene Expression Data in R scattermore 1.2 Scatterplots with More Points scatterpie 0.2.4 Scatter Pie Plot scatterplot3d 0.3-44 3D Scatter Plot SCENIC 1.3.1 SCENIC (Single Cell rEgulatory Network Inference and Clustering) SCopeLoomR 0.13.0 Build .loom files (compatible with SCope) and extract data from .loom files. SCPA 1.6.2 Single Cell Pathway Analysis sctransform 0.4.1 Variance Stabilizing Transformations for Single Cell UMI Data scuttle 1.18.0 Single-Cell RNA-Seq Analysis Utilities segmented 2.1-4 Regression Models with Break-Points / Change-Points Estimation (with Possibly Random Effects) selectr 0.4-2 Translate CSS Selectors to XPath Expressions sem 3.1-16 Structural Equation Models semPlot 1.1.6 Path Diagrams and Visual Analysis of Various SEM Packages' Output sendmailR 1.4-0 Send Email Using R seqinr 4.2-36 Biological Sequences Retrieval and Analysis seqLogo 1.74.0 Sequence logos for DNA sequence alignments seqminer 9.7 Efficiently Read Sequence Data (VCF Format, BCF Format, METAL Format and BGEN Format) into R seriation 1.5.7 Infrastructure for Ordering Objects Using Seriation servr 0.32 A Simple HTTP Server to Serve Static Files or Dynamic Documents sessioninfo 1.2.3 R Session Information Seurat 5.3.0 Tools for Single Cell Genomics SeuratData 0.2.2.9002 Install and Manage Seurat Datasets SeuratDisk 0.0.0.9021 Interfaces for HDF5-Based Single Cell File Formats SeuratObject 5.1.0 Data Structures for Single Cell Data SeuratWrappers 0.4.0 Community-Provided Methods and Extensions for the Seurat Object sf 1.0-20 Simple Features for R sfd 0.1.0 Space-Filling Design Library sfheaders 0.4.4 Converts Between R Objects and Simple Feature Objects sftime 0.3.0 Classes and Methods for Simple Feature Objects that Have a Time Column shadowtext 0.1.4 Shadow Text Grob and Layer shape 1.4.6.1 Functions for Plotting Graphical Shapes, Colors shiny 1.10.0 Web Application Framework for R shinyAce 0.4.4 Ace Editor Bindings for Shiny shinyBS 0.61.1 Twitter Bootstrap Components for Shiny shinycssloaders 1.1.0 Add Loading Animations to a 'shiny' Output While It's Recalculating shinydashboard 0.7.3 Create Dashboards with 'Shiny' shinyFiles 0.9.3 A Server-Side File System Viewer for Shiny shinyjs 2.1.0 Easily Improve the User Experience of Your Shiny Apps in Seconds shinystan 2.6.0 Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models shinythemes 1.2.0 Themes for Shiny shinyWidgets 0.9.0 Custom Inputs Widgets for Shiny ShortRead 1.66.0 FASTQ input and manipulation Signac 1.14.0 Analysis of Single-Cell Chromatin Data simdistr 1.0.1 Assessment of Data Trial Distributions According to the Carlisle-Stouffer Method SingleCellExperiment 1.30.0 S4 Classes for Single Cell Data SingleR 2.10.0 Reference-Based Single-Cell RNA-Seq Annotation singscore 1.28.0 Rank-based single-sample gene set scoring method sitmo 2.0.2 Parallel Pseudo Random Number Generator (PPRNG) 'sitmo' Header Files SKAT 2.2.5 SNP-Set (Sequence) Kernel Association Test slam 0.1-55 Sparse Lightweight Arrays and Matrices slider 0.3.2 Sliding Window Functions smacof 2.1-7 Multidimensional Scaling smoof 1.6.0.3 Single and Multi-Objective Optimization Test Functions sn 2.1.1 The Skew-Normal and Related Distributions Such as the Skew-t and the SUN sna 2.8 Tools for Social Network Analysis snakecase 0.11.1 Convert Strings into any Case snow 0.4-4 Simple Network of Workstations SNPRelate 1.42.0 Parallel Computing Toolset for Relatedness and Principal Component Analysis of SNP Data soGGi 1.40.0 Visualise ChIP-seq, MNase-seq and motif occurrence as aggregate plots Summarised Over Grouped Genomic Intervals sourcetools 0.1.7-1 Tools for Reading, Tokenizing and Parsing R Code sp 2.2-0 Classes and Methods for Spatial Data spacesXYZ 1.5-1 CIE XYZ and some of Its Derived Color Spaces spacetime 1.3-3 Classes and Methods for Spatio-Temporal Data spam 2.11-1 SPArse Matrix SparseArray 1.8.0 High-performance sparse data representation and manipulation in R SparseM 1.84-2 Sparse Linear Algebra sparseMatrixStats 1.20.0 Summary Statistics for Rows and Columns of Sparse Matrices sparsesvd 0.2-2 Sparse Truncated Singular Value Decomposition (from 'SVDLIBC') sparsevctrs 0.3.3 Sparse Vectors for Use in Data Frames SPAtest 3.1.2 Score Test and Meta-Analysis Based on Saddlepoint Approximation spatial 7.3-18 Functions for Kriging and Point Pattern Analysis SpatialExperiment 1.18.0 S4 Class for Spatially Resolved -omics Data spatstat 3.3-2 Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests spatstat.data 3.1-6 Datasets for 'spatstat' Family spatstat.explore 3.4-2 Exploratory Data Analysis for the 'spatstat' Family spatstat.geom 3.3-6 Geometrical Functionality of the 'spatstat' Family spatstat.linnet 3.2-5 Linear Networks Functionality of the 'spatstat' Family spatstat.model 3.3-5 Parametric Statistical Modelling and Inference for the 'spatstat' Family spatstat.random 3.3-3 Random Generation Functionality for the 'spatstat' Family spatstat.sparse 3.1-0 Sparse Three-Dimensional Arrays and Linear Algebra Utilities spatstat.univar 3.1-2 One-Dimensional Probability Distribution Support for the 'spatstat' Family spatstat.utils 3.1-3 Utility Functions for 'spatstat' spData 2.3.4 Datasets for Spatial Analysis spdep 1.3-11 Spatial Dependence: Weighting Schemes, Statistics speedglm 0.3-5 Fitting Linear and Generalized Linear Models to Large Data Sets SpliceWiz 1.10.0 interactive analysis and visualization of alternative splicing in R splines 4.5.0 Regression Spline Functions and Classes splitstackshape 1.4.8 Stack and Reshape Datasets After Splitting Concatenated Values splus2R 1.3-5 Supplemental S-PLUS Functionality in R sqldf 0.4-11 Manipulate R Data Frames Using SQL SQUAREM 2021.1 Squared Extrapolation Methods for Accelerating EM-Like Monotone Algorithms squash 1.0.9 Color-Based Plots for Multivariate Visualization StanHeaders 2.32.10 C++ Header Files for Stan stapler 0.8.0 Simultaneous Truth and Performance Level Estimation stars 0.6-8 Spatiotemporal Arrays, Raster and Vector Data Cubes statmod 1.5.0 Statistical Modeling statnet.common 4.11.0 Common R Scripts and Utilities Used by the Statnet Project Software stats 4.5.0 The R Stats Package stats4 4.5.0 Statistical Functions using S4 Classes STRINGdb 2.20.0 STRINGdb - Protein-Protein Interaction Networks and Functional Enrichment Analysis stringdist 0.9.15 Approximate String Matching, Fuzzy Text Search, and String Distance Functions stringi 1.8.7 Fast and Portable Character String Processing Facilities stringr 1.5.1 Simple, Consistent Wrappers for Common String Operations StructuralVariantAnnotation 1.24.0 Variant annotations for structural variants styler 1.10.3 Non-Invasive Pretty Printing of R Code SummarizedExperiment 1.38.0 A container (S4 class) for matrix-like assays SuperLearner 2.0-29 Super Learner Prediction survey 4.4-2 Analysis of Complex Survey Samples survival 3.8-3 Survival Analysis survminer 0.5.0 Drawing Survival Curves using 'ggplot2' survMisc 0.5.6 Miscellaneous Functions for Survival Data sva 3.56.0 Surrogate Variable Analysis svglite 2.1.3 An 'SVG' Graphics Device svUnit 1.0.6 'SciViews' - Unit, Integration and System Testing sys 3.4.3 Powerful and Reliable Tools for Running System Commands in R systemfonts 1.2.2 System Native Font Finding systemPipeR 2.14.0 systemPipeR: Workflow Environment for Data Analysis and Report Generation tcltk 4.5.0 Tcl/Tk Interface tensor 1.5 Tensor product of arrays tensorA 0.36.2.1 Advanced Tensor Arithmetic with Named Indices terra 1.8-42 Spatial Data Analysis testthat 3.2.3 Unit Testing for R textshaping 1.0.0 Bindings to the 'HarfBuzz' and 'Fribidi' Libraries for Text Shaping TFBSTools 1.46.0 Software Package for Transcription Factor Binding Site (TFBS) Analysis TFisher 0.2.0 Optimal Thresholding Fisher's P-Value Combination Method TFMPvalue 0.0.9 Efficient and Accurate P-Value Computation for Position Weight Matrices TH.data 1.1-3 TH's Data Archive threejs 0.3.4 Interactive 3D Scatter Plots, Networks and Globes tibble 3.2.1 Simple Data Frames tidybayes 3.0.7 Tidy Data and 'Geoms' for Bayesian Models tidycmprsk 1.1.0 Competing Risks Estimation tidygraph 1.3.1 A Tidy API for Graph Manipulation tidymodels 1.3.0 Easily Install and Load the 'Tidymodels' Packages tidyr 1.3.1 Tidy Messy Data tidyselect 1.2.1 Select from a Set of Strings tidytree 0.4.6 A Tidy Tool for Phylogenetic Tree Data Manipulation tidyverse 2.0.0 Easily Install and Load the 'Tidyverse' tiff 0.1-12 Read and Write TIFF Images timechange 0.3.0 Efficient Manipulation of Date-Times timeDate 4041.110 Rmetrics - Chronological and Calendar Objects timereg 2.0.6 Flexible Regression Models for Survival Data tinytex 0.57 Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents tippy 0.1.0 Add Tooltips to 'R markdown' Documents or 'Shiny' Apps tmap 4.0 Thematic Maps tmaptools 3.2 Thematic Map Tools tmvnsim 1.0-2 Truncated Multivariate Normal Simulation tools 4.5.0 Tools for Package Development topGO 2.59.0 Enrichment Analysis for Gene Ontology torch 0.14.2 Tensors and Neural Networks with 'GPU' Acceleration treeio 1.32.0 Base Classes and Functions for Phylogenetic Tree Input and Output triebeard 0.4.1 'Radix' Trees in 'Rcpp' truncnorm 1.0-9 Truncated Normal Distribution TSP 1.2-4 Traveling Salesperson Problem (TSP) tune 1.3.0 Tidy Tuning Tools twang 2.6.1 Toolkit for Weighting and Analysis of Nonequivalent Groups tweenr 2.0.3 Interpolate Data for Smooth Animations TxDb.Hsapiens.UCSC.hg19.knownGene 3.2.2 Annotation package for TxDb object(s) TxDb.Hsapiens.UCSC.hg38.knownGene 3.21.0 Annotation package for TxDb object(s) TxDb.Mmusculus.UCSC.mm10.knownGene 3.10.0 Annotation package for TxDb object(s) TxDb.Mmusculus.UCSC.mm9.knownGene 3.2.2 Annotation package for TxDb object(s) txdbmaker 1.4.0 Tools for making TxDb objects from genomic annotations tximeta 1.26.0 Transcript Quantification Import with Automatic Metadata tximport 1.36.0 Import and summarize transcript-level estimates for transcript- and gene-level analysis tzdb 0.5.0 Time Zone Database Information ucminf 1.2.2 General-Purpose Unconstrained Non-Linear Optimization UCSC.utils 1.4.0 Low-level utilities to retrieve data from the UCSC Genome Browser UCSCXenaTools 1.4.8 Download and Explore Datasets from UCSC Xena Data Hubs umap 0.2.10.0 Uniform Manifold Approximation and Projection UniprotR 2.4.0 Retrieving Information of Proteins from Uniprot units 0.8-7 Measurement Units for R Vectors UpSetR 1.4.0 A More Scalable Alternative to Venn and Euler Diagrams for Visualizing Intersecting Sets urlchecker 1.0.1 Run CRAN URL Checks from Older R Versions urltools 1.7.3 Vectorised Tools for URL Handling and Parsing useful 1.2.6.1 A Collection of Handy, Useful Functions usethis 3.1.0 Automate Package and Project Setup utf8 1.2.4 Unicode Text Processing utils 4.5.0 The R Utils Package uuid 1.2-1 Tools for Generating and Handling of UUIDs uwot 0.2.3 The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction V8 6.0.3 Embedded JavaScript and WebAssembly Engine for R variables 1.1-1 Variable Descriptions variancePartition 1.38.0 Quantify and interpret drivers of variation in multilevel gene expression experiments VariantAnnotation 1.54.0 Annotation of Genetic Variants vcd 1.4-13 Visualizing Categorical Data vcfR 1.15.0 Manipulate and Visualize VCF Data vctrs 0.6.5 Vector Helpers vegan 2.6-10 Community Ecology Package velocyto.R 0.6 RNA velocity estimation in R venn 1.12 Draw Venn Diagrams VennDiagram 1.7.3 Generate High-Resolution Venn and Euler Plots VGAM 1.1-13 Vector Generalized Linear and Additive Models VIM 6.2.2 Visualization and Imputation of Missing Values vipor 0.4.7 Plot Categorical Data Using Quasirandom Noise and Density Estimates viridis 0.6.5 Colorblind-Friendly Color Maps for R viridisLite 0.4.2 Colorblind-Friendly Color Maps (Lite Version) visNetwork 2.1.2 Network Visualization using 'vis.js' Library vroom 1.6.5 Read and Write Rectangular Text Data Quickly vsn 3.76.0 Variance stabilization and calibration for microarray data waiter 0.2.5 Loading Screen for 'Shiny' waldo 0.6.1 Find Differences Between R Objects warp 0.2.1 Group Dates webshot 0.5.5 Take Screenshots of Web Pages weights 1.0.4 Weighting and Weighted Statistics WGCNA 1.73 Weighted Correlation Network Analysis whisker 0.4.1 {{mustache}} for R, Logicless Templating WhiteStripe 2.4.3 White Matter Normalization for Magnetic Resonance Images widgetframe 0.3.1 'Htmlwidgets' in Responsive 'iframes' withr 3.0.2 Run Code 'With' Temporarily Modified Global State wk 0.9.4 Lightweight Well-Known Geometry Parsing wordcloud 2.6 Word Clouds workflows 1.2.0 Modeling Workflows workflowsets 1.1.0 Create a Collection of 'tidymodels' Workflows writexl 1.5.4 Export Data Frames to Excel 'xlsx' Format xfun 0.52 Supporting Functions for Packages Maintained by 'Yihui Xie' xgboost 1.7.10.1 Extreme Gradient Boosting xlsx 0.6.5 Read, Write, Format Excel 2007 and Excel 97/2000/XP/2003 Files xlsxjars 0.6.1 Package required POI jars for the xlsx package XML 3.99-0.18 Tools for Parsing and Generating XML Within R and S-Plus xml2 1.3.8 Parse XML xopen 1.0.1 Open System Files, 'URLs', Anything xtable 1.8-4 Export Tables to LaTeX or HTML xts 0.14.1 eXtensible Time Series XVector 0.48.0 Foundation of external vector representation and manipulation in Bioconductor yaml 2.3.10 Methods to Convert R Data to YAML and Back yardstick 1.3.2 Tidy Characterizations of Model Performance yulab.utils 0.2.0 Supporting Functions for Packages Maintained by 'YuLab-SMU' zellkonverter 1.18.0 Conversion Between scRNA-seq Objects zigg 0.0.2 Lightweight Interfaces to the 'Ziggurat' Pseudo Random Number Generator zip 2.3.2 Cross-Platform 'zip' Compression zlibbioc 1.54.0 An R packaged zlib-1.2.5 zoo 1.8-14 S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)"},{"location":"software/schrodinger/","title":"Schr\u00f6dinger","text":""},{"location":"software/schrodinger/#schrodinger","title":"Schr\u00f6dinger","text":"<p>Schr\u00f6dinger is licensed by the Department of Biochemistry and available to all MCW investigators.</p> <p>Users interested in contributing funds or discussing Schr\u00f6dinger licensing at MCW should contact Dawn Wenzel and Brian Smith.</p> <p>The Schr\u00f6dinger Small-Molecule Drug Discovery Suite includes a GUI client that you can run on your Windows, Mac, or Linux desktop/laptop. This client can be configured to send jobs to the HPC Cluster. Example use includes molecular modeling, docking, molecular dynamics simulation, etc. See below for installation.</p>"},{"location":"software/schrodinger/#requirements","title":"Requirements","text":"<ul> <li>RCC user account</li> </ul>"},{"location":"software/schrodinger/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>Installation Changes</p> <p>Please note, starting with version 2025-3, the <code>schrodinger.hosts</code> file is no longer used to connect to the cluster. This has been replaced by the Schr\u00f6dinger Job Server. The steps below have been updated to reflect these changes. Please plan to update your Schrodinger to 2025-3.</p>"},{"location":"software/schrodinger/#download-and-install","title":"Download and Install","text":"<p>Download the 2025-3 software from https://www.schrodinger.com and run the installer.</p>"},{"location":"software/schrodinger/#configure-licensing","title":"Configure licensing","text":"WindowsMac <p>Open the Configure Software tool and select Server Identifiers. Enter the license server hostname and port number found in the configuration info and select Install License.</p> <p>Open the Configuration tool and select Server Identifiers. Enter the license server hostname and port number found in the configuration info and select Install License.</p>"},{"location":"software/schrodinger/#configure-cluster-connection","title":"Configure Cluster Connection","text":"WindowsMac <p>Open the <code>Remote Login Configuration</code> tool. Select <code>Generate Keys</code>. Select <code>Initialize Host Access</code>. Enter the host IP Address (this is in your schrodinger.hosts file) and your MCW username. Select <code>Initialize</code>.</p> <p>Configure password-less SSH for remote login. Open a terminal and follow this guide.</p>"},{"location":"software/schrodinger/#register-job-server","title":"Register Job Server","text":"<p>Launch the Maestro application. You should be prompted with a <code>Register with Job Server</code> window. If not, select <code>Help &gt; Register with Job Servers...</code> to open that window. Select <code>New server</code> and enter the job server hostname and port number found in the configuration info. Enter your MCW username and password, and select <code>Register</code>. If the server is registered successfully, you can restart Maestro to complete the setup.</p>"},{"location":"software/schrodinger/#upgrading","title":"Upgrading","text":"<p>To upgrade your Schr\u00f6dinger installation, first uninstall the current version. The uninstall process should leave your configuration intact. Then install the upgraded version of Schr\u00f6dinger and remember to update your schrodinger.hosts file as it appears above. If you have issues upgrading, please contact help-rcc@mcw.edu.</p>"},{"location":"software/schrodinger/#remote-job-submission","title":"Remote Job Submission","text":"Included in your install are configurations for the remote server connection to HPC Cluster. This allows you to send large or computationally intensive workloads to the cluster from your desktop. The cluster will run the job and return the results. <p>server_cpu - HPC Cluster connection for Schr\u00f6dinger CPU jobs</p> <p>server_gpu - HPC Cluster connection for Schr\u00f6dinger GPU jobs</p> <p>When running Schr\u00f6dinger jobs, please follow these guidelines:</p> <ul> <li> <p>Run all pre-processing jobs on the localhost (your machine).</p> </li> <li> <p>Run docking, MD, etc. jobs with remote connection to HPC Cluster.</p> </li> </ul>"},{"location":"software/schrodinger/#direct-job-submission","title":"Direct Job Submission","text":"<p>You can also submit jobs directly from the cluster command line. This is helpful for submitting large numbers of jobs, where using the desktop launch interface might be repetitive. All direct jobs are launched from a cluster login node using Schr\u00f6dinger to submit the job to the SLURM job scheduler. Please follow this syntax using the <code>server_cpu</code> or <code>server_gpu</code> for <code>-HOST</code> as appropriate.</p> <pre><code># print the command-line options for Desmond (molecular dynamics package)\n/hpc/apps/schrodinger/2025-3/desmond -h\n\n# submit a desmond job to the SLURM job scheduler\n/hpc/apps/schrodinger/2025-3/desmond -HOST server_gpu -c x.cfg -in x.cms\n</code></pre> <p>Mistakes to avoid</p> <ul> <li>Do not run the Schr\u00f6dinger apps in a job script. Schr\u00f6dinger will submit the job to SLURM for you.</li> <li>Do not use <code>localhost</code> as the <code>-HOST</code> setting. This will cause the job to run on the login node and/or bypass the license checking. Either case will cause your job and other jobs to fail.</li> </ul>"},{"location":"software/schrodinger/#schrodinger-utility-scripts","title":"Schr\u00f6dinger Utility Scripts","text":"<p>Schr\u00f6dinger includes additional utility scripts that are not included in the Maestro interface and must be run directly within a job script. See Schr\u00f6dinger scripts for a complete list and details.</p> <p>Why are utility scripts submitted via job script?</p> <p>These utility scripts do not have the advanced job submission options that the Schr\u00f6dinger apps (Desmond, Epik, etc.) include. Therefore, we have to write a separate SLURM job script for submission.</p> <p>To see the options for a particular script on the cluster:</p> <pre><code># script has no requirements\n/hpc/apps/schrodinger/2025-3/run entropy_calc.py -h\n</code></pre> <p>Some scripts are dependent on a specific Schr\u00f6dinger app. Check the Schr\u00f6dinger scripts page Requires column.</p> <pre><code># run a script requiring the Desmond app\n/hpc/apps/schrodinger/2025-3/run -FROM desmond trj_center.py -h\n</code></pre> <p>To run the script in a job on the cluster, adapt the following job submission script to your specific command:</p> schrod.slurm <pre><code>#!/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n#SBATCH --output=%x-%j.out\n\n/hpc/apps/schrodinger/2025-3/run script.py\n</code></pre>"},{"location":"software/schrodinger/#troubleshooting","title":"Troubleshooting","text":"<p>Schr\u00f6dinger can be sensitive to changes in networking. This is often an issue for laptop users. You may see errors such as:</p> <ul> <li>Launch failed: no JobId found.</li> <li>WARNING: You did not specify for -maxjob. Remember its default value is 1.</li> </ul> <p>In this case, you should reset your connection to the Schr\u00f6dinger server using the following steps.</p> <p>Mac OS X only</p> <p>This solution works on Mac OS X and assumes that you have installed Schr\u00f6dinger 2025-3. Modify the version number if you have an older version installed.</p> <p>On your laptop/desktop:</p> <ul> <li>Shutdown Schr\u00f6dinger.</li> <li>Open terminal and run the following commands:</li> </ul> <pre><code>/opt/schrodinger/suites2025-3/utilities/jserver -shutdown\n/opt/schrodinger/suites2025-3/utilities/jserver -cleanall\n/opt/schrodinger/suites2025-3/utilities/jserver -proxy -shutdown\n/opt/schrodinger/suites2025-3/utilities/jserver -proxy -cleanall\n</code></pre> <p>On the cluster:</p> <ul> <li>Run the following commands:</li> </ul> <pre><code>/hpc/apps/schrodinger/2025-3/utilities/jserver -shutdown\n/hpc/apps/schrodinger/2025-3/utilities/jserver -cleanall\n/hpc/apps/schrodinger/2025-3/utilities/jserver -proxy -shutdown\n/hpc/apps/schrodinger/2025-3/utilities/jserver -proxy -cleanall\n</code></pre>"},{"location":"software/schrodinger/#help","title":"Help","text":"<p>Having issues installing? Contact help-rcc@mcw.edu to schedule a support session.</p> <p>For general Schr\u00f6dinger questions, see https://www.schrodinger.com/kb.</p> <p>For training opportunities, see https://www.schrodinger.com/seminars/current and https://www.schrodinger.com/training.</p>"},{"location":"software/tensorflow/","title":"TensorFlow","text":""},{"location":"software/tensorflow/#tensorflow","title":"TensorFlow","text":"<p>TensorFlow on HPC can be run in batch, interactive, or Jupyter Notebook.</p>"},{"location":"software/tensorflow/#tensorflow-interactive-job","title":"TensorFlow interactive job","text":"<p>Start the job.</p> <pre><code>srun --job-name=tensorflow --ntasks=1 --time=1:00:00 --gres=gpu:1 --pty bash\n</code></pre> <p>Load the TensorFlow module.</p> <pre><code>module load tensorflow\n</code></pre> <p>Start your training or other commands.</p> <pre><code>python train.py options input output\n</code></pre> <p>When your commands end, always remember to end the interactive job.</p> <pre><code>exit\n</code></pre>"},{"location":"software/tensorflow/#tensorflow-batch-job","title":"TensorFlow batch job","text":"<p>This is an example of running a Python script in a SLURM batch job.</p> learning-ml.pylearning-ml.slurm <pre><code>import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsession = tf.Session()\ntf.print(hello)\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=learning\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00\n#SBATCH --account={PI_NetID}\n#SBATCH --partition=gpu\n#SBATCH --gres=gpu:1\n#SBATCH --output=%x-%j.out\n\nmodule load tensorflow\npython /scratch/g/pi_netid/learning-ml.py &gt;&gt; output.log  \n</code></pre> <p>Save these scripts to your scratch directory, and submit the job:</p> <pre><code>sbatch learning-ml.slurm\n</code></pre> <p>This simple job should print Hello, TensorFlow! to your output file.</p>"},{"location":"software/tensorflow/#tensorflow-jupyter-notebook-job","title":"TensorFlow Jupyter Notebook job","text":"<p>This functionality is provided by Open OnDemand!</p> <p>See Jupyter on Open OnDemand for details.</p>"},{"location":"software/tensorflow/#tensorboard-job","title":"TensorBoard Job","text":"<p>This functionality is provided by Open OnDemand!</p>"},{"location":"storage/data-access/","title":"Mounting Drives","text":""},{"location":"storage/data-access/#mounting-drives","title":"Mounting Drives","text":"<p>SMB access is not enabled by default.</p> <p>Before mounting an SMB drive, you must request SMB access to your designated storage space by contacting help-rcc@mcw.edu. Please note that if you are granted SMB access to a specific folder, you will no longer be able to access that folder through SSH or OnDemand due to differences in file paths and permissions between the cluster's Linux environment and SMB protocols.</p>"},{"location":"storage/data-access/#mount-a-smb-drive","title":"Mount a SMB Drive","text":""},{"location":"storage/data-access/#windows","title":"Windows","text":"<p>Microsoft provides a helpful guide. In step #4, please make sure to type in your drive path, which will be in the form <code>\\\\qfs2.rcc.mcw.edu\\pi_netid</code>. Do not select Browse. Use your MCW username and password when prompted. If you are not on an MCW-owned or managed machine, please make sure to preface your MCW username with <code>mcwcorp\\</code>.</p> <p>If you are having trouble remounting your drive, please first right-click on the drive and select disconnect. If the drive disconnects successfully, then try to remount, but make sure to select Connect using different credentials. Please contact help-rcc@mcw.edu if the issue persists.</p>"},{"location":"storage/data-access/#macos","title":"MacOS","text":"<p>In the Finder menu, select <code>Go</code> and <code>Connect to Server</code>. In the Server Address box, enter your drive path, which will be in the form <code>smb://qfs2.rcc.mcw.edu/pi_netid</code>. You will be prompted for your MCW username and password when you connect. Make sure to select <code>Registered User</code> and preface your MCW username with <code>mcwcorp\\</code>.</p>"},{"location":"storage/data-sharing/","title":"Sharing Data","text":""},{"location":"storage/data-sharing/#sharing-data","title":"Sharing Data","text":"<p>Your lab group storage can be used to collaborate with non-lab members. This is done with by-request project directories. For example, if lab <code>pi</code> would like to collaborate with some data for a project called <code>zephyr</code>, then the lab PI would contact RCC to create a project directory.</p> <pre><code>$ ls -l /group/pi\ntotal 8\ndrwxrws---. 3 root sg-pi-zephyr 512 Mar 26 13:52 zephyr\ndrwxrws---. 4 root sg-pi       1024 May 19 14:04 work\n</code></pre> <p>In addition to the usual <code>work</code> directory, there is now a <code>zephyr</code> project directory, which is controlled by the <code>sg-pi-zephyr</code> security group. Project data would go in that directory, and any number of collaborators (MCW researchers) can be added to the security group.</p>"},{"location":"storage/file-cleanup/","title":"File Cleanup &amp; Archiving","text":""},{"location":"storage/file-cleanup/#file-cleanup-archiving","title":"File Cleanup &amp; Archiving","text":"<p>There are several tools and commands available to help you manage your data, including finding large directories that you can compress down or delete if no longer needed.  Keeping your directories free of old or unwanted files will help keep your account under your disk usage quota.</p>"},{"location":"storage/file-cleanup/#check-storage-quota-limits","title":"Check Storage Quota Limits","text":"<p>You can easily find your available storage directories and current utilization on the clusters with the <code>mydisks</code> command.</p> <pre><code>$ mydisks\n=====My Lab=====\nSize  Used Avail Use% File\n47G   29G   19G  61% /home/user\n932G  158G  774G  17% /group/pi\n4.6T     0  4.6T   0% /scratch/g/pi \n</code></pre>"},{"location":"storage/file-cleanup/#finding-large-files-and-directories","title":"Finding Large Files and Directories","text":"<p>Several tools exist to help identify file size and type. Here we discuss use of <code>du</code>.</p> <p>To list the top 20 files/directories, sorted by size:</p> <pre><code>du -ah . | sort -n -r | head -n 20\n</code></pre> <p>Another variation of the du command to show directories and their sizes:</p> <pre><code>du -h --max-depth=1\n</code></pre>"},{"location":"storage/file-cleanup/#compressing-and-archiving-directories","title":"Compressing And Archiving Directories","text":"<p>So now you have identified some folders that you don't immediately need.  You can compress and archive those directories or files using <code>tar</code> and <code>gzip</code>.</p> <p>To recursively compress every file and directory inside the path you specify:</p> <pre><code>tar -czvf name-of-archive.tar.gz /path/to/directory-or-file\n</code></pre> <p>The <code>tar</code> command has many switches. We used the most common set in the previous command:</p> <pre><code>-c Create an archive.  \n-z Compress the archive with gzip.  \n-v Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode.  \n-f Allows you to specify the filename of the archive.\n</code></pre> <p>If you have a directory called myproject in your current home directory that you want to compress and archive, you can run the following.</p> <pre><code>tar -czvf myproject.tar.gz myproject\n</code></pre> <p>You can also use the tar command to check the contents of your archive. The following will print a list of directories and files in the archive.</p> <pre><code>tar -tf myproject.tar.gz\n</code></pre> <p>Once you are satisfied with your archive, you can then delete the original directory using <code>rm</code>.</p> <pre><code>rm -rf myproject\n</code></pre> <p>USE CAUTION!</p> <p>Most deletions in Linux are permanent.</p> <p>If sometime down the road, you need to access the content of directory myproject again, you can extract the archive by running this command.</p> <pre><code>tar -xzvf myproject.tar.gz\n</code></pre> <p>The <code>-x</code> flags tells tar to extract.  Once the command completes, you will now have the folder myproject available in your current directory.</p>"},{"location":"storage/file-cleanup/#creating-a-file-manifest","title":"Creating a file manifest","text":"<p>When archiving a dataset, it is often helpful to have a full manifest, or list, of the original file hierarchy. A file manifest is also a key piece of metadata.</p> <p>To list files in an .tar.gz achive:</p> <pre><code>tar -tf myarchive.tar.gz\n</code></pre> <p>To create a manifest file of the listing, run the same command, but pipe it to a text file:</p> <pre><code>tar -tf myarchive.tar.gz &gt; myarchive.manifest\n</code></pre>"},{"location":"storage/file-cleanup/#managing-archive-file-size","title":"Managing archive file size","text":"<p>Some file archives can be quite large. If you're uploading your archive to a repository, there may be a max single file upload size. In this case it is useful to split the archive file into smaller chunks.</p> <p>To split a <code>.tar.gz</code> archive into smaller chunksize:</p> <pre><code>split -b 500M myarchive.tar.gz \"myarchive.tar.gz.part\"\n</code></pre> <p>Now you should have a set of 500M files.</p> <p>To join the file chunks and recreate the full archive:</p> <pre><code>cat myarchive.tar.gz.parta* &gt; myarchive.tar.gz.joined\n</code></pre>"},{"location":"storage/file-permissions/","title":"File Permissions","text":""},{"location":"storage/file-permissions/#file-permissions","title":"File Permissions","text":"<p>Proceed with caution</p> <p>When modifying file permissions in Linux, use caution and take time to fully understand the effect of your proposed permission changes. Errant permission changes can make a file unusable by yourself, or others.</p> <p>Users may want to modify file/directory permissions to enable sharing. This should only be done in spaces that are meant for sharing. Each lab has shared group and scratch directories with inherited group permissions. Never modify permissions to open your home directory for sharing.</p> <p>If you need group write permission for a file (not recommended in Linux):</p> <pre><code>chmod g+w /path/to/file\n</code></pre> <p>Here we add the <code>w</code> write permission, for <code>g</code> group.</p> <p>Why is group write not recommended?</p> <p>Linux does not include group write permission by default, even in your shared lab spaces. This protects users. For instance, if multiple users attempt to edit the same file, collisions may occur and there is a high risk the file would be corrupted and information lost. Again, group write permissions are not recommended. It is much better for each user to make their own copy of a file.</p> <p>If you'd like to add write permissions on a directory and its files, add the <code>-R</code> recursive flag:</p> <pre><code>chmod -R g+w /group/PI_NetID/work/path/to/directory\n</code></pre> <p>You can check permissions before and after with command <code>ls -l /group/PI_NetID/work/path/to/file</code>.</p>"},{"location":"storage/file-permissions/#advanced","title":"Advanced","text":"<p>Additional utilities are available to help you manage your file permissions. Here we'll highlight the <code>find</code> utility for locating and managing file objects.</p> <p>To add group write permissions on all files in the current directory:</p> <pre><code>find . -type f -exec chmod g+w {} \\;\n</code></pre> <p>To add group write permissions on all directories in the current directory:</p> <pre><code>find . -type d -exec chmod g+w {} \\;\n</code></pre>"},{"location":"storage/file-recovery/","title":"File Recovery","text":""},{"location":"storage/file-recovery/#file-recovery","title":"File Recovery","text":"<p>This guide explains how you might recover a file using a data protection feature called a snapshot. Snapshots are available on your home and group directories. For reference, a snapshot is an on-disk reference point of file changes from a time period. It is very similar to file versions in Microsoft products. This feature allows you to look back at previous versions of your files, and recover when needed. Below you'll find info on how to recover a file, when possible.</p> <p>This procedure will not work in every case!</p> <p>Sometimes a file is created and deleted before a snapshot can be completed. For example, home directories are snapshotted once daily. If you cannot locate your file with the following procedures, contact help-rcc@mcw.edu.</p>"},{"location":"storage/file-recovery/#linux-all-clusters","title":"Linux (All Clusters)","text":"<p>To recover files in a directory, access the <code>.snapshot</code> directory. Snapshot directories are numbered with the largest number being the latest snapshot. However, if you would like to see the time points, use command <code>ls -l</code>.</p> <pre><code>ls -l .snapshot/\n# snapshots listed with timestamp\ndrwx------. 92 user sg-pi 73728 May 20 12:00 543_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 21 12:00 545_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 22 12:00 547_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 23 12:00 549_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 24 12:00 551_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 25 12:00 553_Home Directory_homefs\ndrwx------. 92 user sg-pi 73728 May 26 12:00 555_Home Directory_homefs\n</code></pre> <p>Once you select a snapshot directory, you can navigate to your files. Then copy back the file you need.</p> <pre><code>cp /home/user/.snapshot/555_Home Directory_homefs/file1 /home/user\n</code></pre> <p>Commands must reference <code>.snapshot</code> directly.</p> <p>The <code>.snapshot/</code> directory is hidden from standard Linux tools. Consider the following example:</p> <pre><code>ls -la /home/user # will not show .snapshot/\nls -la /home/user/.snapshot/ # displays the contents of .snapshot/\n</code></pre>"},{"location":"storage/file-recovery/#windows","title":"Windows","text":"<p>To recover a file in your Windows share:</p> <ol> <li>Open the folder that previously contained your lost file.</li> <li>Right-click in the window and select Properties.</li> <li>Select the Previous Versions tab.</li> <li>Select the version you would like to explore.</li> <li>In the new window, locate the file you want to recover.</li> <li>Copy the file or folder to the previous location.</li> </ol>"},{"location":"storage/file-recovery/#help","title":"Help","text":"<p>Remember, snapshots are timestamped. Look for the timestamped version that may contain your file. For instance, if you deleted a file Tuesday, then try a timestamped version from Monday. Contact help-rcc@mcw.edu with questions.</p>"},{"location":"storage/file-transfer/","title":"File Transfer","text":""},{"location":"storage/file-transfer/#file-transfer","title":"File Transfer","text":"<p>Several methods are available for transferring data to/from the HPC Cluster. These include Open OnDemand, command-line, and desktop client software. RCC recommends Open OnDemand for all users, especially for remote work.</p>"},{"location":"storage/file-transfer/#open-ondemand","title":"Open OnDemand","text":"<p>Open OnDemand is a web portal for using HPC and includes a file management app. For more information, see Open OnDemand Files App.</p>"},{"location":"storage/file-transfer/#command-line","title":"Command-line","text":"<p>Several command-line options are available for secure data transfer.</p>"},{"location":"storage/file-transfer/#scp","title":"scp","text":"<p>Secure copy (scp) is a tool for secure data transfer between UNIX-like systems using your MCW username and password. Available on Linux and Mac OS X. All commands should be run from the command-line in a terminal app on your computer.</p> <p>Copy a file to the HPC Cluster:</p> <pre><code>scp local_file user@login-hpc.rcc.mcw.edu:/path/to/remote/target-directory\n</code></pre> <p>Copy a directory to the HPC Cluster:**</p> <pre><code>scp -r local_directory user@login-hpc.rcc.mcw.edu:/path/to/remote/target-directory\n</code></pre> <p>Copy a file from the HPC Cluster:</p> <pre><code>scp user@login-hpc.rcc.mcw.edu:/path/to/remote_file /path/to/local/target-directory\n</code></pre> <p>Copy a directory from the HPC Cluster:</p> <pre><code>scp -r user@login-hpc.rcc.mcw.edu:/path/to/remote_directory /path/to/local/target-directory\n</code></pre>"},{"location":"storage/file-transfer/#rsync","title":"rsync","text":"<p>Remote sync (rsync) is a fast and secure data transfer tool. Available on Linux and Mac OS X. All commands should be run from the command-line in a terminal app on your computer.</p> <p>Copy a file to the HPC Cluster:</p> <pre><code>rsync -avz local_file user@login-hpc.rcc.mcw.edu:/path/to/target-directory\n</code></pre> <p>Copy a directory to the HPC Cluster:</p> <pre><code>rsync -avz local_directory user@login-hpc.rcc.mcw.edu:/path/to/target-directory\n</code></pre> <p>Copy a file from the HPC Cluster:</p> <pre><code>rsync -avz  user@login-hpc.rcc.mcw.edu:/path/to/remote_file /path/to/local/target-directory\n</code></pre> <p>Copy a directory from the HPC Cluster:</p> <pre><code>rsync -avz  user@login-hpc.rcc.mcw.edu:/path/to/remote_directory /path/to/local/target-directory\n</code></pre>"},{"location":"storage/file-transfer/#rclone","title":"rclone","text":"<p>RClone is a command line utility for copying files between cloud servers (i.e. oneDrive, Google Drive, Dropbox, Box, etc) and another server or workstation. All commands should be run from the command-line in a terminal app on your computer.</p>"},{"location":"storage/file-transfer/#running-rclone-in-your-local-computer","title":"Running rclone in your local computer","text":"<p>Install RClone:</p> <pre><code>sudo -v ; curl https://rclone.org/install.sh | sudo bash\n</code></pre> <p>Configure the connection to the RCC:</p> <pre><code>rclone config\n</code></pre> <p>Then, follow the instructions in the command line, leaving any default values as they are. Select option 48 (ssh) for the type of storage. Write login-hpc.rcc.mcw.edu for the host. SSH username is the same that you use to login to the cluster. Finally, when asked about the SSH password, select y (type in my own password). Here you will find more detailed instructions. This configuration only needs to be done once.</p> <p>Configure the connection to the Cloud:</p> <p>This configuration depends on the cloud service that you wish to connect. For a list of cloud servers and detailed information on how to configure each of them please follow the instructions in this page: RClone configuration. This configuration only needs to be done once.</p> <p>Copying a file or directory from the cloud to the HPC Cluster:</p> <pre><code>rclone copy cloud:/path/to/file_or_folder user@login-hpc.rcc.mcw.edu:/path/to/target-directory\n</code></pre> <p>Replace <code>cloud</code> by the name you gave to your cloud connection during the configuration above.</p> <p>Copying a file or directory from the HPC Cluster to the cloud:</p> <pre><code>rclone copy user@login-hpc.rcc.mcw.edu:/path/to/target-directory cloud:/path/to/file_or_folder\n</code></pre> <p>Replace <code>cloud</code> by the name you gave to your cloud connection during the configuration above.</p>"},{"location":"storage/file-transfer/#running-rclone-in-the-rcc","title":"Running rclone in the RCC","text":"<p>Configure the connection to the cloud:</p> <p>In order to do this configuration, you will need a web browser. First, connect to onDemand and login with your credentials. Go to Interactive Apps on top and select Remote Desktop. Then, launch a new interactive session. Your remote desktop session might take a few minutes to be ready, depending on the number of cores, the time requested and how overloaded is the system at the time.</p> <pre><code>module load rclone\n</code></pre> <p>The instructions on how to configure the connection to the cloud, as mentioned above, depend on the cloud service that you are connecting. For a list of cloud servers and detailed information on how to configure each of them please follow the instructions in this page: RClone configuration. When the browser window opens, log into your account and click the Grant Access link that appears. This configuration only needs to be done once.</p> <p>You can now use rclone in the Remote Desktop or in your terminal when connected to the RCC.</p> <p>Copying a file or directory from the cloud to the HPC Cluster:</p> <pre><code>rclone copy cloud:/path/to/file_or_folder /path/to/target-directory\n</code></pre> <p>Replace <code>cloud</code> by the name you gave to your cloud connection during the configuration above.</p> <p>Copying a file or directory from the HPC Cluster to the cloud:</p> <pre><code>rclone copy /path/to/target-directory cloud:/path/to/file_or_folder\n</code></pre> <p>Replace <code>cloud</code> by the name you gave to your cloud connection during the configuration above.</p> <p>For a full list of commands visit The RClone Commands page</p>"},{"location":"storage/file-transfer/#desktop-clients","title":"Desktop Clients","text":"<p>Several software packages are available for data transfer using the secure file transfer protocol (SFTP).</p> <ul> <li>CoreFTP is recommended and licensed by MCW-IS. Use the link to login and search for CoreFTP in the Software section.</li> <li>MobaXterm has a built-in SFTP client.</li> <li>WinSCP is a popular SFTP client for Windows.</li> <li>Cyberduck is a secure data transfer client available for Windows and Mac OS X users.</li> </ul>"},{"location":"storage/globus/","title":"Globus","text":""},{"location":"storage/globus/#globus","title":"Globus","text":"<p>Globus is a secure file transfer tool for research data. It is popular at research institutions and commonly deployed as a server. The unique features of Globus allow file transfer between MCW systems and between MCW and other schools. We are working on a central Globus Connect Server, but do not have a timeline yet. Here we present a work-around to transfer data between MCW and other institutions that do have Globus Connect Server.</p> <p>Globus @ MCW</p> <p>RCC is working to deploy a Globus Connect Server. This future installation will simplify data movement inside and outside MCW. We do not yet have a firm date. Please contact help-rcc@mcw.edu with questions and stay tuned for updates.</p>"},{"location":"storage/globus/#setup","title":"Setup","text":"<p>We will use Globus Connect Personal running on a cluster login node, which will allow transfer of data to/from storage locations on the cluster. This is a one-time setup with future sessions requiring only to start Globus.</p> <p>To get started, open your browser to https://app.globus.org/collections/gcp?generate_key. You will be prompted to login and may have to create your account the first time. Select Medical College of Wisconsin from the drop-down menu and continue to login with your MCW credentials.</p> <p></p> <p>You will setup a new collection, i.e. endpoint. Enter a Collection Display Name that is memorable to you, e.g. \"Cluster Dirs\", and select Generate Setup Key.</p> <p></p> <p>As of 5/1/2024, we have identified an issue with Globus producing empty keys with the above step, and they have been notified and are working on the issue.</p> <p>The workaround is to create the collection and get the setup key via the Globus CLI.  Run the following in a SSH session on a RCC cluster login node: </p><pre><code>module load globusconnect\nglobus login\nglobus gcp create mapped \"Cluster Dirs\"\n</code></pre><p></p> <p>The next step requires the cluster command-line. Make sure you have SSH access to a cluster login node. To proceed, load the Globus module and run the setup.</p> <pre><code>module load globusconnect\nglobusconnect -setup GLOBUS_KEY_HERE\n</code></pre> <p>If you see success, then you can start globusconnect.</p> <pre><code>globusconnect -start\n</code></pre> <p>Verify that you see successful startup. Before the next step, stop the process with Control-C.</p> <p>We can add folder paths on the RCC by editing <code>~/.globusonline/lta/config-paths</code>.</p> <pre><code>/scratch/g/PI_NetID,0,1\n/group/PI_NetID,0,1\n</code></pre> <p>Run the start command again and your folders will be accessible in the globusconnect application.</p> <pre><code>globusconnect -start\n</code></pre> <p>Don't forget to Control-C to disconnect/stop the session when you are done transferring data.</p>"},{"location":"storage/globus/#transfer-data","title":"Transfer data","text":"<p>Globus data transfer is controlled via web page. To get started, open your browser to https://app.globus.org/file-manager.</p> <p></p> <p>In the upper right corner, you can adjust your panel layout. When transferring data between endpoints, its easiest to select the side-by-side layout. Next you can search for your collections. Start by typing your MCW username into the left side collection box, which should find collections associated with you. Select the collection we created in previous steps. In the right side, enter your destination, which might be another school, or your desktop/laptop. We suggest you transfer data left to right to keep things simple.</p>"},{"location":"storage/mcw-storage/","title":"Additional Storage Options","text":""},{"location":"storage/mcw-storage/#additional-storage-options","title":"Additional Storage Options","text":"<p>MCW offers multiple campus-wide storage options.</p>"},{"location":"storage/mcw-storage/#mcw-is-storage","title":"MCW-IS Storage","text":"<p>Most users with MCW-owned or managed machines will recognize this storage as <code>G:\\</code> drive on their Windows computer. This storage is free and backed up, but limited in size and performance, and meant for productivity data, i.e., documents, spreadsheets, etc. Please do not store raw research data. Contact MCW-IS help desk with questions.</p>"},{"location":"storage/mcw-storage/#onedrive","title":"OneDrive","text":"<p>Every MCW user has a finite amount of free space available on OneDrive. This solution is cloud based, and utilizes your MCW credentials for access. It is suitable for productivity data, i.e., documents, spreadsheets, presentations, etc. Please do not store raw research data. In addition, there is a max file upload size, and an option to share data. Contact MCW-IS help desk with questions.</p>"},{"location":"storage/mcw-storage/#electronic-lab-notebook","title":"Electronic Lab Notebook","text":"<p>Each electronic lab notebook has a finite amount of space available and a max single file upload size. ELN is free to all users, but should not be considered a primary storage system. ELN should be used to store data that would normally be stored in a paper notebook, or supplemental storage. Data sharing is possible.</p>"},{"location":"storage/paid-storage/","title":"Paid Additional Storage","text":""},{"location":"storage/paid-storage/#paid-additional-storage","title":"Paid Additional Storage","text":""},{"location":"storage/paid-storage/#overview","title":"Overview","text":"<p>Additional research group storage, i.e., <code>/group/PI_NetID</code>, is available through a paid subscription.</p>"},{"location":"storage/paid-storage/#cost","title":"Cost","text":"<p>The 2025 rate is $80/TB/year. All fees must be prepaid and minimum addition is 1 TB. Minimum duration is 1 year (January 1 to December 31), or the number of months until the start of the next calendar year. Fees will be prorated according to the number of months until December 31. Research Computing will not refund storage fees for any reason.</p> <p>Storage price increases</p> <p>The price of storage will increase incrementally starting in 2025 and beyond. The pricing schedule is as follows:</p> <p>2025 - $80/TB/year 2026 - $100/TB/year 2027 - $120/TB/year</p> <p>RCC will re-evaluate storage pricing in FY27 and welcome your feedback on this issue. For more information, please see our recent news post about storage price changes.</p>"},{"location":"storage/paid-storage/#unpaid-fees","title":"Unpaid Fees","text":"<p>If fees are unpaid, RCC admins will change your research group storage directory to read-only (i.e. no new data can be written). The PI will have until March 31 to pay any fees that are due or remove the data.</p>"},{"location":"storage/paid-storage/#ownership","title":"Ownership","text":"<p>This storage service is a subscription-based lease of storage space for a finite duration. RCC retains ownership of all hardware associated with this service.</p>"},{"location":"storage/paid-storage/#availability","title":"Availability","text":"<p>Storage availability is not guaranteed. Requests greater than 50 TB will require review. Please contact help-rcc@mcw.edu to inquire.</p>"},{"location":"storage/paid-storage/#sign-up-payment","title":"Sign-up &amp; Payment","text":"<p>A RCC account is required. If you do not already have a RCC account, please submit a request. RCC will then process your account and provision the storage. If you are not a PI, your PI must also have an account.</p> <p>Payment is made through the MCW-IS service desk website using the following guide.</p> <ol> <li>Proceed to https://servicedesk.mcw.edu and login.</li> <li>Select Software &gt; RCC Software, then Research Group Storage - 1TB.</li> <li>To pay for 1 TB, select Add to Cart. If you are paying for multiple TBs, select Add Multiple and enter the quantity.</li> <li>Select Place Your Order and enter your 16-17 Digit Account Number.</li> <li>Finally, you'll be asked to enter PI information. Use this if you're a non-PI requesting the storage on behalf of a PI.</li> </ol> <p>Questions?</p> <p>If you have questions about your quota limit, please email help-rcc@mcw.edu. If you have questions about the self-service payment process, please contact the MCW-IS help desk.</p>"},{"location":"storage/rcc-storage/","title":"Storage Overview","text":""},{"location":"storage/rcc-storage/#storage-overview","title":"Storage Overview","text":"<p>Research Computing provides storage with a dedicated purpose to hold and support analysis of raw research data. Every MCW lab is eligible for a limited amount of free storage. For many labs, this amount of free storage is sufficient for their research. For labs with large data needs, additional storage is available for fee.</p> <p>All storage is connected via high speed link to the cluster and available to you via Linux command-line, SFTP, or Open OnDemand.</p> <p>Each user has the same set of default storage paths:</p> Type Path Quota Protection Description <code>Home</code> /home/netid 100 GB snapshot, replication account configuration and scripts <code>Group</code> /group/pi_netid 1 TB, expandable with payment snapshot, replication shared raw research data <code>Scratch</code> /scratch/g/pi_netid 25 TB none temporary job files You can easily find your available storage paths and current utilization on the cluster  with the <code>mydisks</code> command. <pre><code>$ mydisks\n=====My Lab=====\nSize  Used Avail Use% File\n47G   29G   19G  61% /home/user\n932G  158G  774G  17% /group/pi\n4.6T     0  4.6T   0% /scratch/g/pi\n</code></pre>"},{"location":"storage/rcc-storage/#storage-paths","title":"Storage Paths","text":""},{"location":"storage/rcc-storage/#home","title":"Home","text":"<p>The home directory is your starting place every time you login to the cluster. It's location is <code>/home/netid</code>, where <code>netid</code> is your MCW username. The purpose of a home directory is storing user-installed software, user-written scripts, configuration files, etc. Each home directory is only accessible by its owner and is not suitable for data sharing. Home is also not appropriate for large scale research data or temporary job files.</p> <p>The quota limit is 100 GB and data protection includes replication and snapshots. For more info on snapshots, and how you might recover a file, please see file recovery.</p>"},{"location":"storage/rcc-storage/#group","title":"Group","text":"<p>Group storage is a shared space for labs to store research data in active projects. Each lab receives 1 TB for free and can expand via additional paid storage. This space is large scale, but low performance. It is not meant for high I/O, and so is not mounted to compute nodes. Data protection includes replication and snapshots. For more info on snapshots, and how you might recover a file, please see file recovery.</p> <p>This space is organized by lab group. Each folder in <code>/group</code> represents a lab, and is named using the PI's NetID (username). For example, a PI with username \"jsmith\" would have a group directory located at <code>/group/jsmith</code>. Directories within that lab space are organized by purpose and controlled by unique security groups. For example, there is a default <code>/group/pi_netid/work</code> directory, which is shared space restricted to lab users. Other shared directories can be created by request for projects that require unique permissions. Additionally, you may have data directories related to your use of a MCW core. These directories will be named for the core and located at <code>/group/pi_netid/cores</code>. For example, a Mellowes Center project could be delivered to your group storage and located at <code>/group/pi_netid/cores/mellowes/example_project1</code>.</p>"},{"location":"storage/rcc-storage/#scratch","title":"Scratch","text":"<p>Scratch storage is intended for temporary job files. Every group has a directory at <code>/scratch/g/pi_netid</code> with quota limit 25 TB. Files on scratch storage are subject to retention limits, which is discussed below. In general, you should avoid storing files on scratch unless you are running a job. Please remember that scratch storage is limited and shared among all groups.</p>"},{"location":"storage/rcc-storage/#retention","title":"Retention","text":"<p>Any file that is older than 60 days (based on creation, modification, or access time) may be deleted by RCC admins without notice. RCC sends periodic reminders to clean up scratch storage directories. These reminders will include your percentage of files older than 60 days. A list of these old files can be provided upon request. If the scratch file system becomes full, RCC admins may begin deleting files without notice, starting with files older than 60 days, followed by any other files as needed to free up space.</p> <p>Scratch storage is for jobs, not projects.</p> <p>Do not use scratch storage for long-term project data. If you are not running a job, you should not have any data in scratch. Failure to adhere to this policy may result in loss of your data located on scratch.</p>"},{"location":"storage/rcc-storage/#local-scratch","title":"Local Scratch","text":"<p>Every compute node has a local scratch space to be used for runtime files. Each job will have a unique folder that appears as <code>/tmp</code>, which is only accessible to processes within the SLURM job. Please note, this space is cleaned (data deleted) after each of your jobs.</p> <p>Local scratch may be the fastest option to store your job runtime files, especially for jobs that are heavily I/O dependent (i.e. lots of files are read/written). However, the speed of the disk should be weighed against the additional time to transfer files from your global scratch directory.</p> <p>All compute nodes have 440 GB of local scratch storage, except as noted below.</p> <p>Some GPU nodes have more local scratch.</p> <p>Compute nodes gn07 and gn08 have 17 TB of local storage and gn09 has 7 TB. These nodes are intended for large scale AI jobs that require local fast storage.</p>"},{"location":"storage/rcc-storage/#permissions","title":"Permissions","text":"<p>Every lab storage path will have an associated security group consisting of the PI and additional users that the PI adds. We require two points of contact that are authorized to request permissions changes. The PI will serve as one point of contact and will provide an alternate. Any group permission changes must come from the PI or alternate. Requests must be made through the MCW ticketing system.</p>"},{"location":"storage/rcc-storage/#restrictions","title":"Restrictions","text":"<p>The following types of data are strictly prohibited on Research Computing systems:</p> <ul> <li> <p>Any data that would violate the MCW Code of Conduct, MCW Corporate Polices, or any applicable data-use agreement (i.e. IRB, federal grant regulations, etc.)</p> </li> <li> <p>Any data that is subject to HIPAA</p> </li> </ul>"},{"location":"storage/rcc-storage/#data-protection","title":"Data Protection","text":"<p>All Research Computing storage systems are highly resilient, allowing for multiple disk and server failures without losing data. We also maintain support contracts for all storage with provisions for prompt hardware replacement. In addition, some file systems have additional protection.</p> <p>Home directories and research group storage, i.e., <code>/group/pi_netid</code>, are protected by snapshots and replication. Replication occurs via two identical storage systems located in our two data centers. Replication occurs every minute (continuously), with changes in the primary file system sent to the target file system. These changes might include new files, edits, deletions, etc. Replication is a disaster recovery measure, protecting against the loss of a full file system. Since all data changes are replicated, it does not protect against accidental or unintended edits, overwrites, deletions, etc.</p> <p>To protect against these unintended changes, we use snapshots. The granularity of snapshots determines which of these changes are captured. It is not guaranteed that all changes will be captured by snapshots, and therefore recoverable. For instance, we keep 14 daily and 6 weekly snapshots. If an unintended change is caught by the snapshot, i.e. you delete a file that has been on the file system for more than 24 hrs, you may be able to recover it. However, snapshots may miss changes that occur within 24 hrs. For instance, if you create and delete a file before the daily snapshot can capture a version of the file, then it would not be recoverable.</p> <p>Disclaimer</p> <p>Research Computing is not responsible for any loss of data. We strongly encourage all users to follow best practice data management strategies.</p>"},{"location":"storage/ref-data/","title":"Reference Data","text":""},{"location":"storage/ref-data/#reference-data","title":"Reference Data","text":"<p>RCC maintains a reference data space on the cluster that is available to all users. It is located at <code>/hpc/refdata</code>. The purpose of this space is to provide central access to commonly used data. This allows users to save space and share a common reference dataset. RCC will also download or build reference data and connect it to applications when possible.</p> <p>Users may request to add reference data by contacting help-rcc@mcw.edu.</p>"},{"location":"storage/ref-data/#examples","title":"Examples","text":"<p>The following are examples of reference data that RCC currently hosts. This is not a complete list.</p> <p><code>/hpc/refdata/blast/2.9.0</code> <code>/hpc/refdata/bowtie2/hg19</code> <code>/hpc/refdata/gatk</code> <code>/hpc/refdata/hisat2/Ensembl_GRCh38</code> </p>"},{"location":"news/page/2/","title":"Recent News","text":""},{"location":"news/page/2/#recent-news","title":"Recent News","text":""},{"location":"news/page/3/","title":"Recent News","text":""},{"location":"news/page/3/#recent-news","title":"Recent News","text":""},{"location":"news/page/4/","title":"Recent News","text":""},{"location":"news/page/4/#recent-news","title":"Recent News","text":""}]}